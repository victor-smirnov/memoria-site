var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement),f=suggestions.classList.contains('d-none');let c=0;b.keyCode===38&&!f?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&!f&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/overview/introduction/",title:"Introduction to Memoria",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eData dominates. If you\u0026rsquo;ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u0026ndash; Rob Pike in \u003ca href="http://www.lysator.liu.se/c/pikestyle.html"\u003e“Notes on Programming in C”\u003c/a\u003e, 1989.\u003c/p\u003e\n\u003ch2 id="what-memoria-is"\u003eWhat Memoria is\u003c/h2\u003e\n\u003cp\u003eMemoria is a full-stack data engineering framework aiming at exploiting inherent structure in data at all scales, starting form bare fabric or reality and ending at high-level visualizations. See \u003ca href="/docs/overview/definitions"\u003edefinitions\u003c/a\u003e for the quick overview of philosophy and math behind Memoria.\u003c/p\u003e\n\u003ch2 id="target-applications"\u003eTarget Applications\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHardware-accelerated Transactional Databases and Storage Engines.\u003c/li\u003e\n\u003cli\u003eFilesystems.\u003c/li\u003e\n\u003cli\u003eAnalytics.\u003c/li\u003e\n\u003cli\u003eArtificial Intelligence and Machine Learning.\u003c/li\u003e\n\u003cli\u003eSoftware and hardware development tools.\u003c/li\u003e\n\u003cli\u003eAnd any combination of the above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="project-structure"\u003eProject structure\u003c/h2\u003e\n\u003cp\u003eMain implementation language is modern C++ (14/17/20). Python bindings are also provided, mainly for ad-hoc manipulation with a data.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStructured data is exposed to applications via \u003cstrong\u003eContainers\u003c/strong\u003e API.\u003c/li\u003e\n\u003cli\u003eContainers are managed by a \u003cstrong\u003eStore\u003c/strong\u003e. It\u0026rsquo;s an API that may have multiple implementations, like in-memory or on-disk.\u003c/li\u003e\n\u003cli\u003eComputations over data are described via low-level \u003cstrong\u003eDataflow graph\u003c/strong\u003e-like language, that may have multiple compilation targets (CPU, Accelerators, FPGAs/ASICs).\u003c/li\u003e\n\u003cli\u003eThe project\u0026rsquo;s structure is deeply automated with dedicated \u003cstrong\u003eMemoria Build Tool (mbt)\u003c/strong\u003e, on top of Clang libraries and Python scripts.\u003c/li\u003e\n\u003cli\u003eAdditional \u003cstrong\u003etools\u003c/strong\u003e, like \u003ca href="/docs/datascope/overview"\u003eDatascope\u003c/a\u003e to accommodate development process.\u003c/li\u003e\n\u003cli\u003eRich set of highly customizable generic data containers and dataflow templates for various narrow fields (SQL Analytics, AI and ML, probabilistic programming, etc).\u003c/li\u003e\n\u003cli\u003eHigh-performance cross-platform (Windows, Linux, MacOSX) runtime environment with Asynchronous IO and Fibers.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="what-memoria-is-not"\u003eWhat Memoria is NOT\u003c/h2\u003e\n\u003cp\u003eMemoria itself is not a DBMS, because \u003cem\u003eno size fits all\u003c/em\u003e. Instead, it can be used to build a memory-focused computational infrastructure (both at the hardware and at the software levels), fitting the needs of specific applications.\u003c/p\u003e\n\u003cp\u003eNevertheless, \u003ca href="/subprojects/swimmer-db/"\u003eSwimmerDB\u003c/a\u003e is a reference implementation of a Memoria-enabled scaleup-oriented (embedded, single-host or small-cluster) database engine, mostly for prototyping data structures, but also for real applications if the database fits their requirements.\u003c/p\u003e\n'}).add({id:1,href:"/docs/overview/definitions/",title:"Basic Definitions",description:"Basic Definitions",content:'\u003cp\u003eBelow there are few definitions that ware used intensively throughout the docs.\u003c/p\u003e\n\u003ch2 id="data-structure"\u003eData Structure\u003c/h2\u003e\n\u003ch2 id="container"\u003eContainer\u003c/h2\u003e\n\u003ch2 id="store"\u003eStore\u003c/h2\u003e\n\u003ch2 id="dataflow"\u003eDataflow\u003c/h2\u003e\n\u003ch2 id="single-writter-mutiple-readers-swmr"\u003eSingle Writter Mutiple Readers (SWMR)\u003c/h2\u003e\n\u003ch2 id="multiple-writers-multiple-readers-mwmr"\u003eMultiple Writers Multiple Readers (MWMR)\u003c/h2\u003e\n'}).add({id:2,href:"/docs/overview/whyc++/",title:"Why C++",description:"Why Memoria is using C++",content:'\u003cp\u003eMemoria is using C++ as its main development language and it\u0026rsquo;s a deliberate decision. This document explains what C++ gives to data platforms and how optimal language and run-time environment might look like.\u003c/p\u003e\n\u003cp\u003eFrom set-theoretic perspective there are much more data structures than algorithms, this is the reason why generic programming (GP) support is important for any modern programming language. The same linear search algorithm can be applied to the great variety of physical data layouts and types. Generic programming hides this variety from the programmer.\u003c/p\u003e\n\u003ch2 id="two-types-of-generic-programming"\u003eTwo Types of Generic Programming\u003c/h2\u003e\n\u003cp\u003eThere are two main ways how generic programming can be implemented. \u003cstrong\u003ePolymorphic\u003c/strong\u003e GP is based on the fact that all objects allocated on the heap have common layout and access pattern. They are long-lived and referenced via their addresses in the heap. Primitive data types may also be wrapped into an object (boxed). In this case it\u0026rsquo;s possible to compile only one instance of generic procedure and then use virtual dispatch to select possibly different execution paths for different types. Polymorphic GP-supporting compiler just checks generic type substitution rules at compile-type, then erase all type-specific information on generic variables because it\u0026rsquo;s not actually needed at run-time. This type of GP is the most scalable, providing that the programmer is happy with heap allocation and primitive data types boxing.\u003c/p\u003e\n\u003cp\u003eAs a specific optimization, some polymorphic GP compilers can do whole or partial program monomorphisation, when they generate type-specific instances of procedures. Note that from the programmer\u0026rsquo;s perspective, this optimization is a black box, there is no flexible way to mange it. Yet it\u0026rsquo;s a cheap way to increase performance of generic code in some cases.\u003c/p\u003e\n\u003cp\u003eCompilers supporting \u003cstrong\u003eMonomorphic\u003c/strong\u003e GP produce specialized instance of generic procedure for each set of types it is used with in the program. This type of GP produces highly data-specialized and potentially the most performant code but it\u0026rsquo;s not scalable. Monomorphic GP code is not modularizable, there is no separate compilation of generic libraries possible. In some cases it\u0026rsquo;s possible to hide generic code behind \u003ca href="https://bitbucket.org/vsmirnov/memoria/wiki/TemplatePimpl"\u003equasi-generic interfaces\u003c/a\u003e, compiled separately from application code using it. Nevertheless, if definition of a generic procedure or datatype is changed, all code using it directly must be recompiled.\u003c/p\u003e\n\u003cp\u003eMonomorphic GP is not limited to a specific heap data layout scheme, it doesn\u0026rsquo;t depend on objects and virtual dispatch, it\u0026rsquo;s truly generic. From other side, polymorphic GP is limited to heap objects but allows separate compilation of generic code because only one type of generic procedure is instantiated. Unfortunately, there is no cheap and elegant way to have both types of GP in a single language. So, practical way is to stick with only one type of GP: Java, C# and other languages with managed heaps support polymorphic GP. C++, D and Rust support mononorphic GP.\u003c/p\u003e\n\u003cp\u003eNote also that while some languages like C# (and Java in a distant future) support elements of monomorphic GP like generics on primitive data types, this should not be considered fully featured monomorphic GP, because such schemes are not Turing-complete and hence can\u0026rsquo;t be considered a programming. More on this below.\u003c/p\u003e\n\u003cp\u003eBy and large, the following is true for both types of GP with minor exceptions. Monomorphic GP is focused on Turing-complete type construction and specialization. The result is a great variety of different physical data layouts in the memory and layout-specific operations on that data, that may lead to unnecessary increase in machine code size.\u003c/p\u003e\n\u003cp\u003ePolymorphic GP is built around specific object layouts in the heap allowing to compile only one instance of a generic procedure for all types. This type of GP does not usually affect run-time code in any way. Instead it allows a programmer to define a set of type substitution rules, which are enforced at compile time. Polymorphic GP just guarantees that actual type substitution in the program is correct.\u003c/p\u003e\n\u003cp\u003eThough monomorphic GP languages may also provide type substitution checking, rules are much more complex in this case, and there is ongoing debate around how to implement this feature efficiently. Rust has some form of substitution checking, but C++ doesn\u0026rsquo;t (yet).\u003c/p\u003e\n\u003ch2 id="generic-programming-for-data-structures"\u003eGeneric Programming for Data Structures\u003c/h2\u003e\n\u003cp\u003eSo, polymorphic GP is scalable in terms of generic code side but limited to object heaps with predefined data layouts. This is fine until our data structures are well-reducible to object graphs (linked lists) without introducing much overhead. Many practically important lightweight in-memory data structures like arrays, maps, trees, generic graphs and so on are of this kind. When specialization of such simple data structure on a primitive data types is desirable, restricted form of monomorphisation like in C# may be provided.\u003c/p\u003e\n\u003cp\u003eThough linked list is pretty generic data structure by itself and suitable to be a foundation for many practical data structures, object heaps are not always optimal when we need more precise (bit-grade) control on physical data layout in main memory. For example, linked list-based implementations of suffix trees, used in bioinformatics, have around 40x space overhead over raw unstructured genome data. But custom schemes have much more improved space properties.\u003c/p\u003e\n\u003cp\u003eIn order to be efficient for generic data structures implementation, any programming language must provide generic configurable mapping from logical data space (ex.: objects) to physical data layout in main and external memory. C++ by itself together with its GP capabilities is well suitable for this task.\u003c/p\u003e\n\u003ch2 id="c-for-data-structures"\u003eC++ for Data Structures\u003c/h2\u003e\n\u003cp\u003eFirst, C++ is a multi-paradigm language with very cheap abstractions which do not impose much overhead at run-time. In the simplest case C++ uses zero-cost object mapping to a raw memory location. That means,  information other than object properties is put into an object by the compiler. Important details of this mapping like alignment can be controlled via compiler attributes and pragma-directives.\u003c/p\u003e\n\u003cp\u003eSecond, C++ has generic value types when a value is physically put inside specified context withing existing memory mapping:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003etemplate \u0026lt;typename T\u0026gt;\nstruct IntContextMapping {\n    int prefix_; // Some data fields before.\n    T value_     // value_ mapping will be in-lined between prefix_ \n                 // and suffix_ using default alignment.\n    int suffix_; // Some data fields after.\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThird, C++ allows building almost arbitrary types using template metaprogramming, and hence, arbitrary memory mappings. The following example demonstrates how different classes can be mapped linearly to a memory region with specified order via linear class inheritance hierarchy generator:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// Forward declaration for linear hierarchy generator helper class. The\n// helper takes list of mapping members which are classes with one template\n// parameter, that is a chained base class.\ntemplate \u0026lt;template \u0026lt;typename Base\u0026gt; class C...\u0026gt; struct LinearHierarchy; \n\n// Main case. Split the list into head and tail. Instantiate Head as a superclass\n// with LinearHierarchy\u0026lt;Tail...\u0026gt; as a parameter.\ntemplate \u0026lt;\n    template \u0026lt;typename\u0026gt; class Head,\n    template \u0026lt;typename\u0026gt; class Tail...\n\u0026gt; \nstruct LinearHierarchy: Head\u0026lt;LinearHierarchy\u0026lt;Tail...\u0026gt;\u0026gt; {};\n\n// There are no more members in the list. Stop the generator. \ntemplate \u0026lt;\u0026gt; struct LinearHierarchy {\n    void visit() {} // do nothing here\n};\n\n\n// First example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member1: Base {\n    int value1_ = 1; // simple data member \n\n    // Function member that may call functions in other layout\n    // members upward the hierarchy providing their names are known.\n    void visit() \n    {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value1_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\n// Second example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member2: Base {\n    int value2_ = 2;\n    void visit() {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value2_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\n// Fird example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member3: Base {\n    int value3_ = 3;\n    void visit() {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value3_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\nvoid do_something() \n{\n    LinearHierarchy\u0026lt;Member1, Member2, Member3\u0026gt; m;\n    m.visit(); // prints 3 2 1\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn other words we can build arbitrary type-to-memory mappings using basic C++ rules for POD types and template metaprogramming. In Memoria complex high-level data structures like relational tables are split into reusable fragments, which are combined by the metaprogramming framework into compound structures and mapped to raw memory blocks. Building a complex data structure from simple blocks, compiler can solve combinatorial optimization problems by selecting fragments most suitable for the specific context.\u003c/p\u003e\n'}).add({id:3,href:"/docs/overview/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:""}).add({id:4,href:"/docs/data-zoo/overview/",title:"Containers Overview",description:"",content:""}).add({id:5,href:"/docs/storage/overview/",title:"Storage Engines Overview",description:"",content:""}).add({id:6,href:"/docs/storage/memory-store/",title:"Memory Store",description:"",content:""}).add({id:7,href:"/docs/storage/swmr-store/",title:"SWMR Store",description:"",content:""}).add({id:8,href:"/docs/storage/overlay-store/",title:"Overlay Store",description:"",content:""}).add({id:9,href:"/docs/d-phil/overview/",title:"Memoria for AI - Overview",description:"",content:""}).add({id:10,href:"/docs/d-phil/ai/",title:"Memoria and Artificial Intelligence",description:"",content:'\u003cp\u003e\u003cstrong\u003eNote that this page is work in progress, as well as other parts of Memoria. So be patient. Thanks!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSince AI is getting a hot topic and many folks started asking me about participating in AGI startups, I decided to outline my view on the topic using easy language. The following is my (Victor Smirnov) personal view on AI that other committers and contributors may not necessary share or support.\u003c/p\u003e\n\u003ch2 id="too-long-dont-read"\u003eToo Long; Don\u0026rsquo;t Read\u003c/h2\u003e\n\u003cp\u003eMy views on AGI are largely influenced by or mostly consistent with the following theories:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eGraziano\u0026rsquo;s Attention Schema Theory (AST), explaining how self-referentiality contributes to phenomenal reports.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDamasio\u0026rsquo;s Somatic Markers Theory (SMT) explaining basics of emotional intelligence: guiding function of emotions in decision making.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJean-Louis Dessalles\' Simplicity Theory explaining subjective attractiveness in a fundamental way.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTononi\u0026rsquo;s Integrated Information Theory (IIT), linking conscious states with descriptional complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDaniel Dennett\u0026rsquo;s Cartesian Theater and Multiple Drafts. Consciousness is not an illusion in a strict sense. But traditional first-person view on it is heavily biased, that leads to various paradoxes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRay Solomonoff\u0026rsquo;s theory of Universal Induction (UI). This theory explains theoretical and practical limits of problem solving.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJurgen Schmidhuber\u0026rsquo;s Artificial Curiosity (AC) and his theory of intrinsic reward, explaining highest emotions in a mathematically universal way.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchmidhuber\u0026rsquo;s Goedel Machine, explaining limits of recursive self-improvements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAST gives clever insights into how certain important subjective phenomena emerges in a self-referential structures like \u0026ldquo;attention schemes\u0026rdquo; but it leaves some important questions unexplored. In this page I introduce a semi-formal self-referential machine, self-referential algorithms and self-referential data structures constituting self-models. There is nothing special about them from mathematical perspective, but they have certain properties by turning computational phenomena into descriptions. For example, how gradual incompleteness of self-referentiality leads to the lack of \u0026ldquo;process introspection\u0026rdquo; and, by that, leads to \u0026ldquo;inexplicable\u0026rdquo; elements in phenomenal reports, like qualia, which are\u003c/p\u003e\n\u003cp\u003eDamassio introduces guiding function of emotions in its SMT that manifests by affecting decision making in an unconscious way. Schmidhuber proposes universal scheme for certain higher emotions, but emotions are not feelings. Conceptual jump from emotions to feelings requires subjectivity, and I\u0026rsquo;ll try to show how feelings are self-models of emotions.\u003c/p\u003e\n\u003cp\u003eSelf-models can be learned directly from data via induction. Solomonoff\u0026rsquo;s UI is complete but not computable. Computable induction can\u0026rsquo;t be complete, but can be approximated with various techniques. If self-models are expressive enough to emulate universal computer, they can be used to host they inductive learning, giving functionally complete metacognition. It turns out that phenomenal consciousness is not just a side-effect of intelligence, it\u0026rsquo;s a substrate-independent AI by itself, capable for problem solving.\u003c/p\u003e\n\u003cp\u003eHard problem of consciousness then can be reformulated the following way: \u003cstrong\u003ecan all phenomenal reports be composable and decomposable in a computable way\u003c/strong\u003e. To prove this, it is necessary to provide minimalistic but universal self-model that can evolve with algorithmic induction into any human phenomenal report \u003cem\u003ein the limit\u003c/em\u003e. So, no subjective phenomena will be left unexplained in the limit.\u003c/p\u003e\n\u003cp\u003eMemoria provides decent environment for self-modeling. It has framework for advanced data structures like self-indexes, that can turn self-models into advanced databases, and allows deploying such structures at scale in distributed and decentralized manner. Memoria has vertically integrated AIO subsystem from raw block devices to networking and C++-based heterogeneous computations unifing CPUs, GPUs and ASICs. It\u0026rsquo;s primary goal to make approximations of Universal Induction practical, which is necessary to evolve self-models.\u003c/p\u003e\n\u003ch2 id="a-sketch-of-main-ideas"\u003eA Sketch of Main Ideas\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThere are theoretical schemes for AGI, but they are not finitely computable. Any finitely computable AGI will not be complete or, in other words, truly generic. These schemes can be approximated, if we are agree to sacrifice some generality to achieve desirable performance in certain domains.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHuman brain is not an AGI in that narrow sense, though it can be true AGI in the limit, providing infinite time and space. Being an approximation of AGI, it can solve some problem classes efficiently (fast), being slow on all other classes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOne of possible AGI approximation strategies is trading memory for speed. AGI is based on search in Turing-complete programs space that is too huge to be tractable for anything except toy domains.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo speedup this process we can apply restrictions to the program space, so the search will be performed only within small subset of the space. The method is no more complete but can be flexibly tuned for very wide variety of problem classes just by changing the system of restrictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn most cases the system of such restrictions is poorly compressible, so the only way to specify it, is by direct description via data structures. In other words, an AGI approximation, built on this principle, will have huge database. The bigger the database, the more generic intelligence such a system will have. There are theoretical results that practical AGI will be complex in any means, and present complexity of advanced ANNs is reflecting this principle.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis database will be much more complex than those ones currently in use. It will be based on very advanced coding schemes and data structures. Memoria is all about that.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt\u0026rsquo;s not that easy to determine classes of problems brain-AGI can solve efficiently: human-like AGI approximation problem.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHard problem of AI reformulated: does phenomenal consciousness have any problem-solving abilities by itself? Yes, it does.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSentiments and emotional Intelligence: emotions, feelings and guiding function of emotions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFrom emotion to feeling. Self-referential machines have a \u0026ldquo;dualistic\u0026rdquo; property represent computations as data and vise versa. They can build self-model allowing them to act on the stored log of their own previous states in the way increasing global and local utility functions. Entire universal Turing machine can be implemented this way, performing self-referential operations on self-referential data structures.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eParticularly, this type of machine can catch certain fundamental computational phenomena and use them as descriptions. For example, inherent incompleteness of self-referentiality (infeasibility to build a complete self-model) results in simplified self-models systematically lacking some causality links. Everything systematic (even if this systemacy is stochastic) can be turned into description.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor example, if we can somehow arrange this lack of causal structure in self-descriptions in a multi-scale way, we can turn corresponding records in memory logs into multiscale data structures, analogous to \u003ca href="https://bitbucket.org/vsmirnov/memoria/wiki/Multiary%20Wavelet%20Tree"\u003ewavelet trees\u003c/a\u003e, instantly enabling many sophisticated algorithms on top of such data structures.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA set of such structures with \u0026ldquo;dualistic\u0026rdquo; properties emerging in self-referential computations can be viewed as a formal language, describing \u0026ldquo;phenomenal reports\u0026rdquo; in memory logs of self-referential machine. This formal language can be Turing-complete, so, in principle we can build another self-referential machine inside a self-referential machine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, a Normalized Information Distance (NID) scheme can be build on top of this formal language, measuring similarities and differences between everything in a pretty generic way. NID-based encoding enables differential encoding when new data is represented as old data + some delta of novelty. NID can be efficiently approximated with Normalized Compression Distance (NCD). Simpler schemes are also possible.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNCD aligns naturally with mutiscale data structures from (12) and efficiently generalizes them to any domain where there is a more-or-less efficient compression scheme, that is Multiscale NCD-decomposition. NCD works fine with lossy compression.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eParticularly, self-referential decomposition of an object can efficiently implement its Mutiscale NCD decomposition relative to the entire history of the self-referential machine. In such decomposition some information, that is completely new to the machine, will appear as a very rude approximation or \u0026ldquo;vagueness\u0026rdquo; in phenomenal reports. Nevertheless, it contains some bits of actual data that contribute to the difference between vague elements of phenomenal report guiding decision-making.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf we define needs and emotions somehow for the self-referential machine, then motivations and feelings are self-models of needs and emotions correspondingly. Both needs and emotions are kind of internal stimulus, so motivations and feelings are sub-type of senses.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAll senses have \u0026ldquo;motivational\u0026rdquo; component that is a manifestation of guiding function of emotions to decision making reflected in phenomenal reports. It looks similar an attractor in non-linear dynamics, or gravity in physics. So, \u0026ldquo;something\u0026rdquo; happens behind the scene, at the level that is not accessible, and at the accessible level we have results of this hidden process.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf needful portrait codes the set of agent\u0026rsquo;s goals, and emotional portrait codes some stimulus relatively to the needful portrait, then sentiment is a self-model of emotional portrait of the stimulus.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUntil this moment it was not implied that self-referential machine has any special properties enabling it to learn like a human just because it resembles our core cognitive abilities. Nothing special is about such machine relative to learning. It\u0026rsquo;s implied that self-models are learned separately from self-referentiality with some kind of approximated program induction mentioned above: (1) - (7).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBut if a self-referential machine is expressive enough, it can host such inductive learning partially or entirely, resembling not just cognition as thinking but also cognition as obtaining new knowledge in a unified way. By separating cognition to self-referentiality and induction over it we are making entire problem much simpler to tackle.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe idea of self-referential machine\u0026rsquo;s solution to problem of qualia in AI is that there is no such thing as \u0026ldquo;red color\u0026rdquo;, the minimal element of memory log of such machine is \u0026ldquo;I see the red color\u0026rdquo;, that is not static strucure encoded in memory, but also a dynamic computation (in another context). In other words, some form of observer is always implied, and it is finally the machine itself. Note that this is a form of pan-proto-experientalism but in the \u0026ldquo;software\u0026rdquo; when higher-level experience is composed from lower-level ones in a computable way. Note also that this \u0026ldquo;experience\u0026rdquo; is also physical, because both the machine and information processing on it are physical.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFrom the functional perspective, qualia are soft bounds in building self-model of stimulus. Self-model developing is a computationally very hard task, but this complexity is not evenly distributed. Some elements of phenomenal report emerges faster than others. Some elements will remain underdeveloped forever and, because of their stable hardness, may become descriptions themselves, like qualia.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNote that if we accept that \u0026ldquo;rational\u0026rdquo; problem solving ability can be unsound and incomplete, then we can reduce rational intelligence to emotional one and implement it on the same computational substrate. Then such rationality will be heavily biased in many ways, and reduces to specific type of phenomenal report like \u0026ldquo;I\u0026rsquo;m reasoning logically here\u0026rdquo;. Note that sound and complete inference not feasible for everything except very narrow sets of problems.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe last question is \u0026ldquo;why\u0026rdquo; self-referentiality. Any on-line learning system must maintain consistency of its knowledge base, so it must somehow reason about itself. Any sufficiently large computer has non-perfect hardware that becomes more error-prone with size and has some kind of \u0026ldquo;homeostasis\u0026rdquo; to maintain. Self-referentiality emerges naturally in such environments. Moreover, all modern IT systems not just stores part of their computational states into logs, but can even act on this stored state, thus having some rudimentary self-model. Artificial consciousness is inevitable for sufficiently large and generic AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNote that while self-hosted induction in a self-referential machine does not explain consciousness in a strict philosophical sense, it transforms the problem from philosophical to functional domain, by explaining phenomenal reports in a functional way. If brain is indeed a self-referential machine, then it can improve its own process introspection in a monotonic way. If all subjective phenomena believed explainable in the limit this way by a person, then she can see herself as a machine phenomenally. Or, in another words, conceptual gap between self-aware person and machine disappears. That is what we need for upcoming age of AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt\u0026rsquo;s the lack of process introspection what make us so unique in our own eye at the first place. Know you machine!\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eForm another side, phenomenal consciousness is not finitely explainable indeed, as some philosophers insist. I mean that while we can have complete functional model of consciousness in a form of certain self-referential machine, this machine will never have complete self-model, so some internal phenomena may only believed explainable in the limit, but never (in a finite lifetime) actually be explained.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThere is one interesting property of self-referential computations. Phenomenal reports are not directly accessible from outside. It\u0026rsquo;s can be possible to read memory of a self-referential machine directly, but in order to reconstruct back memory logs into phenomenal reports, we have to run the machine. So the only generic way to access phenomenal reports is to ask the machine about its current experience.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSo far, form technical perspective, we have two complex problem. First is basic self-referential machine, tuned to process and generate human-compatible phenomenal reports. This is a key requirement that is needed for a machine to understand human and vise versa.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecond, is a feasible induction approximation to learn this machine from data, because it will be not manually programmable, except for toy problems. Self-referential machine must be expressible enough to host this induction approximation schemes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs it has been already said, universal induction can be approximated by trading time for memory, by restricting program search space. This system of restrictions will be poorly compressible, so the only way to represent it is direct description in a form of a database. If for some subset of tasks such system is compressible, then it reduces to a finite system of heuristics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria has long-term goals in hosting applications targeted these two problems. It\u0026rsquo;s an integrated data engineering framework providing (A) vertically integrated Asynchronous IO engine, starting from raw block storage to networking and visualization, (B) C++-based heterogeneous computing, merging CPU cores, GPUs and ASICs in a single data flow framework, and (C) confluently-persistent decentralized data structures, both basic ones like arrays, maps, sets, vectors, tables, trees, graphs, and advanced ones like mutiary wavelet trees.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria seamlessly integrates IO- and Compute-intensive tasks by making data as close as possible to computational resources.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBut unlike other AI-related platforms, Memoria does not specialize only on A[G]I, allowing to solve wide range of practical tasks currently common to BigData.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria\u0026rsquo;s design follows bottom-up evolutionary path to AGI. Instead of building \u0026ldquo;AGI-in-a-box\u0026rdquo;, Memoria\u0026rsquo;s approach consists in decomposing such complex thing as human-level metacognition into well-manageable building blocks that can be used together or independently, to bring new qualities into existing applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="types-of-ai"\u003eTypes of AI\u003c/h2\u003e\n\u003cp\u003eThere are many types of AI, and traditionally what we call Artificial Intelligence is a \u003cstrong\u003eGood Old-Fashioned AI\u003c/strong\u003e or \u003cstrong\u003eGOFAI\u003c/strong\u003e, or catching human ways of reasoning during \u003cem\u003eproblem solving\u003c/em\u003e in special notations, which for those times were logic languages and frames theory. The ultimate AI of this kind is an emotion-less \u0026ldquo;pure mind\u0026rdquo; or Expert System augmenting human reasoning through question-answering.\u003c/p\u003e\n\u003cp\u003eGOFAI has two main conceptual limitations: there was wide disagreement how to model higher mental functions and uncertainty. And if latter problem has finally been solved with probability theory, the question if computers can exactly model consciousness is still one of the most controversial in AI.\u003c/p\u003e\n\u003cp\u003eHistorically there were several main types of AI:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStrong AI\u003c/strong\u003e. In sort, this is AI with true consciousness, having the same nature with humans consciousness. Or, more certainly, that human consciousness can in principle be modeled on a digital computer. If Strong AI hypothesis is true, we can upload our consciousness into a computer and survive death this way. Term \u0026ldquo;strong\u0026rdquo; may be misleading because it does not describe or define problem solving abilities of such an AI, only its substantial equivalence to a human being (and other animals, of course). Strong AI can be as weak as an newborn infant, and still be qualified as strong, because it has fully-functioning consciousness. So, \u0026ldquo;strong\u0026rdquo; here means power of philosophical argument.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeak AI\u003c/strong\u003e. If you are quaking like a duck, flying like a duck and swimming like a duck, then you are a duck. Weak AI doesn\u0026rsquo;t try to explain mental states, only behaviors and verbal reports. And this type of AI can have much more problem solving abilities than humans typically have, so the naming it \u0026ldquo;weak\u0026rdquo; reflects its abilities as philosophical argument, not intelligence. \u003cstrong\u003eArtificial General Intelligence\u003c/strong\u003e or \u003cstrong\u003eAGI\u003c/strong\u003e mainly falls into this category if it doesn\u0026rsquo;t explicitly address the problem of consciousness.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eNarrow AI\u003c/strong\u003e or individual intelligent functions modeling, like computer vision or speech recognition, without intention to achieve good performance beyond certain domain. GOFAI is mainly falls into this category.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHuman-level AI\u003c/strong\u003e. An AI having general problem solving capabilities on par with average human, so it can replace humans in various domains. It\u0026rsquo;s not necessary an AGI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHuman-like AI\u003c/strong\u003e. An AI that looks and feels like a human, but not necessary has human-level problem solving abilities. Its main task is understand humans well and make human understand machines well. For example, when we are doing sentiment analysis, we are doing human-like AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHard AGI\u003c/strong\u003e. An AGI in a strict sense, or ultimate problem solver. This type of AGI is infeasible to build because it\u0026rsquo;s not finitely computable. But we can build its feasible approximations, which will be computable, but will not be completely generic. Note that \u0026ldquo;Hard\u0026rdquo; prefix here is used only distinguish this type of AI from the next one.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSoft AGI\u003c/strong\u003e. This is the type of AI that performs practically \u0026ldquo;well\u0026rdquo; in wide range of \u0026ldquo;complex domains\u0026rdquo;, but not necessary is a Hard AGI, especially because the latter is not finitely computable. Humans are \u0026ldquo;Human-like Strong Soft AGI\u0026rdquo; if we are trying to identify ourselves in this system of definitions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSuperintelligence\u003c/strong\u003e, or AI having problem solving abilities which are much beyond to what currently possible for non-augmented humans in wider range or domains and environments than it\u0026rsquo;s possible for humans. This type of AI is mostly feared of, because it potentially can enslave or even exterminate humanity just with its problem-solving abilities. Superintelligence is usually assumed to be a Hard AGI in the limit.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAs an example, when AI is reading certain emotional expressions, it\u0026rsquo;s Narrow AI. When it is reading emotions in general, it\u0026rsquo;s Weak AI. When AI is understanding feelings, it\u0026rsquo;s Strong AI. If someone is doing benevolent AI God, she is doing emotional superintelligence.\u003c/p\u003e\n\u003ch2 id="algorithmic-information-theory-basics"\u003eAlgorithmic Information Theory Basics\u003c/h2\u003e\n\u003cp\u003eExpressiveness and Visibility. Algorithms and data structures \u0026ndash; two edge cases of highly compressible strings and low compressible strings. What is in-between, is poorly visible for us. Compressed data structures: data that can answer more questions. Universal sequence prediction through compression. Reducing problems to sequence prediction. Correspondence with human behavior and cognition: artificial curiosity and simplicity principle, intrinsic motivation.\u003c/p\u003e\n\u003cp\u003eIncompressibility theorem and its implications for programming and AI: complex things are complex in any notation. For \u0026ldquo;practical\u0026rdquo; AGI there are approximation strategies throws ensemble (of narrow AIs) methods, that means that amount of intelligence, or the number of problem classes this AGI can solved efficiently, grows linearly with the size of ensemble. What we need is identify the set of problem classes human brain is able to solve efficiently. That is not an easy thing, but broad set of human cognitive material in form of literature, art, engineering and program sources can be used.\u003c/p\u003e\n\u003cp\u003eIn AIT we measure amount of information in bit string $S$ by the length of the shortest program $P$ generating or computing $S$. This length depends on properties of the string $S$ and on the language $L$ the program \u003cem\u003eP\u003c/em\u003e is written in. This shortest length we call \u003cstrong\u003eKolmogorov complexity\u003c/strong\u003e of $S: l = K_L(S)$. The main property of $K$ is that it does not depend on selection of $L$ more than a constant $C$ that is a length of interpreter form any $L1$ to $L2$. This additive constant $C$ depends on $L1$ and $L2$, but doesn\u0026rsquo;t depend on $S: K_{L1}(S) \u0026lt;= K_{L2}(S) + C(L1, L2)$. So, in AIT we drop $L$ and $C$ just keeping them in mind. $K$ has another important properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eAny universal programming language $L$ is a universal description language: any computable object $O$ can be first encoded as binary string $S$ and then described succinctly with some program $P$ generating cor computing $S$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLet $|S|$ is a length of string $S$ in bits. If for some object $O1$, $K(S1) \u0026laquo; |S1|$, then we say that $S1$ is \u003cstrong\u003ehighly compressible\u003c/strong\u003e or \u003cstrong\u003esimple\u003c/strong\u003e. If $K(S1) \\propto |S1|$, then we say that $S1$ is \u003cstrong\u003euncompressible\u003c/strong\u003e or \u003cstrong\u003ecomplex\u003c/strong\u003e. There are may intermediate descriptional complexity classes between highly compressible and uncompressible strings.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA string is completely \u003ca href="http://www.scholarpedia.org/article/Algorithmic_randomness"\u003erandom\u003c/a\u003e in AIT if it is uncompressible. The more compressible string is, the more causal structure can be found in it, the less random it is.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$K(S)$ is uncomputable. To find the shortest program $P$ generating $S$ we have to enumerate all programs an evaluate then in a diagonal manner. Not all programs will halt eventually, and for any fixed moment in time we can\u0026rsquo;t know if some program $Pi$ is still computing $S$ or have already hanged. In other words, to get exact value of $K(S)$ we need to solve halting problem that is infeasible unless we have supercomputations. Best method to find an approximation of $K(S)$ is \u003ca href="http://www.scholarpedia.org/article/Universal_search"\u003eUniversal Search\u003c/a\u003e that\u0026rsquo;s is a kind of enumeration of running programs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNevertheless, $K(S)$ is \u003cstrong\u003esemicomputable\u003c/strong\u003e or \u003cstrong\u003ecomputable form above\u003c/strong\u003e in an any-time fashion: for any time $T$ we will have set of programs ${P_i}$ already completed. Just find the shortest one $P_i$ generating $S$. With bigger time, the shorter program can be found, if it exists. The only problem that we never know how close our current estimation of complexity of $S$ to its Kolmogorov complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKolmogorov complexity of a string S in non-reducible. It can\u0026rsquo;t be reduced by means of any finite transformation of S. Complex things will always be complex. That means there is no \u0026ldquo;silver bullet of programming\u0026rdquo; \u0026ndash; so advanced universal programming language that all programs become simple by being written in it.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNevertheless, we can have \u003cem\u003edomain-specific\u003c/em\u003e languages or DSLs that make some limited subset of problems described much shorter than others.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSubjective simplicity has one-sided correspondence with Kolmogorov complexity: complex strings will always appear subjectively complex, simple strings can appear both subjectively simple and complex. The depends on how actual structures in the string fit the set of structures our brain can recognize.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href="https://books.google.com/books?id=JvXiBwAAQBAJ\u0026amp;pg=PA96\u0026amp;lpg=PA96\u0026amp;dq=incompressibility+theorem\u0026amp;source=bl\u0026amp;ots=yPy044Hkmq\u0026amp;sig=XjB49Gc-8FZc0dry4nBYj1T6Myw\u0026amp;hl=ru\u0026amp;sa=X\u0026amp;ved=0ahUKEwiMooPQ7NjYAhWp7oMKHdGpAJgQ6AEIKDAA#v=onepage\u0026amp;q=incompressibility%20theorem\u0026amp;f=false"\u003eIncompressibility theorem\u003c/a\u003e states that simple, or well-compressible, strings are rare. Most strings are uncompressible. This theorem suggests that arbitrarily selected object \u003cem\u003eO\u003c/em\u003e will have high Kolmogorov complexity, and most of such objects will be just random.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConditional complexity $K(S1 | S2)$ measures how much of new information there is in $S1$ relative to $S2$. Or, it\u0026rsquo;s the length shortest program computing $S1$ by given $S2$. \u003ca href="https://en.wikipedia.org/wiki/Normalized_compression_distance"\u003eNormalized information distance\u003c/a\u003e (NID) is a generalization of this principle and can measure similarity between arbitrary strings in the most universal way. An approximation of NID is a Normalized Compression Distance (NCD). Simple form of NCD is \u003ca href="https://en.wikipedia.org/wiki/Levenshtein_distance"\u003eLevenshtein Distance\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUniversal Search is theoretically optimal but infeasible method of estimating complexity of $S$ for everything except toy problems, because the space of programs is incredible large. Nevertheless, there are suboptimal but much more feasible methods:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/Genetic_programming"\u003eGenetic Programming\u003c/a\u003e and \u003ca href="https://en.wikipedia.org/wiki/Meta-optimization"\u003eMetaoptimization\u003c/a\u003e. Here we restrict both both sample program complexity and search space by \u003cstrong\u003eexploiting\u003c/strong\u003e certain properties of programs. When exploitation is not possible, these methods resort to random walk in the space of programs that is slower than Universal Search (because we can visit some programs many times).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eData/Image compression, both lossless and lossy. Pretty complete method of compression where we seriously restrict complexity of programs. For \u003ca href="https://en.wikipedia.org/wiki/LZ77_and_LZ78"\u003eexample\u003c/a\u003e to probabilistic finite automation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSoftware programming. We are looking for a program with certain properties (size, execution time) compactly describing certain complex domain, and our brain is doing it in a very efficient, though still not complete way. Unfortunately, we still don\u0026rsquo;t know exactly how it\u0026rsquo;s doing this search.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMachine Learning. For example, object classification. Instead of doing compression of $S$ in order to decompress it back again, we are looking for some model $M$ discriminating string $S$ from other strings from certain set ${S}$. Though classifier complexity is not necessary depend on complexity of $S$, it depends on complexity of the border between classes strings $S$ belong to. For a multi-layer perceptron this border is approximated with hyper-planes, more complex border requires more hyper-planes to describe it exactly. So we are looking for pretty simple models $M$ still having good classification accuracy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the last case (ML) we are not looking explicitly for the shortest dicriminative or generative models ever. It\u0026rsquo;s enough if the model fits our computational budgets. Nevertheless, program length can be exploited when we are talking about probabilistic prediction. Solomonoff\u0026rsquo;s \u003ca href="http://www.scholarpedia.org/article/Algorithmic_probability"\u003eAlgorithmic Probability\u003c/a\u003e can be used as universal prior for Bayesian sequence prediction like it\u0026rsquo;s done in Hutter\u0026rsquo;s \u003ca href="https://en.wikipedia.org/wiki/AIXI"\u003eAIXI\u003c/a\u003e agent.\u003c/p\u003e\n\u003cp\u003eThe idea behind Algorithmic Probability (ALP) is simple: we assign (exponentially) higher probabilities to shorter programs. Then, algorithmic probability of a string $S$ is a sum over probabilities of all programs $P_i$, computing $S$ as output. This does not corresponds directly to probabilities of physical events, but nevertheless may have some connections to higher-level cognition.\u003c/p\u003e\n\u003cp\u003eConsider a hypothetical physical experiment. Let\u0026rsquo;s we have a true random number generator generating binary sequences with uniform distribution. And in the first experiment this RNG produces the long, say 1024 symbols, series of zeroes. According to normal distribution, probability of such event will be so low, it unbelievable it can happen. Nevertheless, such unusual output of an RNG is a valid output. The reason why we are usually confused with such experimental results is that we implicitly assign higher subjective probabilities to events we can recognize some structure in. This and many other related phenomena are trying being generalized in \u003ca href="https://simplicitytheory.telecom-paristech.fr/"\u003eSimplicity Theory of Mind\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eProminent result of Solomonoff\u0026rsquo;s Alogrithmic Probability that it\u0026rsquo;s a complete and universal method for machine learning and it\u0026rsquo;s at the same time not finitely computable. Moreover, neither feasible machine learning method can be compete in this sense because in order to achieve completeness, we need supercomputations. Incompleteness means that for an arbitrarily given string, we can\u0026rsquo;t find and exploit for prediction all potential regularities it contains. Fortunately, it\u0026rsquo;s not always necessary in practice for a ML method to be complete in a strict sense. Incompleteness of ALP just means that true AGI is not finitely computable, and all feasible AGIs, like human-level AI, will be somewhat incomplete.\u003c/p\u003e\n\u003cp\u003eAnother interesting practical property of ALP is that it\u0026rsquo;s well-approximable \u003ca href="https://en.wikipedia.org/wiki/Ensemble_learning"\u003eensemble method\u003c/a\u003e. If we truncate the sum over all possible models of $S$ to just one shortest model, ALP will still perform well in many cases. This method is called \u003ca href="http://www.scholarpedia.org/article/Minimum_description_length"\u003eMinimum Description Length\u003c/a\u003e principle. From another side, we can restrict the model class we will be looking for simplest models of $S$. For example in \u003ca href="https://arxiv.org/abs/0909.0801"\u003eMC-AIXI\u003c/a\u003e authors propose to approximate ALP with ensemble over variable order Markov models computed via \u003ca href="https://en.wikipedia.org/wiki/Context_tree_weighting"\u003eContext Tree Weighting\u003c/a\u003e algorithm.\u003c/p\u003e\n\u003cp\u003eIn spite of so appealing properties of ALP and derived ensemble methods like Solomonoff\u0026rsquo;s Induction and AIXI, it\u0026rsquo;s not that easy to apply it to, for example, human-level intelligence. There are two main problems.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBootstrap problem\u003c/strong\u003e or \u003cstrong\u003einitial training sequence problem\u003c/strong\u003e. In order ALP-based methods to work efficiently on practical problems we have to supply initial set of models either directly, or in the form of initial training sequence. It\u0026rsquo;s not that easy to determine such set of models for human brain. According to decent results, human brain is described with several tens of megabits of DNA code \u0026ndash; the Kolmogorov complexity of our brain. The models we are looking for are implicitly encoded in this DNA\u0026rsquo;s part. It seems that even if several megabits of this code is sufficient, this task may have unpredictable complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eIncremental learning problem\u003c/strong\u003e. Even if we already selected basic machine and identified bootstrapping set of models, we need an approximation of Universal Search to infer new models from data. Though this process is well-defined mathematically, it\u0026rsquo;s not that easy to implement it in practice, because program space is too big for exhaustive search. In order to achieve desired performance we have to restrict this space somehow, either by restricting model class (as in MC-AIXI) or by using various heuristics as in Genetic Programming and Metaoptimization, or by a database of declarative and procedural restrictions applied to the search space. The latter case is the most flexible and the most complex one because such database may be very big in size even for simplest problems.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhat is important, these two problems can be developed independently, by different teams focused on their problems. This the way to go for Memoria. It will be focused on \u003cstrong\u003ecompressed data structures\u003c/strong\u003e, where compression is understood not just in space-saving context, by as an enabler of generic intelligence. Additionally, Universal Search approximations are very important for general intelligence.\u003c/p\u003e\n\u003cp\u003eHigh-level ideas behind ALP was informally explained by \u003ca href="https://www.youtube.com/watch?v=nXrTXiJM4Fg"\u003eMarvin Minsky\u003c/a\u003e. In short, Solomonoff\u0026rsquo;s approach to AGI is a top-down approach, when solid theoretical framework of AI is established first, and then we derive custom AIs with specific properties using approximation techniques. As opposite, classical bottom-up approach is playing around with different substances being inspired either from neuroscience or from cognitive science and waiting for intelligence to emerge in experiments. They can work together very well, as custom AIs are developed in a bottom-up experimental way and then merged into more powerful ensemble in top-down way by Solomonoff\u0026rsquo;s Induction.\u003c/p\u003e\n\u003ch2 id="self-referential-machine"\u003eSelf-Referential Machine\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s just a language to describe certain computational phenomena succinctly, there is nothing special from computational perspective.\u003c/p\u003e\n\u003ch2 id="psychology-basics"\u003ePsychology Basics\u003c/h2\u003e\n\u003cp\u003ePsychological definitions of higher mental functions are not suitable for AI because they intermix subjective and objective planes, that leads to controversies. Better system of definitions is necessary.\u003c/p\u003e\n\u003ch2 id="emotional-intelligence"\u003eEmotional Intelligence\u003c/h2\u003e\n\u003cp\u003eWhat it is and how it works from AIT\u0026rsquo;s perspective. Rational intelligence as a custom case of emotional one, and not as a separate system. Rationality is illusion.\u003c/p\u003e\n\u003ch2 id="hard-problems-of-ai"\u003eHard Problems of AI\u003c/h2\u003e\n\u003ch2 id="process-introspection"\u003eProcess Introspection\u003c/h2\u003e\n\u003cp\u003eProcess introspection \u003ca href="https://medium.com/@victorsmirnov/how-to-compensate-introspection-illusion-62f357e9326c"\u003ecan be improved.\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="strong-ai-is-a-basic-right"\u003eStrong AI is a Basic Right\u003c/h2\u003e\n\u003cp\u003eStrong AI is about us, not about machines that think and feel.\u003c/p\u003e\n\u003ch2 id="evolutionary-strategy-for-agi"\u003eEvolutionary Strategy for AGI\u003c/h2\u003e\n'}).add({id:11,href:"/docs/mbt/overview/",title:"MBT Overview",description:"",content:""}).add({id:12,href:"/docs/tests/overview/",title:"Tests -- Overview",description:"",content:""}).add({id:13,href:"/docs/datascope/overview/",title:"Datascope Overview",description:"",content:""}).add({id:14,href:"/docs/storage/",title:"Dbfs",description:"",content:""}).add({id:15,href:"/docs/tests/",title:"Tests List",description:"",content:""}).add({id:16,href:"/docs/d-phil/",title:"AI \u0026 ML List",description:"",content:""}).add({id:17,href:"/docs/data-zoo/",title:"Containers List",description:"",content:""}).add({id:18,href:"/docs/mbt/",title:"MBT",description:"",content:""}).add({id:19,href:"/docs/overview/",title:"Overview List",description:"Overview List",content:""}).add({id:20,href:"/docs/datascope/",title:"Datascope List",description:"Datascope List.",content:""}).add({id:21,href:"/docs/",title:"Docs",description:"Docs Memoria.",content:""}),search.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()