<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://memoria-framework.dev/main.4aee7b9c653a9441aeccbab4bf9a20bb4827ed71a21077328be36d74cbe63081492792ee312478e64d73f5e762a2ad3c04431ecfa8d16da63373639763e86b0c.css integrity="sha512-Su57nGU6lEGuzLq0v5ogu0gn7XGiEHcyi+NtdMvmMIFJJ5LuMSR45k1z9edioq08BEMez6jRbaYzc2OXY+hrDA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Hybrid AI - Memoria</title><meta name=description content><link rel=canonical href=https://memoria-framework.dev/docs/applications/aiml/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Hybrid AI"><meta property="og:description" content="What is &ldquo;Hybrid AI&rdquo; in Memoria? Historically, by Hybrid AI people meant something related to Khaneman&rsquo;s Dual process theory or any combination of &ldquo;intuitive reasoning&rdquo; (shallow, fast and wide System 1) and &ldquo;symbolic reasoning&rdquo; (deep, slow and narrow System 2) that are expected to complement each other. LLMs turned out to be well-hybridizable with many different technologies, not limited to symbolic reasoners and databases. So, interests in ANNs is fueling secondary interest in technologies that previously have been resting in an oblivion."><meta property="og:url" content="https://memoria-framework.dev/docs/applications/aiml/"><meta property="og:site_name" content="Memoria"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Hybrid AI"><meta name=twitter:description content><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Hybrid AI"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/1","name":"Memoria","url":"https://memoria-framework.dev/","sameAs":[],"image":{"@type":"ImageObject","@id":"https://memoria-framework.dev/#/schema/image/1","url":"https://memoria-framework.dev/\u003cnil\u003e","width":null,"height":null,"caption":"Memoria"}},{"@type":"WebSite","@id":"https://memoria-framework.dev/#/schema/website/1","url":"https://memoria-framework.dev/","name":"Memoria","description":"","publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"}},{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/applications/aiml/","url":"https://memoria-framework.dev/docs/applications/aiml/","name":"Hybrid AI","description":"","isPartOf":{"@id":"https://memoria-framework.dev/#/schema/website/1"},"about":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"https://memoria-framework.dev/docs/applications/aiml/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://memoria-framework.dev/docs/applications/aiml/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://memoria-framework.dev/docs/applications/aiml/"]}]},{"@type":"BreadcrumbList","@id":"https://memoria-framework.dev/docs/applications/aiml/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/","url":"https://memoria-framework.dev/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/","url":"https://memoria-framework.dev/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/applications/","url":"https://memoria-framework.dev/docs/applications/","name":"Applications"}},{"@type":"ListItem","position":4,"item":{"@id":"https://memoria-framework.dev/docs/applications/aiml/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://memoria-framework.dev/#/schema/article/1","headline":"Hybrid AI","description":"","isPartOf":{"@id":"https://memoria-framework.dev/docs/applications/aiml/"},"mainEntityOfPage":{"@id":"https://memoria-framework.dev/docs/applications/aiml/"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","author":{"@id":"https://memoria-framework.dev/#/schema/person/2"},"publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"image":{"@id":"https://memoria-framework.dev/docs/applications/aiml/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/2","name":"Victor Smirnov","sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://memoria-framework.dev/docs/applications/aiml/#/schema/image/2","url":null,"contentUrl":null,"caption":"Hybrid AI"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://memoria-framework.dev/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://memoria-framework.dev/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://memoria-framework.dev/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://memoria-framework.dev/site.webmanifest></head><body class="docs single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=/ aria-label=Memoria>Memoria</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-start border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>Memoria</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/docs/overview/introduction/>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/subprojects/overview/>Subprojects</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/issues>Issues</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/discussions>Discussions</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/wiki>Releases</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li></ul></div></div></nav></header><nav class="doks-subnavbar py-2 sticky-lg-top" aria-label="Secondary navigation"><div class="container-xxl d-flex align-items-md-center"><form class="doks-search position-relative flex-grow-1 me-auto"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><button class="btn doks-sidebar-toggle d-lg-none ms-3 order-3 collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle documentation navigation"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></button></div></nav><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=false>
Overview</button><div class=collapse id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=true>
Applications</button><div class="collapse show" id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=false>
Overview</button><div class=collapse id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=true>
Applications</button><div class="collapse show" id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#what-is-hybrid-ai-in-memoria>What is &ldquo;Hybrid AI&rdquo; in Memoria?</a></li><li><a href=#probabilistic-lm-based-hybridization>Probabilistic LM-based Hybridization</a></li><li><a href=#hybrid-and-approximate-reasoning>Hybrid and Approximate Reasoning</a></li><li><a href=#associative-memory-for-llm>Associative memory for LLM</a></li><li><a href=#mc-aixi-ctw>MC-AIXI-CTW</a></li><li><a href=#hardware>Hardware</a></li><li><a href=#software>Software</a></li><li><a href=#beyond-reasoning>Beyond Reasoning</a></li><li><a href=#memoria-as-a-dataset>Memoria as a Dataset</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Hybrid AI</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#what-is-hybrid-ai-in-memoria>What is &ldquo;Hybrid AI&rdquo; in Memoria?</a></li><li><a href=#probabilistic-lm-based-hybridization>Probabilistic LM-based Hybridization</a></li><li><a href=#hybrid-and-approximate-reasoning>Hybrid and Approximate Reasoning</a></li><li><a href=#associative-memory-for-llm>Associative memory for LLM</a></li><li><a href=#mc-aixi-ctw>MC-AIXI-CTW</a></li><li><a href=#hardware>Hardware</a></li><li><a href=#software>Software</a></li><li><a href=#beyond-reasoning>Beyond Reasoning</a></li><li><a href=#memoria-as-a-dataset>Memoria as a Dataset</a></li></ul></nav></div></nav><h2 id=what-is-hybrid-ai-in-memoria>What is &ldquo;Hybrid AI&rdquo; in Memoria?<a href=#what-is-hybrid-ai-in-memoria class=anchor aria-hidden=true>#</a></h2><p>Historically, by Hybrid AI people meant something related to Khaneman&rsquo;s <a href=https://en.wikipedia.org/wiki/Dual_process_theory>Dual process theory</a> or any combination of &ldquo;intuitive reasoning&rdquo; (shallow, fast and wide System 1) and &ldquo;symbolic reasoning&rdquo; (deep, slow and narrow System 2) that are expected to <em>complement</em> each other. LLMs turned out to be well-hybridizable with many different technologies, not limited to symbolic reasoners and databases. So, interests in ANNs is fueling <em>secondary</em> interest in technologies that previously have been resting in an oblivion.</p><p>In Memoria, the meaning of this term is slightly different (but not contradicting). Memoria project follows the intuition that there is no any specific &ldquo;secret&rdquo; in human intelligence in particular, and in intelligence in general: at the end of the day (after all possible <em>optimizations</em> have been applied) it&rsquo;s all about resources &ndash; <em>compute</em> and <em>memory</em>. This position should not be confused with Sutton&rsquo;s <a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html>The Bitter Lesson</a>. While both are similar in wording and final resolutions, Memoria implies that <em>resources are always limited</em>, and that makes huge difference: <em>problem-specific optimizations do matter</em>. Ultimately:</p><ul><li>If there is some algorithm or mathematical method, or data structure that can reduce computational complexity of AI, it&rsquo;s worth using.</li><li>If there is a custom hardware architecture that can improve raw performance <em>and</em>/<em>or</em> performance per watt, it&rsquo;s worth using.</li><li>If there is some <em>physical process</em> that we can utilize to improve performance characteristics, it&rsquo;s worth considering.</li><li>Quantum supremacy? Perfect!</li><li>If we can <a href=https://medium.com/@victorsmirnov/how-to-compensate-introspection-illusion-62f357e9326c>improve our introspection</a> and get some bits about inner machinery of mind that may help us to achieve better <em>human-likeness</em> of AI, let&rsquo;s do it!</li><li>Any useful bits form other disciplines are always welcome!</li></ul><p>Memoria is grounded in <a href=https://en.wikipedia.org/wiki/Algorithmic_information_theory>Algorithmic Information Theory</a> and it&rsquo;s compression-based approach to AI. From this perspective, Systems 1 and 2 are just different <em>compressibility domains</em>. System 2 corresponds with highly-compressible domain, and System 1 corresponds with low-compressible domain. Traditional for programming distinction to <em>algorithms</em> and <em>data structures</em> has the same nature.</p><p>There may be many more compressibility domains than just two, so, potentially, we may have System 1&mldr;N in our AI architecture, where N is pretty large. Even within the same complexity domain there are many <em>sub-domains</em>, so methods like <a href=https://en.wikipedia.org/wiki/Mixture_of_experts>Mixture of Experts</a> and <a href=https://en.wikipedia.org/wiki/Ensemble_learning>Ensemble learning</a> are efficient. These methods work across even distant domains too, it&rsquo;s just a technical question how to make it working efficiently. Those technical questions are in the focus of Memoria.</p><p>In Memoria, by &ldquo;Hybrid AI&rdquo; it&rsquo;s meant an architecture spawning multiple different compression domains.</p><h2 id=probabilistic-lm-based-hybridization>Probabilistic LM-based Hybridization<a href=#probabilistic-lm-based-hybridization class=anchor aria-hidden=true>#</a></h2><p>A probabilistic language model is simply a probability distribution over a set of strings $S$ representing texts in this language: $P(S)$, where $S = w_0w_1w_2&mldr;w_i$ &ndash; is a sequence of text elements (usually, <em>tokens</em>). Probabilistic models are used by <em>sampling</em> (generating elements) from them. For sampling from language models we may use <em>autoregressive</em> schema by sampling form <em>conditional distribution</em> $P(w_i|w_{i-1}&mldr;w_0)$ &ndash; probability of a next element in the string given its prefix.</p><p>Autoregressive sampling means that we generate a string in an element by element, left-to-right way, each time appending newly sampled elements to the prefix. Additional techniques, like <a href=https://en.wikipedia.org/wiki/Beam_search>beam search</a>, may be used to increase the probability value of the generated string. Autoregressive sampling gives us one additional important feature: we can sample strings that are <em>continuations</em> of a given prefix, that is called a <em>prompt</em>.</p><p>The language model has to be created somehow, and the simplest way is to learn the model inductively from the set of strings drawn from the real language. There are a lot of scientific and technical challenges here, but, basically, there are three different notable approaches: statistical <a href=https://en.wikipedia.org/wiki/Word_n-gram_language_model>n-gram-based</a>, <a href=https://en.wikipedia.org/wiki/Large_language_model>NN-based</a> and <a href=https://arxiv.org/abs/0909.0801>VMM-based</a>. In all cases we feed the mode a corpus of strings and expect it to predict those (seen) strings correctly. What we do want from the model is to predict correctly the <em>unseen</em> strings. In ML they call it <em>generalization</em>. When we are solving AI problems with ML, this is where al the &lsquo;magic&rsquo; happens.</p><p>It have turned out that some very large neural language models (LLM) can generalize so well over natural language that they can solve some logical and mathematical problems, follow instructions, reason about some emotions and mental states (sic!), write program code, translate from one language to another, change style of a text, summarize/elaborate and maintain conversation &ndash; all from the natural language (e.g.: English).</p><p>Of course LLMs aren&rsquo;t doing <em>everything</em> good enough (at the expert human-level), they are making a lot of hard to recognize and fix mistakes that is seriously limiting their practical suitability. They are pretty good at tasks in <em>low-compressible domain</em>: translation, style transfer, summarization and elaboration, and some others. The reason is probably that in low-compressible domains the role of generalization isn&rsquo;t that high and the model size/scale <a href=https://arxiv.org/abs/2001.08361>is all that ultimately matters</a>.</p><p>In highly compressible domains like basic math, logic puzzles, constraint solving, board games, database query execution and logical inference &ndash; generalization matters, but generalizability depends on many factors. The most important of them are training <em>data quality</em>, <em>model architecture</em> and <em>learning algorithms</em>. Both model architecture and learning algorithms are fixed for neural LM. There is no way single architecture may be good for everything, one may even say that NFL theorem prohibits this. There some indirect evidence that effects behind In-Context Learning in Transformers <em>may</em> help models to adapt to specific narrow tasks like basic arithmetic beyond what would be expected from the architecture alone. But those effects are severely limited. Basically, no amount of scaling can make a database engine out of a neural network.</p><p>Actually, the latter isn&rsquo;t an issue if we want to achieve HXL-AI (Human-Expert Level AI), because humans aren&rsquo;t that good at symbolic tasks either. The point is that <em>mere scaling</em> of LLMs is a wrong direction. Instead, we need to identify certain domains where scaling doesn&rsquo;t work but there can be different solutions, and provide custom solvers &ndash; arithmetic, logical reasoning, constraint solving&mldr; and so on. A relatively small but fine-tuned LLM may be used here to pre-process the input, find narrow formal problems in it, invoke corresponding solvers and then post-process the result. Text in a mixture of natural language and structured formats can be seen as <em>Intermediate Representation</em> for this type of hybrid AI.</p><p>Using LLMs as a human-like interface to various problem solvers greatly increases their exposure to the potential audience. Problem solvers, despite having great potential <em>value</em> are pretty hard to use directly.</p><h2 id=hybrid-and-approximate-reasoning>Hybrid and Approximate Reasoning<a href=#hybrid-and-approximate-reasoning class=anchor aria-hidden=true>#</a></h2><p>By &ldquo;reasoning&rdquo; we mean what is usually meant by &ldquo;declarative problem solving&rdquo;: logic (of various types), constraint solving, SAT/SMT, theorem proving, planning and many other types of combinatorial problem solving (CPS). CPS was initially meant as a main purpose of AI because of the <em>value</em> it creates. It literally solves problems and makes our life <em>better</em>. But there are two main obstacles:</p><ol><li>CPS is rather hard to set up and use: &ldquo;you need a PhD for that&rdquo;.</li><li>CPS is <em>very slow</em>. Many important problems are out of our reach.</li></ol><p>Complexity of CPS methods may be addressed with specialized LLMs, translating from problem description obtained in a dialogue with a user to structured problem representation. Right now (2024) it doesn&rsquo;t work well in many ways, a lot of improvements of are still ahead. But at least this specific direction looks feasible and economically viable.</p><p><em>Computational complexity</em> of CPS is mach harder to solve issue because, basically, there is no workaround. Logic is computationally complex. If we augment an LLM with a reasoner that will be logical parts of the query, it may take practically infinite time for even apparently simple problems. Inference time is rather hard to predict.</p><p>Approximate reasoning may be useful in <em>some</em> cases. We, humans, aren&rsquo;t perfect in logic and other types of CPS either, but that&rsquo;s OK. Especially if there are ways to <em>improve</em> a partial or approximate solution. There are two main ways to implement approximate CPS:</p><ol><li>Heuristic (ex: greedy) methods.</li><li>Trading speed for memory.</li></ol><p>Heuristic methods are some statistically and <em>statically</em> inferred rules that we can use to reduce computational complexity of a CPS method and produce a &ldquo;very good&rdquo; result in many (but not most!) cases.</p><p>Trading speed for memory (TSM) is a large family of computational complexity reducing methods, with dynamic programming as a famous example. TSM may also be used for saving energy, if energy costs of storing and retrieving a solution is lower than costs or recomputing it.</p><p>TSM can also be viewed as a heuristic method with an <em>unbound</em> number of heuristics, so we accumulate useful heuristics in memory as soon as they help reducing computational complexity (even at the expense of precision). Example: heuristic instance-based reasoning.</p><p>The challenge is that the number of instances/heuristics/solutions stored in memory and their descriptional complexity may be pretty large. Specifically for that, Memoria provides highly-functional solutions: advanced data structures, query engines, possibility of hardware acceleration, integrated storage stack from bare metal to high-level computing, decentralisation and many other useful features.</p><h2 id=associative-memory-for-llm>Associative memory for LLM<a href=#associative-memory-for-llm class=anchor aria-hidden=true>#</a></h2><p>One of the most well-known ways of augmenting an LLM with external memory is <a href=https://arxiv.org/abs/2312.10997>RAG</a>. Here, simply speaking, a prompt is translated into one or more queries to external data sources like web pages and databases. Retrieved result is summarized and returned to the user. Another way is to augment transformer with <a href=https://arxiv.org/abs/2407.01178><em>explicit</em> memory</a>. By doing this, authors have reported significant reduction in effective model size required for the same level of performance.</p><p>Implicit or <em>parameters</em> memory of LLMs is very expensive: computational costs are in the other of O(N) where N is a number of parameters. The number of parameters itself is quadratic from the number of structural units in attention and fully connected layers. This does not scale well, especially when a model generalizes poorly for objective reasons and needs to memorize more.</p><p>From other side, database technology provides searchable spatial data structures with logarithmic (on average) lookup time complexity. Memoria has specially designed <a href=/docs/data-zoo/associative-memory-2>associative memory</a> &ndash; multiary relation complying with Paul Smolensky&rsquo;s <a href=https://www.researchgate.net/publication/360353836_Neurocompositional_computing_From_the_Central_Paradox_of_Cognition_to_a_new_generation_of_AI_systems>requirements for compositional neuro-symbolic representations</a>. Unlike traditional database technologies where relations link <em>points</em> together, associative memory links together <em>sub-volumes</em>. And a point is a special case of unit volume. Like with neural networks splitting space with hyper-planes, associative memory splits space with volumes, and infinitely many actual data points may fit into a single volume.</p><p>Given those &lsquo;hybrid&rsquo; properties of Memoria&rsquo;s associative memory, it&rsquo;s a much better candidate for using with connectionist ML than classical graphs- and relations-based data structures (like classical RDF-like knowledge graphs).</p><p>Running complex queries over classical relational and graph data is a costly process, both in terms of memory and compute. Querying &lsquo;hybrid&rsquo; advanced data structures like associative may be even more costly, because we need to use <em>sampling</em>-like algorithms for that. While it&rsquo;s nothing special from algorithmic perspective, we do need a specialized hardware for achieving maximal efficiency. Memoria Acceleration Architecture (<a href=/docs/overview/accel>MAA</a>) may use associative memory as one of its design and performance targets.</p><h2 id=mc-aixi-ctw>MC-AIXI-CTW<a href=#mc-aixi-ctw class=anchor aria-hidden=true>#</a></h2><p><a href=https://en.wikipedia.org/wiki/AIXI>AIXI</a> is a theoretical mathematical formalism for artificial general intelligence. It&rsquo;s a reinforcement learning <em>agent</em>, maximizing the expected total reward received from the environment. AIXI is a clever <em>mathematical trick</em> that is based on so-called <a href=https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior>Universal Prior</a> (UP). It&rsquo;s universal, because it already contains all possible solutions for all problems packed into a format of a <em>probability distribution</em>. AIXI is an ultimate, universal RL-based agent, but it&rsquo;s uncomputable. So, it&rsquo;s not feasible in its ultimate form. Navertheless it&rsquo;s a simple and elegant formalism demonstrating how very different algoritms can be get working together as a single holistic system, by reducing everything to probabilistic string prediction. Auto-regressive LLMs are also just predicting next token in a text, but a lot of &lsquo;magic&rsquo; implicitly happens behind the scene.</p><p>AIXI is infeasible to implement, but surprisingly it can be <em>approximated</em>. One of such known approximations is <a href=https://arxiv.org/abs/0909.0801>MC-AIXI-CTW</a>. It approximates Universal Prior with <a href=https://en.wikipedia.org/wiki/Variable-order_Markov_model>variable-order markov models</a> (VMM), represented as <em>trees</em>. For unknown string probability estimations it uses <a href=https://en.wikipedia.org/wiki/Context_tree_weighting>Context Tree Weighting</a> method.</p><p>What is interesting about MC-AIXI-CTW, is that:</p><ol><li>It&rsquo;s based on a language model, backed with VMMs. Very much like with NN-bassed LLMs, intelligence is proportional to the model&rsquo;s abilities to estimate probability of unknown strings correctly. This is what we call &lsquo;generalization&rsquo; of a model.</li><li>It&rsquo;s an <em>agent</em> acting in a <em>environment</em> according to some RL <em>policy</em>. So, ulike a raw LLM, it&rsquo;s an almost-ready-to-use AI.</li><li>VMM, implemented as a tree, is <strong>much easier interpretable and hybridizable</strong> than a neural network.</li><li>VMM is a <em>database</em>, requiring latency-optimized architecture. And can be an interesting benchmark for <a href=/docs/overview/accel/>MAA</a>.</li></ol><p>Unfortunately for AIXI approximations, they have been lost in the shade of DL revolution in 2010th. Now, with broad interest resurrecting to many previously forgotten AI approaches, AIXI may see its second life. What we do need here is <em>specialized hardware</em> (and related software) to accelerate this type of probabilistic models.</p><h2 id=hardware>Hardware<a href=#hardware class=anchor aria-hidden=true>#</a></h2><p>Neural network is just a bunch of dense arrays &ndash; pretty simple <em>data structures</em>. Matrix multiplication generates simple and predictable memory access pattern. Computations can be easily scheduled statically ahead-of-time and at the scale of an entire cluster.</p><p>In case of Hybrid AI we need full set of hardware architectures, optimized for <em>static</em> and <em>dynamic</em> parallelism, optimized for minimizing memory access latency and maximizing throughput. There is no way to provide a single capable architecture. Instead, we need a constructor to build an architecture, specialized for a specific problem class. <a href=/docs/overview/accel/>Memoria Accelerated Architecture</a> (MAA) is addressing this issue.</p><h2 id=software>Software<a href=#software class=anchor aria-hidden=true>#</a></h2><p>NN-oriented ML frameworks are relatively simple. Neural network is a bunch of dense arrays, the only fundamental data structure we need. We also need powerful optimizing compiler converting data-flow graph of a program into sequence of computational kernels invocation, handling alos data flow in the process. Specifics of most neural networks is that computations have highly regular and predictable data flow, so there is a lot of opportunity for static-time optimizations, even claster-wide.</p><p>Symbolic AI, explicitly or implicitly, relies on some <em>search</em> technique is <em>state space</em>. So, <em>represetation</em> of the state space becomes crucial. The state space may be huge and highly irrecular requiring large complex data structures and software and hardware that can leverage <em>dynamic parallelism</em>.</p><p>Sufficiently general <em>reasoning engine</em> is much more complex than an advanced relational DBMS and includes it. <a href=https://en.wikipedia.org/wiki/Relational_algebra>Relational algebra</a> is a derivative of <a href=https://en.wikipedia.org/wiki/Relational_calculus>relational calculus</a> &ndash; that is a tractable subset of <a href=https://en.wikipedia.org/wiki/First-order_logic>first-order logic</a> (FOL). RDBMS sacrifice expressiveness of FOL for predictable performance, they also lose the <em>deductive</em> component &ndash; ability to infer <em>new facts</em> from <em>existing ones</em>. Reasoning engines may generate large intermediate state and/or results and operate on large datasets, so building it on top of a powerful and generic query engine is essential.</p><p>Memoria Framework provides necessary basic and advanced elements for building standalone and hybrid reasoning engines. The main elements of the stack are:</p><ol><li><a href=/docs/overview/hermes>Hermes</a> data format, deeply ontegrated with the rest of the framework, including hardware acceleration at the level of MAA if/when necessary. Hermes is flexible enough to support all the necessary structured formats like knowlege graphs, no thierd-party libraries are needed.</li><li>Hermes-based <a href=/docs/overview/hrpc>HRPC</a> accelerated communication protocol.</li><li>Rich set of trivial and advanced <a href=/docs/overview/containers>data structures</a>. No external database or storage system is ever needed.</li><li>Accelerated query <a href=/docs/overview/vm>execution engine</a>. Turing complete superset of SQL/Datalog and beyond. Backward chaining mode (query execution) and <a href=https://en.wikipedia.org/wiki/Rete_algorithm>RETE</a>-based forward-chaining engine for event-driven computations: robotics, agents, embedded systems and <a href=/docs/applications/eiot>IoT</a>.</li><li>Computational <a href=/docs/applications/storage>storage layer</a>. Memoria fully <a href=/docs/overview/accel>redefines</a> storage and processing stacks comparing to a traditional ones, based on CPUs and monolithic operating systems with integrated complex storage layers (file systems). Complex distributed and heterogeneous architectures become much simpler.</li></ol><h2 id=beyond-reasoning>Beyond Reasoning<a href=#beyond-reasoning class=anchor aria-hidden=true>#</a></h2><p>LLMs are considered to be <em>language</em> models. Language models trained on human-generated textual data are <em>much more</em> than just that. They are capturing not only the language itself, but also <em>functional approximations</em> of higher mental functions, including <em>agency</em>. Conversation with a sufficiently powerful and properly-tuned LLM looks and feels like a conversation with a real well-educated person with encyclopedic knowledge. Apparent human-likeness of conversations with LLMs is impressive, especially when we take into account implicit functional aspects of it (intuitive understanding), but it should not be deceiving. LLMs do not <em>feel</em> or really <em>experience</em> what they say <em>the way we do</em>: generative processes in LLMs are sufficiently different from ours. We should not attribute any of our <em>mental states</em> to them (we may do it, but only with great precaution).</p><p>Despite &lsquo;mental states&rsquo; of LLMs (if any) are very different from ours, their <em>observable effects</em> are consistent with corresponding effects of our mental states (providing that there is enough training data). This is <em>the</em> reason why it <a href=https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/>feels so human-like</a> in conversations. Certain higher mental functions are <a href=https://en.wikipedia.org/wiki/Hard_problem_of_consciousness>known to be really hard</a> to formalize, they are nevertheless very important for human-machine integration. This entire topic has been considered largely uninteresting in AI/ML community. But recent second dawn of LLM-based <a href=https://en.wikipedia.org/wiki/Multi-agent_system>multi-agent systems</a> have brought old question to the table again. There is much more to human cognition than just &lsquo;reasoning&rsquo; and &lsquo;learning&rsquo;, that is may be just 1% of all mental activity related to the problem solving. If we want our computational agents be integrateable into human-centric interactions, or humans be integrable into societies of computational agents, both parties need to <em>understand</em> each others at the emotional, intuitive, unconscious levels.</p><p>Capturing mental states and higher mental functions in LLMs via machine learning has been already proven efficient, but the real problem is that internal state of black box ML models is hardly interpretable in terms of external objects those models interact with. LLMs work, until they don&rsquo;t. And if they don&rsquo;t, there is no specific way to fix it. Moreover, textual datasets are extremely <a href=https://en.wikipedia.org/wiki/Skewness>skewed</a> at textual descriptions of mental states and higher mental functions, seriously limiting LLM&rsquo;s abilities to reason about them. Mental states describing external <em>objects</em> are very well represented in textual data. The same is true for certain emotional state, but now their descriptions depend on some context, introducing <em>subjectivity</em> into interpretations.</p><p>Certain important mental states have no textual expression at all. They may be not even <em>reportable</em>. One of foundational questions that may trick and freak programmers is &ldquo;How do <em>you</em> write programs?&rdquo;. Only the most experienced programmers notice that the very process of program creation is not really <em>accessible</em>. Less experienced programmers may say something about methodologies and philosophy. But these things are just reflections on generalisations of the process, not how actual mechanisms of writing a program emerge in our heads. The same is true for any other types of <em>reasoning</em> (and accessible mental activity in general): there is a lot of an inaccessible (or <em>intuitive</em>) component of the process happening in the background. Getting this access may be the key to making our thinking processes efficient and our MLhttps://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md models economically viable.</p><p>It&rsquo;s yet to be proven that reducing skewedness of datasets by enriching them with textual descriptions of inner mental processes (intermediate cognitive material) will improve performance of language models in general and agent-related tests. Nevertheless, it&rsquo;s a solid and grounded intuition because it works this way for humans. In order to get this data from our minds, we need to develop <a href=https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences#Intrapersonal>intrapersonal intelligence</a> &ndash; the ability to understand yourself, including your thoughts, feelings, motivations, and fears, and to use that understanding to make decisions and communicate &ndash; in a way that is compatible with AI. Figuratively speaking, we need to understand our &lsquo;inner machine&rsquo; and describe it in terms of algorithms and data structures expressed as scripts for agents.</p><p>There is an ongoing <a href=https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md>multidisciplinary research process</a> around Memoria targeting unification of <strong>intrapersonal intelligence</strong> concept from Psychology (Howard Gardner) with Philosophy of Mind, Physiology, Mathematics and Computer Science. The goal is to build a conceptual bridge between first-person experience, first-person self-psychology and computations in the form of minimalist computational models of fundamental higher mental functions like <em>Observer</em> and <em>beingness</em>, <em>feelings</em>, <em>intuition</em> and many others.</p><p>Inrapersonal intelligence has been proven to play crucial role in many aspects of general human intelligence: self-awareness and emotional regulation, motivation and goal-setting, critical thinking and reflection, problem-solving and decision-making, personal growth and adaptability, interpersonal relationships. These and many others aspects of intrapersonal intelligence can positively impact overall cognitive abilities and personal success. The basic idea is enhance it with advanced computational theories of mind that is expected to extend conscious access and self-control, because much more complex mind states become accessible and interpretable. The Hard problem of consciousness isn&rsquo;t that hard if we can see our &lsquo;inner machine&rsquo;. Seamless integration with The Machinekind is also much simpler this way.</p><p>On the HW/SW side of things insights form intrapersonal intelligence are expected to give new advanced algorithms and data structure, integrated circuits, programming language concepts, software development and collaboration patterns and many other cool things. Memoria Framework will be adopting those things once they are available. The first candidate of the road is the concept of <a href=https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md#self-referential-machine>self-referential Turing Machine</a> implemented with <a href=/docs/overview/vm>DSLEngine</a>.</p><h2 id=memoria-as-a-dataset>Memoria as a Dataset<a href=#memoria-as-a-dataset class=anchor aria-hidden=true>#</a></h2><p>LLMs create a lot of opportunities for opensource software. A lot of code is currently unmaintained and forgotten. OSS authors struggles looking for users attention to their projects. If this code is used for training LLMs it will be used. LMM this this sense create completely new opportunity for idealistically-motivated authors. Even if their works do not have sufficient <em>direct</em> audience right now, <em>the message</em> hidden in thier works will live in the models trained on these works.</p><p>Memoria, as a project, does recognize this opportunity to provide not only direct software artifacts, but also indirect generative architectural patters if/when the project is used for LLM training. Memoria itself is going to use this pattern for automation of the project evolution. More details of this later&mldr;</p><p>TBC &mldr;</p><div class="docs-navigation d-flex justify-content-between"><a href=/docs/data-zoo/associative-memory-2/><div class="card my-1"><div class="card-body py-2">&larr; Associative Memory (Part 2)</div></div></a><a class=ms-auto href=/docs/applications/storage/><div class="card my-1"><div class="card-body py-2">Computational Storage &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a href=https://www.github.com/>Github</a>, <a href=https://gohugo.io/>Hugo</a>, and <a href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline><li class=list-inline-item><a href=/privacy-policy/>Privacy</a></li></ul></div></div></div></footer><script src=/js/bootstrap.min.73ca27033146a505b6a0f66b79d99f613a18e778bc9606fd223476d0ebf0fc10508b0d4f5c448b0a946fa1d71fbeffaae9732adc0c2890e61c449217fd6ee1c0.js integrity="sha512-c8onAzFGpQW2oPZredmfYToY53i8lgb9IjR20Ovw/BBQiw1PXESLCpRvodcfvv+q6XMq3AwokOYcRJIX/W7hwA==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.c5b6bb65307e087bfc3dd5a73cf000f3dc6a8b665db8c3fcbd62a3368e2f82ee494fd1ff5f025a09216cf6390bac7565c5469c6958caac1b9d09c85ba0adfefc.js integrity="sha512-xba7ZTB+CHv8PdWnPPAA89xqi2ZduMP8vWKjNo4vgu5JT9H/XwJaCSFs9jkLrHVlxUacaVjKrBudCchboK3+/A==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/katex.min.07c5862e6eea64c90e601fcaacaa0dbdb03f60dbbac68a5a5830130a00332df28001a5fa2375b739426606b441725db1af012c7e4b04b8fb222025cc0d2ac073.js integrity="sha512-B8WGLm7qZMkOYB/KrKoNvbA/YNu6xopaWDATCgAzLfKAAaX6I3W3OUJmBrRBcl2xrwEsfksEuPsiICXMDSrAcw==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/contrib/auto-render.min.bc779bab10cdc862f139e7cd6255a8f021bc483db2b7bc6d553238fb220e937dbe5cd511100d2ab764ee5d9f9092a2cdcc4e6ae109966efc18338f0b275d927d.js integrity="sha512-vHebqxDNyGLxOefNYlWo8CG8SD2yt7xtVTI4+yIOk32+XNUREA0qt2TuXZ+QkqLNzE5q4QmWbvwYM48LJ12SfQ==" crossorigin=anonymous defer></script>
<script src=/main.min.56cf146845ee56429c57a9bb74bee52540a1aa942a32506ad0d185db760f2f7a5d76f39cde824735c8b7e89f2e4453cc4da383eaa2c2a7d6a2c9dafd7061e74d.js integrity="sha512-Vs8UaEXuVkKcV6m7dL7lJUChqpQqMlBq0NGF23YPL3pddvOc3oJHNci36J8uRFPMTaOD6qLCp9aiydr9cGHnTQ==" crossorigin=anonymous defer></script>
<script src=https://memoria-framework.dev/index.min.351d4fd145bc9de2500f96b0dd8a4043efa5cada74662dc884cac27a7e2c50536d77ec092e64bf5fb4a8f3ab0e1a866c4b6c1f18a628d76abc98a78219972d89.js integrity="sha512-NR1P0UW8neJQD5aw3YpAQ++lytp0Zi3IhMrCen4sUFNtd+wJLmS/X7So86sOGoZsS2wfGKYo12q8mKeCGZctiQ==" crossorigin=anonymous defer></script></body></html>