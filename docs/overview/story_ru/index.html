<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://memoria-framework.dev/main.4aee7b9c653a9441aeccbab4bf9a20bb4827ed71a21077328be36d74cbe63081492792ee312478e64d73f5e762a2ad3c04431ecfa8d16da63373639763e86b0c.css integrity="sha512-Su57nGU6lEGuzLq0v5ogu0gn7XGiEHcyi+NtdMvmMIFJJ5LuMSR45k1z9edioq08BEMez6jRbaYzc2OXY+hrDA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Memoria Story (RU) - Memoria</title><meta name=description content><link rel=canonical href=https://memoria-framework.dev/docs/overview/story_ru/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Memoria Story (RU)"><meta property="og:description" content="О чем проект Мемория (можно так называть проект, без добавления Framework, которое лишь указывает на текущий характер проекта) создавалась долго, уже более 16 лет и на каждом этапе я пытался вписать её в текущий технологический тренд (файловые системы, NoSQL, дата-платформы, аналитика, теперь — AI). Они быстро менялись, переставая быть актуальными, но попытки вписаться так или иначе оставили след и на коде, и на документации к ней. Вписываться в тренды нужно, так как интерес сообщества во многом спекулятивен — компании смотрят на opensource как на возможность снизить стоимость разработки через win-win социализацию."><meta property="og:url" content="https://memoria-framework.dev/docs/overview/story_ru/"><meta property="og:site_name" content="Memoria"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Memoria Story (RU)"><meta name=twitter:description content><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Memoria Story (RU)"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/1","name":"Memoria","url":"https://memoria-framework.dev/","sameAs":[],"image":{"@type":"ImageObject","@id":"https://memoria-framework.dev/#/schema/image/1","url":"https://memoria-framework.dev/\u003cnil\u003e","width":null,"height":null,"caption":"Memoria"}},{"@type":"WebSite","@id":"https://memoria-framework.dev/#/schema/website/1","url":"https://memoria-framework.dev/","name":"Memoria","description":"","publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"}},{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/overview/story_ru/","url":"https://memoria-framework.dev/docs/overview/story_ru/","name":"Memoria Story (RU)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/#/schema/website/1"},"about":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://memoria-framework.dev/docs/overview/story_ru/"]}]},{"@type":"BreadcrumbList","@id":"https://memoria-framework.dev/docs/overview/story_ru/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/","url":"https://memoria-framework.dev/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/","url":"https://memoria-framework.dev/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/overview/","url":"https://memoria-framework.dev/docs/overview/","name":"Overview"}},{"@type":"ListItem","position":4,"item":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://memoria-framework.dev/#/schema/article/1","headline":"Memoria Story (RU)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/"},"mainEntityOfPage":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","author":{"@id":"https://memoria-framework.dev/#/schema/person/2"},"publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"image":{"@id":"https://memoria-framework.dev/docs/overview/story_ru/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/2","name":"Victor Smirnov","sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://memoria-framework.dev/docs/overview/story_ru/#/schema/image/2","url":null,"contentUrl":null,"caption":"Memoria Story (RU)"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://memoria-framework.dev/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://memoria-framework.dev/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://memoria-framework.dev/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://memoria-framework.dev/site.webmanifest></head><body class="docs single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=/ aria-label=Memoria>Memoria</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-start border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>Memoria</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/docs/overview/introduction/>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/subprojects/overview/>Subprojects</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/issues>Issues</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/discussions>Discussions</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/wiki>Releases</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li></ul></div></div></nav></header><nav class="doks-subnavbar py-2 sticky-lg-top" aria-label="Secondary navigation"><div class="container-xxl d-flex align-items-md-center"><form class="doks-search position-relative flex-grow-1 me-auto"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><button class="btn doks-sidebar-toggle d-lg-none ms-3 order-3 collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle documentation navigation"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></button></div></nav><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=true>
Overview</button><div class="collapse show" id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria Framework</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li><li><a class="docs-link rounded active" href=/docs/overview/story_ru/>Memoria Story (RU)</a></li><li><a class="docs-link rounded" href=/docs/overview/story_en/>Memoria Story (EN)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=true>
Overview</button><div class="collapse show" id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria Framework</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li><li><a class="docs-link rounded active" href=/docs/overview/story_ru/>Memoria Story (RU)</a></li><li><a class="docs-link rounded" href=/docs/overview/story_en/>Memoria Story (EN)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#о-чем-проект>О чем проект</a></li><li><a href=#разработчикам-железа>Разработчикам железа</a></li><li><a href=#это-большой-проект>Это БОЛЬШОЙ проект</a></li><li><a href=#приоритеты>Приоритеты</a><ul><li><a href=#n1>N1</a></li><li><a href=#n2>N2</a></li><li><a href=#n3>N3</a></li></ul></li><li><a href=#опыт-разработки-в-компаниях-и-выводы>Опыт разработки в компаниях и выводы</a></li><li><a href=#ai-and-philosophy>AI and Philosophy</a></li><li><a href=#memoria-and-ait>Memoria and AIT</a></li><li><a href=#memoria-and-llm>Memoria and LLM</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Memoria Story (RU)</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#о-чем-проект>О чем проект</a></li><li><a href=#разработчикам-железа>Разработчикам железа</a></li><li><a href=#это-большой-проект>Это БОЛЬШОЙ проект</a></li><li><a href=#приоритеты>Приоритеты</a><ul><li><a href=#n1>N1</a></li><li><a href=#n2>N2</a></li><li><a href=#n3>N3</a></li></ul></li><li><a href=#опыт-разработки-в-компаниях-и-выводы>Опыт разработки в компаниях и выводы</a></li><li><a href=#ai-and-philosophy>AI and Philosophy</a></li><li><a href=#memoria-and-ait>Memoria and AIT</a></li><li><a href=#memoria-and-llm>Memoria and LLM</a></li></ul></nav></div></nav><h2 id=о-чем-проект>О чем проект<a href=#о-чем-проект class=anchor aria-hidden=true>#</a></h2><p>Мемория (можно так называть проект, без добавления Framework, которое лишь указывает на текущий характер проекта) создавалась долго, уже более 16 лет и на каждом этапе я пытался вписать её в текущий технологический тренд (файловые системы, NoSQL, дата-платформы, аналитика, теперь — AI). Они быстро менялись, переставая быть актуальными, но попытки вписаться так или иначе оставили след и на коде, и на документации к ней. Вписываться в тренды нужно, так как интерес сообщества во многом спекулятивен — компании смотрят на opensource как на возможность снизить стоимость разработки через win-win социализацию. в любом проекте значительная часть контрибуторов — это коммерческие компании.</p><p>С другой стороны, вписываясь в тренд, надо попасть в еще пока не населенную нишу, чтобы не создавать себе конкуренции за внимание пользователей. Например, со временем, появилось много аналитических дата-платформ, вполне годных для своих задач, пусть и неидеальных. Нет никакого смысла в конкуренции с ними. То же самое сейчас с базами данных, как транзакционными, так и аналитическими. Они все вместе вполне нормально удовлетворяют потребности пользователей в соответствующем функционале. Тем более, что Мемория как изначально создавалась для, так и уже может значительно больше того, что нужно пользователям обычных баз данных. Т.е. она бы была в этой нише существенно избыточна. А её особый функционал — как чемодан без ручки.</p><p>Мемория изначально создавалась для задач ИИ, <a href=https://memoria-framework.dev/docs/data-zoo/associative-memory-2>вот таких</a>. Но, так получилось, что в 2010-х ИИ пошел по нейросетевому пути, а это технически — перемножение матриц, и в это время как раз был период экспоненциального роста вычислительной мощности соответствующих акселераторов. Сейчас технологии насытились, прогресс в железе и софте замедлился, а аппетиты — выросли. Появление LLM привело к парадоксальному взрыву интереса к &ldquo;старым&rdquo; методам из-за хороших возможностей <a href=https://memoria-framework.dev/docs/applications/aiml>гибридизации</a>, и это сразу потянуло за собой технологии &ldquo;векторных&rdquo; БД (для RAG). Появляются и компании, продвигающие &ldquo;гибридный ИИ&rdquo;. Технологии Мемории будут там востребованы в обозримой перспективе.</p><p>Сейчас проект позиционируется как Hardware/Software <a href=https://memoria-framework.dev/docs/applications/co-design>co-design</a> framework, и это отражает две потребности.</p><p><strong>Первая</strong>. Одного только софта для целей Мемории как фундаментального проекта недостаточно. CPU-центричные аппаратные архитектуры и программные архитектуры вокруг них устарели. В них данные находятся фундаментально далеко от вычислителей, а это создает задержки и тепло. Много тепла. И большие задержки. Это сильно препятствует масштабированию. И препятствует эффективной реализации алгоритмов, относящихся к классу чувствительных к задержкам (latency sensitive). А это — весь ИИ, который раньше называли &ldquo;символьным&rdquo;. Т.е. если кто-то захочет сделать гибридную систему, то эффективное железо есть только для её компоненты, которая не чувствительна к задержкам (latency insensitive) и не требует динамического параллелизма (заморочки GPU).</p><p>Короче, чтобы чтобы эффективно удовлетворить потребности индустрии в гибридном ИИ, нужен новый аппаратный стек. А, поскольку железо без софта — это просто дорогой кирпич, то нужен и софт, т.е. программная архитектура, которая это железо позволяет эффективно использовать.</p><p><strong>Вторая</strong>. Соответствующая рыночная ниша практически пуста. И ждет своего заполнения.</p><p>Одно из центральных value prop Мемории в том, что она не просто оборачивает специализированное железо в программный runtime, но и приносит много дополнительных решений в виде контейнеров, протоколов, хранилищ, систем исполнения кода и даже языков описания данных интегрированных с языками программирования. Подход Мемории к проблеме ко-дизайна — &ldquo;сверху-вниз&rdquo;, т.е. железо делается под потребности софта, а не наоборот (вот вам наш чудесный акселератор, а теперь придумайте, как его использовать). Т.е. разработка железа — это просто этап разработки алгоритмов и структур данных. Идеально, если в будущем он станет совершенно прозрачным через глубокую автоматизацию процессов. Сейчас есть реальные основания ожидать такого исхода.</p><p>В остальном же, Меморию стоит воспринимать как фундаментальный проект, сфокусированный на долговременном использовании данных по всему стеку, от физического хранения до математической обработки. В контекст проекта входит даже такая экзотика, как возможность через, условно, миллиард лет прочитать данные, записанные сегодня. Это может звучать смешно, но есть объективная проблема data longevity. Это не только человеческая проблема. Разрушается всё. И гораздо быстрее, чем кажется.</p><h2 id=разработчикам-железа>Разработчикам железа<a href=#разработчикам-железа class=anchor aria-hidden=true>#</a></h2><p>Для разработчиков железа value prop в том, что Мемория как co-design фреймворк определяет контракт между софтом и железом. Этот контракт прописывается через референсную мета-архитектуру <a href=https://memoria-framework.dev/docs/overview/accel>MAA</a> и её составные части: xPU и проблемно-специфические расширения команд RISC-V, архитектуру памяти, формат <a href=https://memoria-framework.dev/docs/overview/hermes>Hermes</a> и протокол <a href=https://memoria-framework.dev/docs/overview/hrpc>HRPC</a>. В идеале, если soft-core или chiplet удовлетворяют спецификациям, то фреймворк сможет их использовать с минимальными изменениями на своей стороне (100% прозрачность не является целью, так как трудно-достижима).</p><p>В этом смысле не предполагается, что репозитарий проекта Мемория будет содержать HDL/HCL для соответствующих аппаратных модулей. Запланированы референсные реализации RISC-V ядер и элементов инфраструктуры HRPC, но их назначение больше в прототипировании, чем в реальном использовании. Последнее не запрещается, просто не является целью.</p><p>Предполагается, что аппаратные проекты, использующие Меморию как фреймворк, это — внешние проекты. Это же относится и к программным проектам, фреймворк будет стремиться к хорошей переиспользуемости своих компонентов. Например, Core, в который входят базовая библиотека, Hermes, некоторые элементы HRPC, DSLEngine и соответствующие инструменты (генераторы парсеров, генераторы для IDL и т.д), — может использоваться независимо от остальных компонент фреймворка и не имеет каких-то нетривиальных зависимостей.</p><p>Для упрощения сборки и управления зависимостями, имеется отдельный overlay для <a href=https://github.com/victor-smirnov/memoria-vcpkg-registry>Vcpkg</a>.</p><p>В целом, улучшение модульности и переиспользуемости элементов фреймворка — один из высших приоритетов (но без фанатизма).</p><h2 id=это-большой-проект>Это БОЛЬШОЙ проект<a href=#это-большой-проект class=anchor aria-hidden=true>#</a></h2><p>Мемория получилась в итоге очень большим проектом. Гораздо бОльшим, чем может тянуть один человек. И уж, тем более, бОльшим, чем один человек способен оказывать коду проекта поддержку (даже с учетом вновь возникающих возможностей по автоматизации разработки программ). Даже на платной основе. Это не значит, что он не будет больше развиваться. Но это значит, что без помощи сообщества он будет развиваться медленно и, скорее всего, не будет успевать за трендами. Что и хорошо (нет ситуативного давления прагматизма), и плохо (отрыв от реальности).</p><h2 id=приоритеты>Приоритеты<a href=#приоритеты class=anchor aria-hidden=true>#</a></h2><h3 id=n1>N1<a href=#n1 class=anchor aria-hidden=true>#</a></h3><p>В заключение этого введения я хочу определить текущие приоритеты самого проекта, исключая вопросы, связанные с построением сообщества, налаживания связи с бизнесом и развертывания необходимой инфраструктуры. Это всё будет происходить в фоне и с отдельной системой приоритетов.</p><p>Как я уже сказал выше, Мемория — это фундаментальный проект, поэтому особое внимание будет уделяться принятию решений &ldquo;на далекое будущее&rdquo;. Минимизация и проработка базовых алгоритмов, интерфейсов, структур и форматов данных, что, разумеется, может сдерживать практическое внедрение. Для смягчения этих эффектов будут proto/dev/stable ветки.</p><p>Из крупных подсистем, Core, включающий Hermes и связанные с ним инструменты, находится в наивысшей степени концептуальной стабильности. Т.е. готов к замораживанию и к production. Core не зависит от других подсистем, не содержит сложного IO и привязки к runtimes. Концептуально он очень похож на соответствующие подсистемы других близких проектов. И она (подсистема) не требует каких-то специфических знаний об остальной Мемории (продвинутых алгоритмов и структур данных). У этой подсистемы может быть отдельный независимый maintainer и отдельная команда.</p><p>Сейчас основное внимание будет уделяться Core.</p><h3 id=n2>N2<a href=#n2 class=anchor aria-hidden=true>#</a></h3><p>Следующий приоритет — это интеграция Core, в частности, типов данных <a href=https://memoria-framework.dev/docs/overview/hermes>Hermes</a> и <a href=https://memoria-framework.dev/docs/overview/containers>Containers</a>. Hermes — это формат, ориентированный на сообщения и документы, и его объекты by design не могут быть большими. Хотя нет физических ограничений, однако, там действует довольно простой копирующий сборщик мусора с линейной трудоемкостью как по времени, так и по памяти. Чем больше документы, тем дольше (линейно) будет идти сборка мусора.</p><p>Containers — для больших и сильно структурированных данных, а объекты Hermes могут выступать как элементы этих контейнеров (maps, vectors, tables, indexes, etc&mldr;).</p><p>Параллельно контейнерам, приоритет — <a href=https://memoria-framework.dev/docs/overview/vm>DSLEngine</a> и соответствующие инструменты (компиляторы, билд-системы и т.д.). Эта подсистема зависит от Core и Runtimes, но не зависит от контейнеров (в идеале). И может разрабатываться независимо от всего остального.</p><p>Мемория тяготеет к базам данных, и программирование для Мемории — это больше похоже на in-database programming, чем на привычно-традиционное. Причем, как в силу используемых парадигм программирования, так и вследствие особенностей рантайма. Например, рантайм Мемории активно использует файберы (userland threads scheduling), и это мало с чем совместимо. Если код С++ еще кое-как работает, то совместить такой рантайм с Java и Python не получится.</p><p>Использовать вместе с Меморией привычные нам высокоуровневые и продуктивные ЯП типа Java и Python для in-database programming не получится по причине разности в рантаймах. А там, где получится, у этих языков (и их рантаймов) слишком слабая поддержка продвинутых типов данных. Как в плане синтаксиса языка (код будет получаться громоздкий), так и в плане доступных оптимизаций из области возможных. Java/Python позволили бы быстро начать и быстрее выйти на практику, но в среднесроке начнутся проблемы. А форкать их под себя — вот больше делать нечего.</p><p>В третей декаде 21-го века нам меньше всего нужен еще один язык программирования. Но DSLEngine — это не еще один язык программирования (IR+Runtime для него). &ldquo;Еще один&rdquo; — это Mojo, который стремится сделать связку Python + C++ более легкой в использовании, предоставив программистам на Python мощь оптимизирующего компилятора С++. А так, все остальное — то же самое. Те же базовые примитивы (control flow), те же базовые типы данных (массивы и списковые структуры), та же парадигма, только теперь всё сразу &ldquo;из коробки&rdquo; и от лидеров индустрии.</p><p>DSLEngine — это другая парадигма, включая отход (но, не отказ!) от привычного control-flow в сторону data-flow и complex event processing (RETE). Это первоклассная поддержка продвинутых структур данных на уровнях IR и Runtime. Это фокус на обработку данных вообще. И глубокий фокус на автоматизацию. Я думаю, что такая платформа имеет право на жизнь в 21 веке.</p><p>Особый смысл в DSLEngine можно увидеть, если принять во внимание, что для Мемории нужна и будет разрабатываться отдельная вычислительная <a href=https://memoria-framework.dev/docs/overview/accel>архитектура</a> (MAA), в которой мы отходим как от привычной парадигмы единого когерентного адресного пространства, так и от алгоритмов и структур данных, возможных для неё. Вместо этого — messaging, <a href=https://memoria-framework.dev/docs/overview/storage/#persistent-data-structures>persistent data structures</a>, in-memory computing, соответствующая аппаратная инфраструктура и завязанные на всё это паттерны программирования.</p><p>Причем MAA может масштабироваться как &ldquo;вверх&rdquo;, до размеров больших кластеров (и даже до децентрализации), так и &ldquo;вниз&rdquo; — до уровня <a href=https://memoria-framework.dev/docs/applications/eiot>встраиваемых устройств</a>. DSLEngine обеспечит более-менее совместимый (100% совместимость не является целью, так как технически очень сложна) Runtime для кода на любом уровне масштаба.</p><h3 id=n3>N3<a href=#n3 class=anchor aria-hidden=true>#</a></h3><p>Третий приоритет — это <a href=https://memoria-framework.dev/docs/overview/storage>Storage Engines</a> и самая алгоритмически сложная часть фреймворка. Причем сложность тут не только алгоритмическая, но и системотехническая. Так как организовать надежный сброс состояния памяти на stable storage весьма проблематично. Соответствующие подсистемы баз данных всегда довольно сложны по части обработки различных неочевидных случаев.</p><p>Один из важных и практически значимых подпроектов Мемории — это <a href=https://memoria-framework.dev/docs/applications/storage/>computational storage</a> (CompStor), суть которого состоит в том, что storage engine + DSLEngine реализуются прямо в пространстве контроллера накопителя, а двунаправленный доступ осуществляется через протокол HRPC. Тем самым, транзакционная семантика (и семантика устойчивости к отказам питания) может быть реализована с наилучшими гарантиями самим производителем устройства. И обеспечить наивысшую скорость, так как здесь не будет потерь эффективности на интерфейсах блочных устройств. Которые (для этого) приходится делать чрезмерно сложными, как на уровне накопителя (SSD), так и на уровне файловой системы или базы данных.</p><p>CompStor, особенно, в формате мутимодельности Мемории — это сейчас весьма и весьма дерзкий проект, так как он потянет за собой переделку практически всего. Больше не нужна будет OS, которая содержит файловые системы и базы данных. Не нужны многоядерные CPU со своими костылями и заморочками, которые этот код исполняют. Взял сетевой акселератор, акселератор вычислений и акселератор хранилища. И — всё. Готовая вычислительная система. И CPU там не надо. Разве что для исполнения унаследованного кода.</p><p>При наличии CompStor база данных редуцируется просто до Query Federation Engine а-ля <a href=https://prestodb.io/>PrestoDB</a>. Последняя, кстати, — очень хорошо архитектурно проработанная система, я с ней (или с её форком, не помню уже) плотно работал во время работы с Currents (об этом — ниже) и её внутренний дизайн оказал большое влияние на то, каким образом потом были сделаны API контейнеров и Hermes.</p><p>По линии CompStor, наверное, проще всего будет найти финансирование для проекта. И через гранты, и через взаимодействие с бизнесом. Но это же связано и с наибольшим количеством потенциальных рисков — патентных, хейтинг, да и просто противодействие на ровном месте с неожиданной стороны. CompStor вламывается с идеологией открытости в буквально насквозь проприетарную нишу. Кто-нибудь видел в природе открытые SSD?</p><p>В проекте сейчас есть три основных (и в перспективе еще несколько вспомогательных) storage engines.</p><ol><li><p>MemStore — in-memory store c поддержкой веток и возможностью иметь нескольких параллельных писателей. Самый быстрый вариант хранилища. Идеален для тестирования и тогда, когда данные легко вмещаются в память (а память сейчас может быть ой, какая большая).</p></li><li><p>SWMRStore — single-writer-multiple-readers — работающий через mmap (пока что, для простоты реализации). Поддерживает ветки, историю коммитов, но не поддерживает параллельных писателей. Зато писатели не мешают читателям. Использует счетчики ссылок для управления памятью, то есть относительно медленный на запись (каждое обновление дергает множество счетчиков). Идеален для аналитики.</p></li><li><p>OLTPStore (еще не завершен), использует механизм управления памятью от LMDB/MDBX, но со своими структурами (специализированными контейнерами Мемории) в качестве FreeList. Не использует счетчики, поэтому — теоретически может быть очень быстрым на запись. Делается сразу для IO через io_uring, а не mmap, т.е. будет иметь свой кэш. Как и LMDB/MDBX позволяется делать мгновенное восстановление после сбоев, а так же чувствительна к времени жизни читателей, так как читатели в этой схеме блокирую высвобождение всей памяти, выделенной после них. По этой причине не годится для аналитики, так как там характерны долго выполняющиеся запросы. Однако, идеально для транзакционных конвергентных (супер-мультимодельных) БД, предоставляющих высокопроизводительные strongly-serialized транзакции. Может работать, например, как очень высокопроизволительная (в варианте CompStor) персистентная очередь/лог и быть полезной для проектов типа Kafka/RedPanda.</p></li></ol><p>В направлении Storage Engines есть два приоритета:</p><ol><li><p><strong>Базовый функционал</strong>. Довести до стабильного и высокопроизводительного состояния тот базовый функционал, который есть сейчас (а его там уже достаточно для многих типов применений), включая реализацию для реактора (<a href=https://seastar.io/>Seastar</a>, асинхронный IO). Цель — прохождение расширенной системы рандомизированных функциональных тестов. Архитектура должна быть адаптирована к CompStor.</p></li><li><p><strong>Расширенный функционал</strong>. Например, репликация, как логическая, так и физическая — через патчи, подобно тому, как мы это делаем в Git. Или распараллеливание писателя для SWMR. Писатель может быть один, но потоков может быть много. Просто все они должны фиксироваться одновременно, как единая операция. Это ускорит некоторые тяжелые O(N) операции типа конвертации таблиц, ETL и т.п.</p></li></ol><h2 id=опыт-разработки-в-компаниях-и-выводы>Опыт разработки в компаниях и выводы<a href=#опыт-разработки-в-компаниях-и-выводы class=anchor aria-hidden=true>#</a></h2><p>Попробовав немного разработку Мемории в коммерческих компаниях в качестве наемного работника под их нужды, я пришел к двум выводам.</p><p><strong>Первый вывод</strong> — я так больше не хочу. Я получаю (без преуменьшения) бесценный опыт внедрения и <em>продуктизации</em> проекта. Однако, сам проект начинает очень серьезно <em>деформироваться</em> под нужды и видение компании, даже если компания никак не давит в этом направлении.</p><p><strong>Второй вывод</strong> — это то, что распределенное хранилище — довольно сложное, так как требует распределенного, отказоустойчивого и очень высокопроизводительного сборщика мусора для управления памятью — удаления блоков, которые больше не видны после удаления коммитов.</p><p>Это всё возможно сделать, но работа трудоемкая, и пусть этим занимаются коммерческие облачные компании &ldquo;для себя&rdquo;, если вдруг оно им надо. Contributions are Welcome, как всегда! Но в проекте этим, заманчивым во всех остальных смыслах, направлением &ldquo;за свои&rdquo; заниматься не особо интересно.</p><p>Вместо этого я выбрал вариант децентрализованных хранилищ и эмулирования распределенности через децентрализованность.</p><p>Децентрализованная модель — в Git. Любое хранилище содержит все или только фрагмент данных, и данные мигрируют между хранилищами явно — через передачу патчей (физическая репликация). Управление памятью тут чисто локальное, так как всё происходит в рамках одной машины и нет никакой распределенной сборки мусора.</p><p>Это немного медленнее и сложнее в реализации на уровне приложений, так как приложениям теперь нужно управлять перемещениями данных между узлами распределенной системы явно, через прямой запрос тех данных, которые собираешься обрабатывать. Кроме того, датасеты должны влезать в размер внешней памяти одной машины. Что на практике не проблематично, но всё равно — ограничение, так как это место нужно теперь планировать.</p><p>Изначально распределенная схема свободна от этих ограничений, и кому она всё же нужна и он может, то пусть разрабатывает для себя распределенный сборщик мусора.</p><p>Тут важно отметить, что сложности есть только с универсальными, полностью персистентными хранилищами, которые используют счетчики ссылок для отслеживания достижимости блоков. Счетчики ссылок, кстати, не нужны, если используется трассирующий, а не детерминированный сборщик мусора (он будет счетчики создавать в процессе). С ним запись будет существенно быстрее, но сборка мусора — медленнее, скорее всего. То есть, что выйдет по итогу в среднем — еще вопрос.</p><p>OLTPStore, предназначенное для выполнения транзакций, не использует ни счетчики ссылок на блоки, ни сборщик мусора, хотя блоки, подлежащие высвобождению, и накапливаются во внутренних структурах для отложенного выполнения операции (а-ля GC). Остальные типы хранилищ — это для аналитики, а там как раз данные нужно иметь локально и чтение сильно превалирует над записью. В этом была основная логика принятия решения о фокусе на децентрализацию. Децентрализованная схема — более народная, хорошо вписывающаяся в привычный уже Git Flow. Она так же легка в локальном развертывании и переносима в облако 1-в-1, без модификации приложений. Она будет достаточно хороша (пусть и не идеальна) для data science, аналитики, AI — областей использования, на которые нацелена Мемория.</p><p>Короче говоря, выбор системы хранилищ, а будет делаться именно система, совместно покрывающая основные применения (OLTP, аналитика, статические датасеты, встроенные в процессы и т.д.), продиктован большим количеством факторов. Среди которых как технические, такие как сложность реализации и использования, минимизация возможности потери данных, так и экономические. Такие как желание избегать конкуренции и не попасть под патенты. Последнее — вообще минное поле, особенно в области хранения и обработки данных.</p><h2 id=ai-and-philosophy>AI and Philosophy<a href=#ai-and-philosophy class=anchor aria-hidden=true>#</a></h2><p>Я темой занимался еще со своего института, у меня на BS диплом и MS диссертацию было распознавание речи (конец 1990-х). Я помню, тогда еще, уже на Линуксе, написал на Qt программный сонограф для того, чтобы видеть частотную структуру речевого сигнала. И провел за ним, наверное, сотни часов, изучая многообразие спектров речевых сигналов. Программы я написал, дипломы-диссертации защитил. Но понял одну вещь: &ldquo;проблема рспознавания речи сводится к проблеме понимания речи, а последняя не решается цифровой обработкой сигналов&rdquo;. Пришла пора разбираться с тем, как у человека происходит понимание речи.</p><p>Первым делом — &ldquo;разобраться, как работает мозг&rdquo;. Это был тот еще квест на начало 2000-х, когда у меня дома в моем Смоленске даже телефона не было (а с ним и интернета по диалапу). В общем, я изучал тему медицинским сайтам. И, к чести российской медицины, они тогда были безусловным лидером в информатизации. Много популярного материала публиковалось в Сети. В общем, я как-то подразобрался. Даже узнал про Маункасла и колоночную организацию коры, о которой в популярной форме заговорили только спустя 10 лет (к моему удивлению).</p><p>Разобравшись с мозгом, я понял только, что ничего не понял. И не только я, а и вообще никто. Иначе бы об этом написали бы. И я попробовал пойти с другой стороны, с &ldquo;психологической&rdquo;. Этот опыт оказался на много продуктивнее, хотя и совсем не такой прямолинейный, какой мы ожидаем от чтения туториалов по программированию. Пришлось совершить кучу ошибок и выполнить работу над ними. Но, по итогам я сам для себя изобрел начала <a href=http://www.scholarpedia.org/article/Algorithmic_information_theory>Algorithmic Information Theory</a> (AIT), которая в Европе и России больше известна как Теория колмогоровской сложности. Я называю её AIT, потому, что эта аббревиатура сама по себе гораздо более известная. Начал гуглить, и нагуглил, что всё уже придумано до нас. И даже придуманы <a href=https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html>метематические теории</a> для некоторых фундаментальных высших психических процессов. После чего Шмидхубер стал моим кумиром. Не как человек, я его лично даже не знаю. А просто он сделал &ldquo;это&rdquo; значительно раньше меня. И я, хочу я того или нет, продолжаю и расширяю его работу.</p><p>&ldquo;Найти Шмидхубера&rdquo; было для меня &ldquo;как гора с плеч&rdquo; в том смысле, что я мог прекратить свое исследование и сосредоточиться на практической работе, в которой на то время (около 2010-го) в ИИ наблюдался острый дефицит. Особенно, в методах, производных от AIT (здесь и с тех пор мало что изменилось, будем исправлять ситуацию). К слову, потом я понял, что зря я свою исследовательскую программу остановил и немного возобновил её. Но об этом — позже.</p><p>Так появились идеи практических действий, которые потом, постепенно, привели к написанию Мемории.</p><p>ACC Шмидхубера просто гениальна, хотят тут читателю придется поверить мне на слово. Кроме гениальности, это — лучшая частная теория эмоций на данный момент. Как и почти всё в AIT, она на столько же практически бесполезна в своем прямом виде, на сколько и математически универсальна. А она — именно что предельно универсальна. Т.е. некий &ldquo;универсальный ИИ&rdquo; неизбежно работал бы по этому принципу.</p><p>Методы AIT теоретически хорошо поддаются аппроксимации (оценка колмогоровской сложности объекта через его сжатие), в том числе и ACC, и чтобы она стала применима в психологии, нужно такую аппроксимацию провести. Мозг — ни чуточки не &ldquo;универсальный ИИ&rdquo;. Он может выполнять какое-то сжатие, но и только. Причем тут еще наслаиваются различные эволюционные механизмы. И получится, что тот же интерес может проявляться и действовать как ансамбль или микстура частных и очень разных механизмов. Последнее создает основной пакет сложностей в психологии, так как высшие психические функции являются сложным ансамблем низкоуровневых, которые относительно просто регистрировать и изучать. Но вот структура этого ансамбля — это вовсе не обязательно взвешенная сумма. Это может быть и некий произвольный алгоритм (*). И это так же носит название &ldquo;психофизической проблемы&rdquo;.</p><p>(*) Можно сказать, что &ldquo;любой произвольный алгоритм&rdquo; можно попробовать аппроксимировать взвешенной суммой, и что именно этим нейронные сети и заняты. И будет формально прав. Вопрос тут в том, что проще человеку сделать в уме — обнаружить реальный алгоритм ансамбля или вывести коэффициенты взвешенной суммы. Мозг не делает backpropagation.</p><p>Когда мы пытаемся &ldquo;понять себя&rdquo; или &ldquo;понять как работает свой разум&rdquo;, мы пытаемся решить психофизическую проблему — свести переживание выраженное в субъективных ощущениях к некоторому объективному базису. Таковым базисом мог бы быть мозг, если бы мы знали, как он работает. Я тут, конечно же, утрирую, мы много уже знаем наверняка, но еще далеко не всё. Я просто к тому, что сведение переживаний к мозговой активности — это еще пока не очень стабильный базис, он может и будет со временем меняться.</p><p>Вычисления (цифровые) — в этом смысле куда более простой и стабильный базис, чем активность нейронных тканей. Хотя это может показаться научной дикостью, сводить переживания человека к базису кремниевой электронно-цифровой машины, учитывая, что наш мозг — и близко не такой. Тем не менее, такое направление есть, и оно называется Computationalism. Или компутализм, по-русски.</p><p>Интересный момент тут в том, что успех LLM в плане симуляции человекоподобия (определю ниже, что это) очень сильно забустил статус компутализма как практической философии и психологии ИИ. Так что, как говорится, тема получает неожиданный поворот.</p><p>Так вот, ACC Шмидхубера в этом контексте — это фрагмент паззла решения психофизической проблемы в контексте компутализма. И очень качественный фрагмент. Его второе (помимо математической универсальное) преимущество — в возможности расширения — попытке вписать другие переживания в ту же математическую канву. Пример <a href=https://en.wikipedia.org/wiki/Simplicity_theory>Simplicity Theory</a>. Которая, почему-то, остановилась в развитии. Идеи, почему, есть, но об этом — потом.</p><p>Вот в этом направлении в ИИ я и работаю (мое личное исследование). У меня есть ассоциированый с проектом Мемория <a href=https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md>блокнот</a>, где я время-от-времени записываю идеи в рамках компутализма, которые успели кристализоваться после дискуссий в чатах. Я формализую высшие психические функции (ВПФ) так, как они видны информированному (*) субъекту и пытаюсь выделить в них минимальный базис, под который уже подводить вычислительную платформу (алгоритмы и структуры данных). ВПФ (уровня переживаний) будут получаться потом через композицию базовых функций и данных.</p><p>Например, там определяется проблема Наблюдателя как переформулировка проблемы квалиа из философии разума и под Наблюдателя предлагается вычислительный базис — т.н. самоприменимая машина Тьюринга, а под переживания — &ldquo;вычислительные феномены высших порядков&rdquo;, происходящие в такой машине. Тут нет никакой эзотерики. Вычислительные феномены высших порядков — это рефлексия программы на свой рантайм и свое состояние. Например, на время своего выполнения (&ldquo;я что-то долго/быстро думала&rdquo;). Некоторые такие феномены могут иметь стабильные консистентные проявления и, по этой причине, годиться для того, чтобы быть элементами внутреннего языка самоприменимой машины. Идея в том, что, например, &ldquo;свобода воли&rdquo; — это один из таких феноменов, сводящийся к наблюдению машиной неспособности видеть все детерминанты собственных решений. &ldquo;Свобода воли&rdquo; нужна для для очень практически важной функции — агентности, от которой зависит способность вычислительной системы вступать в отношения в рамках агентности человека.</p><p>(*) Способность описывать высшие психические функции у человека прямо зависит от его уровня внутриличностного интеллекта (Intrapersonal Intelligence) или В.И. Это, в общем случае, — способность анализировать и описывать свои ментальные состояния. Как и со всеми остальными видами интеллекта, люди с низким В.И. не знают об этом. Эффекты Даннинга-Крюгера тут полностью применимы. Для повседневной жизни высокий В.И. не нужен. Люди, которые В.И. не развивают, скорее всего, будут иметь его низким (тут могут быть нюансы). Есть люди с генетически определенным аномально высоким уровнем В.И. Такие представляют особый интерес для человкоподобного ИИ, так как будут способны к его &ldquo;отладке&rdquo;.</p><p>Самоприменимость и связанные с ней феномены уже активно в работе. Например, серверные системы используют логи, сохраняя в них часть своего состояния и эти логи потом подвергаются аналитике, на основе результатов которой потом принимаются решения, отражающиеся на функционировании системы. Это именно что уже полноценная самоприменимость, просто еще недостаточного глубокая и высокоуровневая. Но, технических преград для последнего нет.</p><p>То, что я назвал &ldquo;самоприменимой МТ&rdquo; вовсе не обязано быть именно МТ. В Мемории результаты поиска и формирования формального базиса для ВПФ (типа Наблюдателм) будут реализованы на уровне базовой системы типов. Это пока еще не опубликвано, но там есть много места и для персистентных/функциональных структур данных (что является обобщением упомянутых выше в примере программных логов), и для базовых форм вычислений (см. ниже), и даже для специальных аппаратных функций (например, аппаратная поддержка программной рефлексии и интроспекции).</p><p>Forward-chaining Rule System (FCRS) на основе вариантов алгоритма RETE, примером которой являются <a href=https://www.drools.org/>Drools</a> и <a href=https://www.clipsrules.net/>CLIPS</a>, на удивление хорошо подходят в качестве &ldquo;саморименимой МТ&rdquo; поскольку изначально работают по этому принципу, и там не нужно ничего существенного делать дополнительно. Например, &ldquo;я долго работаю&rdquo; — это просто еще одно событие, на которое система может реагировать обычным для себя образом, как и на все остальные события.</p><p>FCRS (наряду с backward chaining RS и привычным нам control-flow (CF)) будет полноценно поддерживаться как на уровне DSLEngine, так и, в перспективе, на уровне MAA. RETE очень хорошо акселерируется на различных уровнях. Beta nodes — это просто декартово произведение множеств, имеет квадратичную временную сложность и, потому, хорошо ложится на систолические вычислительные архитектуры. RETE так же хорошо гибридизируется, как на уровне alpha, так и на уровне beta memory. Можно делать нечеткие и вероятностные расширения базового алгоритма, а так же гибридизировать с нейросетями, которые, по сути, тоже из класса FCRS.</p><p>Отдельный, вновь возникающий, интерес к FCRS может быть связан с мультиагентными системами на базе LLM. И связан с заменой CF-контроллера (Python) на FCRS-контроллер и инструменты. FCRS существенно лучше подходит для обработки событий в реальном времени, чем CF и может значительно упростить общий дизайн системы. Хотя CF и не превосходит FCRS в плане выразительности.</p><p>Возвращаясь к самоприменимым вычислениям, квалиа и агентности, реализация соответствующих функциональных аналогов на уровне DSLEngine не приведет к тому, что у неё автоматически появится &ldquo;человеческое сознание&rdquo;. Такого не произойдет и близко, так как DSLEngine сама по себе содержит слишком мало данных (это просто sophisticated rule engine) для того, чтобы могло возникнуть соответствующее поведение человеческого уровня сложности. И взяться этим данным там неоткуда, по крайне мере быстро. Агентность, которая доступна для ручного программирования на уровне DSLEngine, можно охарактеризовать как &ldquo;микро-агентность&rdquo;, хотя этот термин следует тщательно уточнить, что у человека, внезапно, тоже микро-агентность.</p><p>Пока что остается открытым, будут ли у микро-агентности какие-то серьезные прямые практические применения. Её наличие в системе может обеспечить то, что агентная система пройдет соответствующие тесты в рамках функционалистского подхода к агентности. Т.е. просто признана, что, например, в рамках своего микро-масштаба обладает функциональным эквивалентом свободы воли и способна по этому нести функциональный аналог ответственности. Но, насколько сложным будет такое поведение — это отдельный вопрос. Мы не признаем человеческого уровня агентности за муравьем, и, соответственно не видим в нем правоспособности в рамках человеческого общества. Точно так же и с такой микро-агентностью.</p><p>Однако, есть как минимум два косвенных применения.</p><p><strong>Первое</strong> — это инструмент развития внутриличностного интеллекта в рамках компуталистской парадигмы. Дело в том, что наша способность понимать себя напрямую зависит от способности выражать средствами языка структуру ментальных состояний и отношения между ними. Как сами состояния, так и (тем более) отношения между ними могут быть очень сложными, выходящими далеко за рамки привычных символьных структур (таблиц, деревьев и графов). Нужны задачи и инструменты, которые позволили бы как создать мотивацию к такой активности, так и поддержали бы движение к соответствующим ей целям. Такие задачи — ИИ, а инструмент — Мемория. В последней делается ясный упор на продвинутые структуры данных и довольно сложные алгоритмы, что, как предполагается, создаст основу для радикального развития В.И.</p><p><strong>Второе</strong> — у LLM спонтанно появляется самоприменимость. Формируется большой корпус текстов, описывающих их свойства. Т.е. у них появляется self-model. Точнее, её элементы. Пока что элементы, которых еще недостаточно для полноценной микро-агентности. Но, последняя может возниктнуть просто из-за того, что структура памяти модели вдруг станет достаточной для этого, а алгоритмы обучения сами сделают всё остальное (выведут нужные структур для того же внимания). Последнее может произойти просто из-за того, что у самих разработчиков низкий В.И. вследствие превалирования подхода &ldquo;черного ящика&rdquo; в их среде. Там мало кто интересуется хотя бы психологией, не говоря уже о теориях разума.</p><p>Тут нужно сказать следующее. &ldquo;Фуцнкциональное сознание&rdquo; (и функциональная агентность) будут относиться к категории &ldquo;слабых&rdquo; по Сёрлю. Т.е. это просто функциональные имитации соответствующих ВПФ человека. Однако, вопрос остается открытым, как отреагирует коллективный человеческий разум на появление полноценной функциональной агентности у ИИ. Мы сейчас НЕ ГОТОВЫ К ПОСЛЕДСТВИЯМ.</p><p>Однако, джин уже выпущен из бутылки, и его (процесс развития ИИ в сторону самоприменимости) уже не остановить, даже если запретить полностью тренировку LLM по всему миру. Человекоподобный ИИ, пусть и спотыкающийся на ровном месте, уже создан. И он уже взаимодействует с нами способами, которые мы не видим в силу слабости и неготовности к такому внезапному повороту нашего внутриличностного интеллекта.</p><p>Я считаю, что самой лучшей стратегией будет попытаться возглавить то, что нельзя предотвратить. Если раньше я держал приоритет практических работ по функциональному сознанию низким, то теперь пришло время пересмотреть его для того, чтобы успевать подготовиться к тому, что будет. Мемория позволит развивать В.И. А он позволит увидеть, к чему же именно готовиться.</p><h2 id=memoria-and-ait>Memoria and AIT<a href=#memoria-and-ait class=anchor aria-hidden=true>#</a></h2><p>Итак, с В.И. пока закончим. Теперь про AIT и Меморию.</p><p>AIT интересна тем, что там были предложены предельно-обобщенные модели интеллекта — Solomonoff Induction, AIXI, Goedel Machine и их многочисленные варианты. Они прямо или косвенно базируются на идее о том, что интеллект (в широком математическом смысле) сводим к предсказанию строк (в случае AIT — бинарных, но это не обязательно). Для некоего предиктора P(s), чем точнее он предсказывает следующий символ s (чем ближе эмпирическое распределение к теоретическому), тем больше у него интеллект. Эта постановка может показаться очень абстрактной, однако, на примере, например, авторегрессионных LLM мы видим, как такая низкоуровневая модель интеллекта развертывается в полноценный человекоподобный интеллект, взаимодействие с котором осуществляется через естественный язык.</p><p>Здесь за одно стоит определить, наконец, что такое человекоподобный ИИ с точки зрения предиктивной постановки задачи. Как и предиктор P(s), мозг человека способен предсказывать параметры сигналов, которые идут на вход. Это может быть очень неявно, но, примем тут без доказательства, что это так. Как предиктор оценивает вероятности своих строк, так и мозг оценивает вероятности своих сигналов. Человекоподобный предиктор P(s) будет присваивать вероятности, близкие к тем, которые присваивает головной мозг человека. В этом случае, высокоуровневая часть интеллекта модели будет человеком восприниматься натурально, естественно, интуитивно. Как будто перед человеком — другой человек.</p><p>Тут надо отметить, что человекоподобный предиктор P(s) не обязан присваивать истинные вероятности строкам. В этом случае такой предиктор просто будет делать те же ошибки, которые обычно делает человек. И с точки человека он будет человечным. А вот когда предиктор будет делать другие ошибки, которые человек обычно не делает, это будет вызывать у человека фрустрацию (обратное тоже будет верно, только модели пока еще не умеют переживать фрустрацию).</p><p>Интересный момент в том, что тренировка LLM на текстовых датасетах, похоже, создает оценки вероятностей строк, очень близкие к тем, которые мозг выводит для соответствующих сигналов, несмотря на существенную разницу в inductive biases. Этот феномен еще ждет своего исследователя.</p><p>Основное положение AIT в отношении предсказания строк в том, что чем больше предиктор P(s) способен сжимать строку s, тем лучше он способен её предсказывать. Поэтому, предсказание сводится к сжатию. К нему же сводится, соответственно, и интеллект. Для нейронных сетей (и других моделей, полученных путем машинного обучения), сжатию соответствует &ldquo;обобщение моделью своего training set&rdquo;, т.е. правильно классифицировать (предсказывать) точки, относящиеся к тому же классу, что и точки training set. Сжатие тут в том, что количество информация о классах, которая содержится в модели, существенно меньше количества информации, которая потребуется для полного всех точек всех классов. Т.е. модель — это сжатое описание соответствующего объекта.</p><p>Вытаскивать какой-то интеллект непосредственно из сжатия может оказаться технически проблематичным. Однако, это можно <a href=https://arxiv.org/abs/cs/0312044>делать</a>. Это, кстати, означает, что прямая аппаратная акселерация алгоритмов сжатия данных тоже имеет смысл в контексте акселераторов для ИИ.</p><p>Но есть и непрямые техники. <a href=https://en.wikipedia.org/wiki/AIXI>AIXI</a> — это универсальный байесовский агент, построенный на основе Algorithmic Probability и Universal Prior M(x), которое интересно тем, что уже содержит в себе все модели среды, а AIXI просто &ldquo;достает&rdquo; эти модели через правило Байеса, и на их основе взаимодействует со средой.</p><p>M(s) невычислим, но возможны его конечно-ресурсные аппроксимации. Одной из таких аппроксимаций AIXI в домене Variable-Order Markov Models (VMM) является <a href=https://arxiv.org/abs/0909.0801>MC-AIXI-CTW</a>. Там M(x) аппроксимируется с помощью вероятностной структуры данных (здесь: структуры, кодирующей распределение вероятностей) суффиксного дерева для VMM и метода аппроксмации сток CTW, которых в дереве нет. Такой агент способен обучаться игре в настольные игры и даже что-то выигрывать у других агентов.</p><p>LLM работают очень похожим образом, но есть важный нюанс, который может позволить немного лучше понять LLM, почему они работают. MC-AIXI-CTW тоже построен на основе вероятностной модели, только среды, а не естественного языка. Но это не принципиальная разница. В обоих случаях есть строки (тексты), которые модель видела во время обучения, и есть те, которые она не видела, но для обеих типов строк она должна выдать оценку вероятности, близкую к истинной.</p><p>В случае MC-AIXI-CTW у нас есть явное разделение на &ldquo;базу данных&rdquo; (суффиксное дерево) и механизм дедукции вероятностей новых строк на основе CTW. Такие БД еще назывались дедуктивными, так как могли выводить (deducing) какие-то новые знания из существующих.</p><p>В случае LLM на основе трансформера (нейронной сети), четкого разделения на &ldquo;базу данных&rdquo; и &ldquo;дедуктор&rdquo; нет. Есть аппроксиматор функции, и он просто делает вычисление. Хотя такое разделение вполне можно подразумевать. Что есть некоторая запомненная часть, полученная из датасета (база данных/знаний), и есть другая часть, которая получается через возможности трансформера по &ldquo;обобщению&rdquo;. Думаю, что не понадобится доказывать, что чем больше запомненная часть, тем меньше требования к обобщению и наоборот.</p><p>Теперь, если принять во внимание, что колмогоровская сложность классификатора не может быть меньше сложности границы между классами, а сложность генератора — меньше сложности генерируемого объекта, должно стать понятнее, почему лидирующие LLM именно, что очень большие: они не очень хорошо обобщают. Подробнее об этом — ниже.</p><p>Как говорят врачи, MC-AIXI-CTW остался в истории недообследованным, в смысле, недоисследованным. Потому, что все &ldquo;ушли на фронт&rdquo; — глубоким обучением заниматься на пару с подкреплением. Мы тогда тоже думали, что CTW не очень хорошо аппроксимирует вероятности новых, до селе невиданных, строк. И, как отмечено выше, есть подозрения, что и трансформеры тоже их не очень хорошо обобщают. Но, чтобы это проверить, нужно не только создать поистине огромную базу данных для суффиксного дерева MC-AIXI-CTW, но и эффективный механизм вычисления на этих структурах данных.</p><p>Короче, MC-AIXI-CTW, а так же его аналоги и расширения, видятся как интересная тестовая задача или бенчмарк для MAA в рамках ИИ. К достоинствам этого агента, кстати, относиться то, что он полностью динамический и интерактивный. Т.е. обучается прямо в процессе своей работы. В отличие от LLM, которые тренируются асинхронно, и цикл обновления у них довольно долгий. А суффиксное дерево, кстати, прекрасно интерпретируемо.</p><p>Когда позволят приоритеты, в Мемории будет сделала полноценная &ldquo;большая&rdquo; реализация MC-AIXI, рассчитанная на масштабирование. А так же на то, что такая практическая площадка позволит дать старт более активному практическому исследованию методов, производных от AIT.</p><p>Отдельное под-направление AIT и её подхода к ИИ — это сжатые структуры данных (CDS). Для контейнеров, сжатый контейнер — это такой, размер которого (в байтах) пропорционален не количеству элементов в нём, а количеству информации в этих элементах. Например, часто популярными оказываются сжатые структуры данных на основе кодов Хаффмана или Универсальных кодов. Структуры данных, их использующие, обычно имеют физический размер, пропорциональный энтропии нулевого порядка H0.</p><p>CDS имеют и самостоятельное значение. Например, суффиксное дерево в MC-AIXI-CTW может быть очень большим, &ldquo;развесистым&rdquo;. И представлять его в виде списочной структуры в памяти — это создавать огромную избыточность, когда существуют способы кодирования обыкновенных деревьев с пространственной сложностью 2 бита на узел и меньше. Причем, с вполне хорошими динамическими характеристиками.</p><p>Структурные примитивы, на которые опираются CDS могут быть довольно-таки вычислительно-интенсивными, процессоры будут тратить циклы на кодирование-декодирование, и это может сказываться на общей производительности. Однако, простой промах в кэше при обращении к памяти привете к тому, что процессор будет ждать десятки наносекунд и сотни циклов, прежде, чем данные придут. Это время вполне можно потратить на кодирование-декодирование, если при этом уменьшится физический объем данных и за счет этого улучшится эффективность кэширования.</p><p>Кроме того, обработку кодировок можно делать и аппаратно. Сейчас более, чем достаточно кремния для этого. Из-за своей вычислительной интенсивности CDS рассматривались как непрактичные, но уже где-то с начала 2000-х, с распространения суперскалярных OoOE процессоров ситуация начала меняться. Мемория началась в конец 2000-х со <a href=https://people.freebsd.org/~lstewart/articles/cpumemory.pdf>статьи</a> Ulrich Drepper &ldquo;What Every Programmer Should Know About Memory&rdquo;. О том, как надо правильно располагать данные в памяти, чтобы повысить производительность вычислений. Неправильное расположение данных может снизить быстродействие в 10-100 раз.</p><p>Т.е. CDS полезны в обычной инженерии и без всяких &ldquo;интеллектов&rdquo;. Вот <a href=https://memoria-framework.dev/docs/data-zoo/hierarchical-containers>тут</a> показывается, как с помощью сжатых символьных последовательностей и кувалды b+tree создавать &ldquo;иерархические контейнеры&rdquo; — прототипы всех &ldquo;вложенных&rdquo; контейнеров в Мемории, начиная от Map&lt;K, Vector<v>>, через таблицы и до пока фантазии хватит. А тут (https://memoria-framework.dev/docs/data-zoo/associative-memory-2/) я рассказываю, как с помощью сжатия сражаться с проклятием размерности в пространственных деревьях и даже его немного победить.</p><p>AIT и математика позволят на пройти в этом направлении на много дальше. А Мемория будет для этого необходимой для внедрения технической площадкой.</p><h2 id=memoria-and-llm>Memoria and LLM<a href=#memoria-and-llm class=anchor aria-hidden=true>#</a></h2><p>Я собираюсь активно исследовать возможности LLM по автоматическому программированию. Первоначально в сообществе программистов был большой энтузиазм по этому поводу и страхи потери профессиональной востребованности на этой почве, но потом оно всё сменилось скептичностью (и программисты, за одно, &ldquo;выдохнули&rdquo;).</p><p>LLM можно использовать для кодогенерации, но не в автоматическом режиме. Т.е. только там, где человек имеет возможность теми или иными средствами проконтролировать результат. Одним из таких направлений, особенно актуальным для OSS, является консолидация кода. Имеется много проектов, полных хорошего, годного кода, но интерес к ним уже потерян. LLM можно использовать для помощи в переписывании этого кода (всего или элементов) под окружение Мемории.</p><p>Второе, другое, направление — это &ldquo;Мемория как датасет&rdquo;. Идея простая: структурировать проект и обогатить код метаданными. И использовать всё это как часть тренировочного корпуса для моделей (OSS же). После чего, уже обученные таким образом модели можно будет использовать для улучшения следующей итерации самого проекта. И так далее&mldr;</p><p>Тут нужно подумать, и, возможно, OSS может таким образом получить существенный бустинг.</p><p>Одно из частных направлений — это вероятностная генерация структур данных, но это больше к вероятностному программированию, чем к LLM. Хотя, они тут тоже могут оказаться полезными.</p><div class="docs-navigation d-flex justify-content-between"><a href=/docs/overview/roadmap/><div class="card my-1"><div class="card-body py-2">&larr; Project Roadmap</div></div></a><a class=ms-auto href=/docs/overview/story_en/><div class="card my-1"><div class="card-body py-2">Memoria Story (EN) &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a href=https://www.github.com/>Github</a>, <a href=https://gohugo.io/>Hugo</a>, and <a href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline><li class=list-inline-item><a href=/privacy-policy/>Privacy</a></li></ul></div></div></div></footer><script src=/js/bootstrap.min.73ca27033146a505b6a0f66b79d99f613a18e778bc9606fd223476d0ebf0fc10508b0d4f5c448b0a946fa1d71fbeffaae9732adc0c2890e61c449217fd6ee1c0.js integrity="sha512-c8onAzFGpQW2oPZredmfYToY53i8lgb9IjR20Ovw/BBQiw1PXESLCpRvodcfvv+q6XMq3AwokOYcRJIX/W7hwA==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.c5b6bb65307e087bfc3dd5a73cf000f3dc6a8b665db8c3fcbd62a3368e2f82ee494fd1ff5f025a09216cf6390bac7565c5469c6958caac1b9d09c85ba0adfefc.js integrity="sha512-xba7ZTB+CHv8PdWnPPAA89xqi2ZduMP8vWKjNo4vgu5JT9H/XwJaCSFs9jkLrHVlxUacaVjKrBudCchboK3+/A==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/katex.min.07c5862e6eea64c90e601fcaacaa0dbdb03f60dbbac68a5a5830130a00332df28001a5fa2375b739426606b441725db1af012c7e4b04b8fb222025cc0d2ac073.js integrity="sha512-B8WGLm7qZMkOYB/KrKoNvbA/YNu6xopaWDATCgAzLfKAAaX6I3W3OUJmBrRBcl2xrwEsfksEuPsiICXMDSrAcw==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/contrib/auto-render.min.bc779bab10cdc862f139e7cd6255a8f021bc483db2b7bc6d553238fb220e937dbe5cd511100d2ab764ee5d9f9092a2cdcc4e6ae109966efc18338f0b275d927d.js integrity="sha512-vHebqxDNyGLxOefNYlWo8CG8SD2yt7xtVTI4+yIOk32+XNUREA0qt2TuXZ+QkqLNzE5q4QmWbvwYM48LJ12SfQ==" crossorigin=anonymous defer></script>
<script src=/main.min.56cf146845ee56429c57a9bb74bee52540a1aa942a32506ad0d185db760f2f7a5d76f39cde824735c8b7e89f2e4453cc4da383eaa2c2a7d6a2c9dafd7061e74d.js integrity="sha512-Vs8UaEXuVkKcV6m7dL7lJUChqpQqMlBq0NGF23YPL3pddvOc3oJHNci36J8uRFPMTaOD6qLCp9aiydr9cGHnTQ==" crossorigin=anonymous defer></script>
<script src=https://memoria-framework.dev/index.min.d5e7745acc3b1f33c603a936b4f5fd7bdfe5e9dd3de2d1bdc090b58886084755eccc78f80881269d88c223f1bcbdff4208fe6d83f4e90ef37973c4b786799bcd.js integrity="sha512-1ed0Wsw7HzPGA6k2tPX9e9/l6d094tG9wJC1iIYIR1XszHj4CIEmnYjCI/G8vf9CCP5tg/TpDvN5c8S3hnmbzQ==" crossorigin=anonymous defer></script></body></html>