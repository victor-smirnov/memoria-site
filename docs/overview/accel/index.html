<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://memoria-framework.dev/main.4aee7b9c653a9441aeccbab4bf9a20bb4827ed71a21077328be36d74cbe63081492792ee312478e64d73f5e762a2ad3c04431ecfa8d16da63373639763e86b0c.css integrity="sha512-Su57nGU6lEGuzLq0v5ogu0gn7XGiEHcyi+NtdMvmMIFJJ5LuMSR45k1z9edioq08BEMez6jRbaYzc2OXY+hrDA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Memoria Acceleration Architecture (MAA) - Memoria</title><meta name=description content><link rel=canonical href=https://memoria-framework.dev/docs/overview/accel/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Memoria Acceleration Architecture (MAA)"><meta property="og:description" content="Basic Information Processing can be compute-intensive, IO-intensive or combined/hybrid. Processing is compute intensive if each element of data is processed many times. Examples: sorting and matrix multiplication. Otherwise it&rsquo;s IO-intensive. Example: hashtable with random access. Hybrid processing may contain both compute-intensive and IO-intensive stages, but they will be clearly separable. Like, in evaluating SQL query, JOIN is IO-intensive and SORT is compute-intensive. Physically, the more processing is compute-intensive, the less it&rsquo;s IO-intensive."><meta property="og:url" content="https://memoria-framework.dev/docs/overview/accel/"><meta property="og:site_name" content="Memoria"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Memoria Acceleration Architecture (MAA)"><meta name=twitter:description content><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Memoria Acceleration Architecture (MAA)"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/1","name":"Memoria","url":"https://memoria-framework.dev/","sameAs":[],"image":{"@type":"ImageObject","@id":"https://memoria-framework.dev/#/schema/image/1","url":"https://memoria-framework.dev/\u003cnil\u003e","width":null,"height":null,"caption":"Memoria"}},{"@type":"WebSite","@id":"https://memoria-framework.dev/#/schema/website/1","url":"https://memoria-framework.dev/","name":"Memoria","description":"","publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"}},{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/overview/accel/","url":"https://memoria-framework.dev/docs/overview/accel/","name":"Memoria Acceleration Architecture (MAA)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/#/schema/website/1"},"about":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"https://memoria-framework.dev/docs/overview/accel/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://memoria-framework.dev/docs/overview/accel/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://memoria-framework.dev/docs/overview/accel/"]}]},{"@type":"BreadcrumbList","@id":"https://memoria-framework.dev/docs/overview/accel/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/","url":"https://memoria-framework.dev/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/","url":"https://memoria-framework.dev/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/overview/","url":"https://memoria-framework.dev/docs/overview/","name":"Overview"}},{"@type":"ListItem","position":4,"item":{"@id":"https://memoria-framework.dev/docs/overview/accel/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://memoria-framework.dev/#/schema/article/1","headline":"Memoria Acceleration Architecture (MAA)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/docs/overview/accel/"},"mainEntityOfPage":{"@id":"https://memoria-framework.dev/docs/overview/accel/"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","author":{"@id":"https://memoria-framework.dev/#/schema/person/2"},"publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"image":{"@id":"https://memoria-framework.dev/docs/overview/accel/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/2","name":"Victor Smirnov","sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://memoria-framework.dev/docs/overview/accel/#/schema/image/2","url":null,"contentUrl":null,"caption":"Memoria Acceleration Architecture (MAA)"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://memoria-framework.dev/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://memoria-framework.dev/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://memoria-framework.dev/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://memoria-framework.dev/site.webmanifest></head><body class="docs single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=/ aria-label=Memoria>Memoria</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-start border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>Memoria</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/docs/overview/introduction/>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/subprojects/overview/>Subprojects</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/issues>Issues</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/discussions>Discussions</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/wiki>Releases</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li></ul></div></div></nav></header><nav class="doks-subnavbar py-2 sticky-lg-top" aria-label="Secondary navigation"><div class="container-xxl d-flex align-items-md-center"><form class="doks-search position-relative flex-grow-1 me-auto"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><button class="btn doks-sidebar-toggle d-lg-none ms-3 order-3 collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle documentation navigation"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></button></div></nav><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=true>
Overview</button><div class="collapse show" id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded active" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=true>
Overview</button><div class="collapse show" id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded active" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=false>
Data Zoo</button><div class=collapse id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#basic-information>Basic Information</a></li><li><a href=#memoria-containers>Memoria Containers</a></li><li><a href=#persistent-data-structures>Persistent Data Structures</a></li><li><a href=#high-level-architecture>High-level Architecture</a></li><li><a href=#processing-element>Processing Element</a></li><li><a href=#accelerator-module>Accelerator Module</a></li><li><a href=#cpu-mode>CPU mode</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Memoria Acceleration Architecture (MAA)</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#basic-information>Basic Information</a></li><li><a href=#memoria-containers>Memoria Containers</a></li><li><a href=#persistent-data-structures>Persistent Data Structures</a></li><li><a href=#high-level-architecture>High-level Architecture</a></li><li><a href=#processing-element>Processing Element</a></li><li><a href=#accelerator-module>Accelerator Module</a></li><li><a href=#cpu-mode>CPU mode</a></li></ul></nav></div></nav><h2 id=basic-information>Basic Information<a href=#basic-information class=anchor aria-hidden=true>#</a></h2><p>Processing can be compute-intensive, IO-intensive or combined/hybrid. Processing is compute intensive if each element of data is processed many times. Examples: sorting and matrix multiplication. Otherwise it&rsquo;s IO-intensive. Example: hashtable with random access. Hybrid processing may contain both compute-intensive and IO-intensive <em>stages</em>, but they will be clearly separable. Like, in evaluating SQL query, JOIN is IO-intensive and SORT is compute-intensive. Physically, the more processing is compute-intensive, the less it&rsquo;s IO-intensive. Just because while we are processing a data element intensively, we can&rsquo;t do IO.</p><p><em>(Note that common definitions of compute-/io-/memory-bound problems are applicable here but not exactly identical to compute-/io-intensity.)</em></p><p>Compute/IO-intensity is not always an intrinsic property of an algorithm, but rather a combination of algorithm and <em>memory architecture</em>. Memory architecture is <em>usually</em> a combination of memory buffers with different size and speed. Usually, <em>large</em> memory can&rsquo;t be <em>fast</em>. Although with current technology there may be many <em>partial</em> exceptions from this rule. From the practical perspective, by IO we mean <em>off-die traffic</em>, because it&rsquo;s usually 100-1000 times slower than intra-die traffic. Sometimes, on-dies memory architecture may be pretty complex, containing rather slow memory but relatively large that even <em>may be</em> considered an IO under certain conditions. But here we are not counting these cases, just for simplicity.</p><p>Each algorithm is characterized by some access pattern that can be <em>predictable</em> (e.g., linear or regular) or <em>random</em>, or mixed. In general, we can find regularities in memory access patters, either <em>statically</em> or <em>dynamically</em>, and utilize them to optimize data structures placement in memory logically and physically, we can keep most memory access intra-die, minimizing extra-die IO.</p><ol><li>Automatic strategy use memory access caching and prefetching.</li><li>Manual strategy use various memory buffers as a fast SRAM and move data between them manually.</li><li>Hybrid strategy implies using both caches and scratchpad memory, as well as explicit memory prefetching.</li></ol><p>Caching and prefetching is, by far, the most popular way to reduce memory access latency. It works pretty well in many practically important cases. But caching has its drawbacks too:</p><ul><li>Caching isn&rsquo;t free. <em>Cache miss</em> costs dozens of cycles and <em>cache hit</em> isn&rsquo;t free either (trying address lookup table). Raw scratchpad SRAM may be much faster in the <em>best case</em>.</li><li>Caching doesn&rsquo;t co-exist well with Virtual Memory because it needs to take address translation into account. Switching contexts invalidate caches (either cache or address translation), that may degrade performance several times.</li><li>Caching of mutable data isn&rsquo;t scalable with the number of cores because of cache-coherency traffic.</li><li>To maximize performance under rather irregular memory access latency we need sophisticated OoOE cores, which are large, hot and expensive to engineer.</li></ul><p>While raw DDR5 DRAM access latency is around 25-40 ns, the full system latency, or the time needed to move data through the memory hierarchy, is around 75 ns, that is more than 2 times higher. These numbers don&rsquo;t take into account virtual memory effects like TLB misses, which may again push the system latency up several times.</p><p>Inter-core communication is mostly done via caches and coherency traffic. One way latencies may be from 5 ns for two <em>virtual</em> cores (SMT) to <a href=https://chipsandcheese.com/2023/11/07/core-to-core-latency-data-on-large-systems>hundreds os nanoseconds</a> in case of multiple sockets. Average latency is pretty high &ndash; around dozens of nanoseconds. What is the worst, it that those numbers may be significantly higher when all cores start talking to each other. Performance may be minuscule if underling Network-on-Chip (NoC) can&rsquo;t handle this coherency traffic gracefully. Even garbage collection via atomic reference counting may be a pathological case for these type of memory architectures.</p><p>Existing Linux-compatible OoOE multicore CPUs, despite currently being the best way to run latency-sensitive workloads like databases, symbolic reasoners, constraint solvers, etc, aren&rsquo;t really being optimized for that. Slow inter-core communication makes them not that suitable for fine-grained <em>dynamic parallelizm</em>.</p><h2 id=memoria-containers>Memoria Containers<a href=#memoria-containers class=anchor aria-hidden=true>#</a></h2><p>Memoria relies heavily on metaprogramming techniques used for design-space exploration of algorithms and data structures &ndash; so called <em>generic programming</em>. In C++ we are using template metaprogramming for that, but, despite being Turing-complete, it has serious limitations. New modern programming languages like Mojo and Zig provide the full language subset for compile-time metaprogramming. <a href=https://github.com/victor-smirnov/jenny>Jenny</a>, Memoria&rsquo;s assisting Clang-based C++ compiler (used for <a href=/docs/overview/vm>DSLEngine</a> and co-design tools for code targeting RISC-V accelerators) also supports calling arbitrary functions at compile time. But currently Memoria relies only on C++ template metaprogramming for design space explorations.</p><p>Memoria&rsquo;s <a href=/docs/overview/containers>Container</a> is the main structured data abstraction unit. Containers internally have block-based structure and represented as B+Trees, either ephemeral or <a href=https://en.wikipedia.org/wiki/Persistent_data_structure>persistent</a> (multi-version). Basically, any data structure that can be (efficiently) represented as an array, can be (efficiently) represented as a container. In Memoria, we use metaprogramming to build application-specific containers out of <em>basic building blocks</em> using <em>specifications</em>.</p><p>There are five <a href=/docs/data-zoo/overview>basic building blocks</a>, all arrays support both fixed- and variable-length elements:</p><ol><li>Unsorted array.</li><li>Sorted array.</li><li>Array-packed <a href=/docs/data-zoo/partial-sum-tree>prefix sums tree</a>.</li><li>Array-packed <a href=/docs/data-zoo/searchable-seq>searchable sequence</a>.</li><li>Array-packed <a href=/docs/data-zoo/compressed-symbol-seq>compressed symbol sequence</a>.</li></ol><p>Below is a schematic representation of searching through multi-ary search tree:</p><figure><img src=tree-search.svg></figure><p>Each node of such search tree takes some space in memory, and the best performance is achieved when the size of the node is in low multiple of a CPU cache line: 32-128 bytes. For prefix sum trees search operation perform additions (accumulation) with comparison with each element of the node, for other tree types the operation may be different. Instead of doing this search in CPU cache (loading data there in the process), we can completely offload them either to:</p><ul><li>memory controller or</li><li>to processing cores attached directly to memory banks on DRAM dies.</li></ul><p>Embedding logic into DRAM is hard (but possible), because the process is not optimized for that. But memory parallelism (throughput <em>and</em> latency) will be the best in this case, together with energy efficiency. This is what is called <em>Processing-In-Memory</em> or PIM.</p><p>The alternative is to put the logic as close to the memory <em>chips</em> as possible, probably right on the memory modules or into the CXL controller. Throughput will be lower, parallelism too, latency somewhat higher. But we can leverage existing manufacturing processes, so solution will be cheaper initially. This is what is called <em>Processing-Near-Memory</em> or PNM.</p><p>The point is that, by and large, accelerating container require as much memory parallelism as possible and corresponding number of xPU is put as close to the physical memory as possible. Existing accelerators, optimizing for neural networks (matrix multiplication), do not optimize for that (for latency). Because matrix multiplication is <em>latency-insensitive</em>. Memoria applications need a separate class of accelerators, maximizing effective <em>memory parallelism</em>.</p><h2 id=persistent-data-structures>Persistent Data Structures<a href=#persistent-data-structures class=anchor aria-hidden=true>#</a></h2><p><a href=https://en.wikipedia.org/wiki/Persistent_data_structure>Persistent data structures</a> or PDS are usually implemented as trees, and Memoria follows this way. PDS are very good for parallelism, because committed versions are immutable, and immutable data can easily be shared (cached) between parallel processing units without any coordination. Nevertheless, PDS require <em>garbage collection</em>, either deterministic (via atomic reference counting) or generational. Both cases require strongly-ordered message delivery and <em>exactly-once-processing</em>. The latter is pretty hard to achieve in a massively distributed environment, but rack-scale or even DC-scale (of reasonable size) is OK. Nevertheless PDS have scalability limitations that may limit even very small systems &ndash; if they are fast enough for them to hit the limitations.</p><p>To accelerate PDS we need acceleration for atomic counters and, probably, for some other concurrency primitives. We need to make sure that communication fabric supports robust exactly-once-delivery. The latter will require some limited form of idempotent counters, that is reducible to keeping update history for a counter for a limited amount of time.</p><p>Persistent and functional data structures are <em>slower</em> than their ephemeral counterparts on a single-threaded sequential access: O(1) access time becomes O(log N) because of trees. Functional programming languages may amortize this overhead in certain cases. But it should be taken into account that benefits of using them may start manifesting only for massively-parallel applications (10+ cores).</p><h2 id=high-level-architecture>High-level Architecture<a href=#high-level-architecture class=anchor aria-hidden=true>#</a></h2><p>Computational architecture in Memoria is inherently heterogeneous and explicitly supports three domains:</p><ol><li>Generic mixed <strong>DataFlow</strong> and <strong>ControlFlow</strong> computations. A lot of <em>practical</em> compute- and IO-intensive applications, that may be run either on CPUs or on specialized hardware, fall into this category.</li><li><strong>Integrated Circuits</strong> for fixed (ASIC) and reconfigurable logic (FPGA, Structured ASIC). May be used for high performance <em>and</em> low power stream/mixed signal processing part of application, giving ability to handle events with a nanosecond-scale resolution.</li><li><strong>Rule/search-based</strong> computations in a <em>forward chaining</em> (Complex Event Processing, Streaming) and <em>backward chaining</em> (SQL/Datalog databases) forms.</li></ol><figure><img src=tri-arch.svg></figure><p>Domains are connected with a unified hardware-accelerated RPC+streaming communication protocol, <a href=/docs/overview/hrpc>HRPC</a>, allowing intra- and cross-domain seamless communication. HRPC is very much like gRPC (that is used to communicate with services in distributed computing tasks), but optimized for direct hardware implementation.</p><p>HRPC, if it&rsquo;s implemented in hardware, eliminates the need for a fully-featured OS Kernel, reducing it to a nano-kernel, that can be as small, as the amount of HRPC functionality, implemented in the software. Memoria <em>computational kernel</em>, a program module, running on a CPU core inside an <em>accelerator</em>, can listen to a stream, generated by reconfigurable logic, running in an FPGA (and vice versa). Or, the same kernel may call storage functions running in the smart-storage device, or in a &ldquo;far&rdquo; memory (near-memory compute in a CXL device):</p><figure><img src=comp-arch.svg></figure><p>In this type of architecture, OS' kernel functionality is split into a services running on different computable devices. Storage functions, which are usually the largest OS-provided piece of functionality, are managed directly by <a href=/docs/applications/storage>&lsquo;smart storage&rsquo; devices</a>, capable of running complex DB-like queries in streaming and batching modes.</p><p>This architecture design should not be considered as a hardware-assisted micro- or nano-kernel, but rather a <strong>distributed system scaled down to a single machine</strong>. Large multicore MMU-enabled CPU is no longer a <em>central</em> PU in this architecture, but rather a PU for running legacy code and code benefited from using a MMU.</p><p>Notable feature of this architecture is that memory is no longer a single shared address space, but rather a set of buffer with different affinity with compute functions. Programming it directly will be a challenge, but it&rsquo;s already a mundane 9-to-5 job in the area of distributed programming.</p><p>Note that in Memoria architecture, cross-environment code portability is <em>not ensured</em>. Different accelerators may provide different default runtime environments, memory and CPU cluster topologies. It&rsquo;s OK if some Memoria code needs a substantial rewrite to be run in a different acceleration environment. But the framework will be trying to reduce portability costs and cross-environment code duplication by using metaprogramming and other types of development automation.</p><h2 id=processing-element>Processing Element<a href=#processing-element class=anchor aria-hidden=true>#</a></h2><p>Reconfigurable extensible processing unit (xPU) is the main structural element of MAA. The main point of this design is that hardware-accelerated HRPC protocol is used for all communication between core and outer environment. From the outside, a core is represented as a set of HRPC endpoints described with using generic HRPC tools (IDL, schema, etc&mldr;). This includes:</p><ol><li>All external (to the core) memory traffic, including cache transfers and DMA;</li><li>All Debug and Observability traffic;</li><li>Runtime exceptions signalling;</li><li>Application-level HRPC communication.</li></ol><p>Such unification allows placing xPU at any place where HRPC network is available:</p><ul><li>In an accelerator&rsquo;s cluster;</li><li>Inside DDR memory controller;</li><li>On a DRAM memory module (<em>near</em> memory chips, CXL-mem, PNM-DRAM);</li><li>Inside a DRAM chip on a separate stacked die (in-package PIM-DRAM);</li><li>On a DRAM memory <em>die</em> (PIM-DRAM);</li><li>Inside a network router;</li><li>&lt; &mldr;your idea here&mldr; ></li></ul><p>In all cases kernel&rsquo;s code running in such xPUs will be able to communicate bidirectionally with the rest of environment.</p><p>Both HRPC (low-level) and system-level endpoints specification is meant to be an open protocol, so independent manufacturers may contribute both <em>specialized cores</em> and <em>middleware</em> into the Framework-supported ecosystem. Software tools will be able to adapt to a new hardware either automatically or with minimal manual efforts.</p><p>Memoria code may be pretty large and have deep function call chains, so instruction cache is essential in this design (with, unfortunately, unpredictable instruction execution latencies caused by that). Another essential part of the core is &lsquo;stack cache&rsquo; &ndash; dedicated data cache for handling thread stacks. It&rsquo;s necessary when internal data memory is used as a scratchpad, not a D$:</p><figure><img src=xpu.svg></figure><p>What this architecture is not going to have, is <em>cache coherency</em> support (unless it&rsquo;s really necessary in some narrow cases). MAA relies on PDS where mutable data is private to a writer, and readers may see only immutable data. If there is some shared structured mutable data access, like atomic reference counting, it can be done explicitly via RPC-style messaging (HRPC) and hardware-accelerated services.</p><h2 id=accelerator-module>Accelerator Module<a href=#accelerator-module class=anchor aria-hidden=true>#</a></h2><p>The whole point of MAA is to maximize utilization of available <em>memory parallelism</em> by moving processing closer to the data, primarily for <em>lowering access latency</em>, but also for <em>increasing throughput</em>.</p><p>Ideally, every <em>memory bank</em> in the system should have either xPU or a fixed functions associated with it. Embedding logic into a DRAM die is technically challenging, although some solutions are already <a href=https://arxiv.org/pdf/2105.03814>commercially available</a>. Stacking DRAM and processing dies is more expensive, but may be better fit into existing processes.</p><p>The simplest way is to put xPU and fixed functions into a DRAM memory controller (MC), making it &lsquo;smart&rsquo; this way. Here the logic is operating at the memory speed and has the shortest part to. No caches, switches and clock doming crossing. But processing throughput is limited comparing to what is possible with PIM mode.</p><p>So, <strong>optimization for memory parallelism with PNM/PIM modes</strong> and <strong>targeting memory access latency</strong>, not just throughput, is what makes some computational architecture good for Memoria applications.</p><p>Other than PNM/PIM and HRPC and the use of persistent data structures, Memoria Framework does not define any specific hardware architecture. Below there is an <em>instance</em> of an accelerator for a <em>design space</em> the framework will be supporting in software and tooling.</p><figure><img src=accelerator.svg></figure><p>Here there are following essential components:</p><ol><li>Processing elements (<a href=#processing-element>xPU</a>) &ndash; RISC-V cores with hardware support for HRPC and essential Memoria algorithms and data structures.</li><li>Network-on-Chip (NoC) in a form of either 2D array (simpler, best for matrix multiplication) or an N-dimensional hypercube (more complex, but best for latency in general case).</li><li>Main HRPC service gateway and many local HRPC routers.</li><li>Service endpoints for hardware-implemented functions like atomic reference counting (ARC) and other shared concurrency primitives.</li><li>Shared on-die SRAM, accessible by all cores. It can be distributed and has many functions &ndash; scratchpad memory, caching, ring buffers and other <em>hardware assisted</em> data structures, etc. May be physically split into many segments and distributed on the die.</li><li>Smart DRAM controller with embedded PNM xPUs and/or hardwired <a href=#memoria-containers>Memoria functions</a>.</li><li>External connectivity modules (Transceivers, PCIe, etc).</li></ol><p>The main feature of this architecture in the context of Memoria is that it&rsquo;s <em>scalable</em>. There are no inherent system-wide scalability bottlenecks like whole-chip cache coherence. Of course, synchronization primitives like ARC and mutexes <em>theoretically</em> aren&rsquo;t scalable, but they can be <em>practically</em> made efficient enough if implemented in the hardware directly, instead of doing it in the software over a cache-coherency protocol (that we can&rsquo;t even control).</p><p>Other properties of this design:</p><ol><li>It can be <em>scaled down</em> to the size and power budget of an MCU and <em>scaled up</em> to the size of an entire wafer (and beyond).</li><li>It&rsquo;s <em>composable</em>. Memoria applications do not rely on a shared array-structured memory. They may use fast structured <a href=/docs/overview/storage>transactional</a> memory for that. At the hardware level it&rsquo;s just bunch of chips talking to each other via an open messaging protocol.</li><li>It&rsquo;s extensible. Extra functionality can be added into xPUs (regular and custom RISC-V instruction set extensions), hardened shared functions, HRPC middleware and others. The only requirement is the use of HRPC protocol for communication via published HW/SW interfaces.</li></ol><h2 id=cpu-mode>CPU mode<a href=#cpu-mode class=anchor aria-hidden=true>#</a></h2><p>As it has been noted above, multicore MMU-enabled CPUs aren&rsquo;t the best runtime environment for running Memoria applications because of the overhead caused by MMU, memory hierarchy and the <a href=https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages>OS</a>. Nevertheless, it&rsquo;s a pretty large deployment base that will only be increasing in size in the foreseeable future. And it&rsquo;s currently the only way to run Memoria applications.</p><p>So, Memoria Framework is going to support it as a first-class member in its hardware platform family. Once specialized hardware becomes available, it can be incrementally included into the ecosystem.</p><div class="docs-navigation d-flex justify-content-between"><a href=/docs/overview/vm/><div class="card my-1"><div class="card-body py-2">&larr; DSL Engine</div></div></a><a class=ms-auto href=/docs/overview/mbt/><div class="card my-1"><div class="card-body py-2">Memoria Build Tool &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a href=https://www.github.com/>Github</a>, <a href=https://gohugo.io/>Hugo</a>, and <a href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline><li class=list-inline-item><a href=/privacy-policy/>Privacy</a></li></ul></div></div></div></footer><script src=/js/bootstrap.min.73ca27033146a505b6a0f66b79d99f613a18e778bc9606fd223476d0ebf0fc10508b0d4f5c448b0a946fa1d71fbeffaae9732adc0c2890e61c449217fd6ee1c0.js integrity="sha512-c8onAzFGpQW2oPZredmfYToY53i8lgb9IjR20Ovw/BBQiw1PXESLCpRvodcfvv+q6XMq3AwokOYcRJIX/W7hwA==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.c5b6bb65307e087bfc3dd5a73cf000f3dc6a8b665db8c3fcbd62a3368e2f82ee494fd1ff5f025a09216cf6390bac7565c5469c6958caac1b9d09c85ba0adfefc.js integrity="sha512-xba7ZTB+CHv8PdWnPPAA89xqi2ZduMP8vWKjNo4vgu5JT9H/XwJaCSFs9jkLrHVlxUacaVjKrBudCchboK3+/A==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/katex.min.07c5862e6eea64c90e601fcaacaa0dbdb03f60dbbac68a5a5830130a00332df28001a5fa2375b739426606b441725db1af012c7e4b04b8fb222025cc0d2ac073.js integrity="sha512-B8WGLm7qZMkOYB/KrKoNvbA/YNu6xopaWDATCgAzLfKAAaX6I3W3OUJmBrRBcl2xrwEsfksEuPsiICXMDSrAcw==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/contrib/auto-render.min.bc779bab10cdc862f139e7cd6255a8f021bc483db2b7bc6d553238fb220e937dbe5cd511100d2ab764ee5d9f9092a2cdcc4e6ae109966efc18338f0b275d927d.js integrity="sha512-vHebqxDNyGLxOefNYlWo8CG8SD2yt7xtVTI4+yIOk32+XNUREA0qt2TuXZ+QkqLNzE5q4QmWbvwYM48LJ12SfQ==" crossorigin=anonymous defer></script>
<script src=/main.min.56cf146845ee56429c57a9bb74bee52540a1aa942a32506ad0d185db760f2f7a5d76f39cde824735c8b7e89f2e4453cc4da383eaa2c2a7d6a2c9dafd7061e74d.js integrity="sha512-Vs8UaEXuVkKcV6m7dL7lJUChqpQqMlBq0NGF23YPL3pddvOc3oJHNci36J8uRFPMTaOD6qLCp9aiydr9cGHnTQ==" crossorigin=anonymous defer></script>
<script src=https://memoria-framework.dev/index.min.d805a3f8093b2ce0aa23e21c4dd2894a7f844357d42abbb869e4afc742e5b56f0bd098e27eca5b43e495413cea82f27971a5a26e3297debcf5b109efe9d784dd.js integrity="sha512-2AWj+Ak7LOCqI+IcTdKJSn+EQ1fUKru4aeSvx0LltW8L0JjifspbQ+SVQTzqgvJ5caWibjKX3rz1sQnv6deE3Q==" crossorigin=anonymous defer></script></body></html>