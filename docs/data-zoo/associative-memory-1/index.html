<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://memoria-framework.dev/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://memoria-framework.dev/main.4aee7b9c653a9441aeccbab4bf9a20bb4827ed71a21077328be36d74cbe63081492792ee312478e64d73f5e762a2ad3c04431ecfa8d16da63373639763e86b0c.css integrity="sha512-Su57nGU6lEGuzLq0v5ogu0gn7XGiEHcyi+NtdMvmMIFJJ5LuMSR45k1z9edioq08BEMez6jRbaYzc2OXY+hrDA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Associative Memory (Part 1) - Memoria</title><meta name=description content><link rel=canonical href=https://memoria-framework.dev/docs/data-zoo/associative-memory-1/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Associative Memory (Part 1)"><meta property="og:description" content="What is Associative Memory Associative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of Memory. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of bits."><meta property="og:url" content="https://memoria-framework.dev/docs/data-zoo/associative-memory-1/"><meta property="og:site_name" content="Memoria"><meta property="article:published_time" content="2021-10-28T02:07:01-04:00"><meta property="article:modified_time" content="2021-10-28T02:07:01-04:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content><meta name=twitter:creator content><meta name=twitter:title content="Associative Memory (Part 1)"><meta name=twitter:description content><meta name=twitter:card content="summary"><meta name=twitter:image:alt content="Associative Memory (Part 1)"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/1","name":"Memoria","url":"https://memoria-framework.dev/","sameAs":[],"image":{"@type":"ImageObject","@id":"https://memoria-framework.dev/#/schema/image/1","url":"https://memoria-framework.dev/\u003cnil\u003e","width":null,"height":null,"caption":"Memoria"}},{"@type":"WebSite","@id":"https://memoria-framework.dev/#/schema/website/1","url":"https://memoria-framework.dev/","name":"Memoria","description":"","publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"}},{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/","url":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/","name":"Associative Memory (Part 1)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/#/schema/website/1"},"about":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"datePublished":"2021-10-28T02:07:01CET","dateModified":"2021-10-28T02:07:01CET","breadcrumb":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://memoria-framework.dev/docs/data-zoo/associative-memory-1/"]}]},{"@type":"BreadcrumbList","@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/","url":"https://memoria-framework.dev/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/","url":"https://memoria-framework.dev/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://memoria-framework.dev/docs/data-zoo/","url":"https://memoria-framework.dev/docs/data-zoo/","name":"Data Zoo"}},{"@type":"ListItem","position":4,"item":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://memoria-framework.dev/#/schema/article/1","headline":"Associative Memory (Part 1)","description":"","isPartOf":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/"},"mainEntityOfPage":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/"},"datePublished":"2021-10-28T02:07:01CET","dateModified":"2021-10-28T02:07:01CET","author":{"@id":"https://memoria-framework.dev/#/schema/person/2"},"publisher":{"@id":"https://memoria-framework.dev/#/schema/person/1"},"image":{"@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://memoria-framework.dev/#/schema/person/2","name":"Victor Smirnov","sameAs":[]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://memoria-framework.dev/docs/data-zoo/associative-memory-1/#/schema/image/2","url":null,"contentUrl":null,"caption":"Associative Memory (Part 1)"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://memoria-framework.dev/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://memoria-framework.dev/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://memoria-framework.dev/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://memoria-framework.dev/site.webmanifest></head><body class="docs single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=/ aria-label=Memoria>Memoria</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-start border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>Memoria</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class=nav-item><a class="nav-link ps-0 py-1 active" href=/docs/overview/introduction/>Docs</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/subprojects/overview/>Subprojects</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/issues>Issues</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/discussions>Discussions</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria/wiki>Releases</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/victor-smirnov/memoria><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li></ul></div></div></nav></header><nav class="doks-subnavbar py-2 sticky-lg-top" aria-label="Secondary navigation"><div class="container-xxl d-flex align-items-md-center"><form class="doks-search position-relative flex-grow-1 me-auto"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><button class="btn doks-sidebar-toggle d-lg-none ms-3 order-3 collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle documentation navigation"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></button></div></nav><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=false>
Overview</button><div class=collapse id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=true>
Data Zoo</button><div class="collapse show" id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded active" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-overview aria-expanded=false>
Overview</button><div class=collapse id=section-overview><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/overview/introduction/>Introduction to Memoria</a></li><li><a class="docs-link rounded" href=/docs/overview/philosophy/>Philosophy</a></li><li><a class="docs-link rounded" href=/docs/overview/hermes/>Hermes</a></li><li><a class="docs-link rounded" href=/docs/overview/hrpc/>HRPC: Hermes RPC Protocol</a></li><li><a class="docs-link rounded" href=/docs/overview/containers/>Containers</a></li><li><a class="docs-link rounded" href=/docs/overview/storage/>Storage Engines</a></li><li><a class="docs-link rounded" href=/docs/overview/runtime/>Runtime Environments</a></li><li><a class="docs-link rounded" href=/docs/overview/vm/>DSL Engine</a></li><li><a class="docs-link rounded" href=/docs/overview/accel/>Memoria Acceleration Architecture (MAA)</a></li><li><a class="docs-link rounded" href=/docs/overview/mbt/>Memoria Build Tool</a></li><li><a class="docs-link rounded" href=/docs/overview/qt_creator_instructions/>Qt Creator Instructions</a></li><li><a class="docs-link rounded" href=/docs/overview/roadmap/>Project Roadmap</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-datazoo aria-expanded=true>
Data Zoo</button><div class="collapse show" id=section-datazoo><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/data-zoo/overview/>Core Data Structures -- Overview</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/partial-sum-tree/>Partial Sums Tree</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/searchable-seq/>Searchable Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/compressed-symbol-seq/>Compressed Symbol Sequence</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/hierarchical-containers/>Hierarchical Containers</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/louds-tree/>Level Order Unary Degree Sequence (LOUDS)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/wavelet-tree/>Multiary Wavelet Trees</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/packed-allocator/>Packed Allocator</a></li><li><a class="docs-link rounded active" href=/docs/data-zoo/associative-memory-1/>Associative Memory (Part 1)</a></li><li><a class="docs-link rounded" href=/docs/data-zoo/associative-memory-2/>Associative Memory (Part 2)</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-apps aria-expanded=false>
Applications</button><div class=collapse id=section-apps><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/docs/applications/aiml/>Hybrid AI</a></li><li><a class="docs-link rounded" href=/docs/applications/storage/>Computational Storage</a></li><li><a class="docs-link rounded" href=/docs/applications/db/>Converged Databases</a></li><li><a class="docs-link rounded" href=/docs/applications/co-design/>HW/SW Co-design</a></li><li><a class="docs-link rounded" href=/docs/applications/eiot/>Embedded and IoT Applications</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#what-is-associative-memory>What is Associative Memory</a></li><li><a href=#motivating-example-rdf>Motivating Example: RDF</a></li><li><a href=#definition-of-associative-memory>Definition of Associative Memory</a></li><li><a href=#multiscale-decomposition>Multiscale Decomposition</a></li><li><a href=#using-louds-for-the-tree>Using LOUDS for the Tree</a></li><li><a href=#analysis>Analysis</a></li><li><a href=#hardware-acceleration>Hardware Acceleration</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>Associative Memory (Part 1)</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#what-is-associative-memory>What is Associative Memory</a></li><li><a href=#motivating-example-rdf>Motivating Example: RDF</a></li><li><a href=#definition-of-associative-memory>Definition of Associative Memory</a></li><li><a href=#multiscale-decomposition>Multiscale Decomposition</a></li><li><a href=#using-louds-for-the-tree>Using LOUDS for the Tree</a></li><li><a href=#analysis>Analysis</a></li><li><a href=#hardware-acceleration>Hardware Acceleration</a></li></ul></nav></div></nav><h2 id=what-is-associative-memory>What is Associative Memory<a href=#what-is-associative-memory class=anchor aria-hidden=true>#</a></h2><p>Associative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of <em>Memory</em>. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of <em>bits</em>.</p><h2 id=motivating-example-rdf>Motivating Example: RDF<a href=#motivating-example-rdf class=anchor aria-hidden=true>#</a></h2><p>In <a href=https://en.wikipedia.org/wiki/Resource_Description_Framework>Resource Description Framework</a> data is modelled with a special form of a labelled graph, consisting from <em>facts</em> (represented as URIs) and <em>triples</em> in a form of $(Subject, Predicate, Object)$ linking various facts together. Logical representation of this <em>semantic graph</em> is a table, enumerating all the triples in the graph. The main operation on the graph is <em>pattern matching</em> using SQL-like query language <a href=https://en.wikipedia.org/wiki/SPARQL>SPARQL</a>. Another common mode of operations over graphs is traversal, but this mode is secondary for semantic graphs. Pattern-matching in semantic graphs is based of <em>self-joins</em> over the triple tables:</p><figure><img src=triples.svg></figure><p>Because of the flexibility of SQPRQL, self-joins can be performed on any combination of Subject, Predicate and Objects, and the join on objects is the main reason why basic representation of graphs for RDF is a relational table. And, if we want fast execution, this table has to be properly indexed.</p><p>The simplest way to provide indexing over a triple table is to sort it in some order, but ordered table only allows ordered <em>composite</em> keys. If a table is ordered like (S, P, O), that means $(Subject, Predicate, Object)$, <em>then</em> we can search first by Subject, <em>then</em> buy Predicate, and only then by Object. But not vise versa. If we want to search by an Object first, we need another table ordered by object: (O, X, Y). To be able to search in any order we need all possible permutations of S, P and O: it&rsquo;s $3! = 6$ tables.</p><p>So, fully indexed RDF triple store will need at least 6 triple tables. How many is it?</p><ol><li>It&rsquo;s <em>up to</em> 6 times the size of the original single-table store (not counting URIs and Objects if they are stored separately).</li><li>It&rsquo;s <em>up to</em> 6 times slower insertions if they are not paralleled. Of course we can make many insertions in parallel, but it&rsquo;s <em>6 times more energy</em> anyway.</li></ol><p>Modern triple stores model semantic graphs with <em>quads</em>, adding <em>Graph ID</em> to a triple, so nodes can link to entire graphs, no just other nodes (self-referentiality). In addition to, higher-dimensional tables ($D > 4$) can also be provided to speedup certain queries. And, in general case, if we have $D$-dimensional data table, we need <em>up to</em> $D!$ orderings of this table. That is 24 for $D=4$ and grows faster than an exponent.</p><p>Sorting relational tables does not scale at all for higher-order graphs ($D > 3$), but we can do better.</p><h2 id=definition-of-associative-memory>Definition of Associative Memory<a href=#definition-of-associative-memory class=anchor aria-hidden=true>#</a></h2><p>So, without loss of generality, associative memory is a $D$-dimensional relation $R$ with <em>set</em> semantics, over integer numbers drawn from some finite set (domain). The main operation on the memory is <em>lookup</em> that can be performed using arbitrary number of dimensions, specifying <em>match</em>, <em>range</em> or <em>point</em> lookup, or any combination of for any number of dimensions. <em>Recall</em> is the result of <em>lookup</em> operation, and is enumeration of all entries in $R$ matching the query.</p><p>Associative memory can be either <em>static</em>, if only lookups are allowed. Or <em>dynamic</em>, if it supports insert, update and delete operation for individual elements (Update operation can be reduced to delete + insert).</p><h2 id=multiscale-decomposition>Multiscale Decomposition<a href=#multiscale-decomposition class=anchor aria-hidden=true>#</a></h2><p>Let we have a $D$-dimensional relation $R = \lbrace {r_0, r_1, r_2, &mldr;, r_{N-1}}\rbrace$, representing a $set$, where $r_i = (c_0, c_1, &mldr;, c_{D-1})_i$ - а $D$-dimensional tuple and $N$ is a &lsquo;size&rsquo; of the table (number of elements in the set). $c_{i,j}$ is a table&rsquo;s cell value from row $i$ and dimension (column) $j$. Each cell value $c_{i,j}$ has a domain of $H$ bits.</p><p>Example: A set of 8x8 images with 8 bits per pixel can be represented with 64-dimensional relation with $H = 8$. Maximal number of images in a set is $N &lt;= 8^{64} = 2^{192}$. Given such table we can easily define an associative memory reducing content-addressable lookup to linear table scans and bit manipulations (time complexity is $O(N)$). And, actually, this is how it&rsquo;s implemented for approximate nearest neighbour search on massively-parallel hardware. Parallel linear scan is fast, but it&rsquo;s not scalable (fast memory is expensive) and it&rsquo;s not energy-efficient.</p><p>Fortunately, we can transform $O(N)$ into $O(P H + M)$ <em>&ldquo;on average&rdquo;</em>, where $1 &lt;= P &lt;= 2^D$ &ndash; average number of nodes per <strong>bucket</strong> (see below), that is, thanks to the <a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality>&ldquo;Blessing of Dimensionality&rdquo;</a>, usually tends to 1. And $M$ is a <em>recall size</em>, number of points returned by the query over $R$.</p><p>To perform the multiscale decomposition of relation $R$, we need to perform the following transformation for each row $r_i$:</p><p>Let $c_{ij} = S_{ij} = (s_0, s_1,s_2,&mldr;,s_{H-1})_{ij}$, where $s \in \lbrace{0, 1}\rbrace$ is a bit-string representation of cell value $c$. Let $s_h = B(S, h)$ &ndash; $h$-th bit of string $S$.</p><p>Now, $M(r)$ is a multiscale decomposition of a row $r = (S_0, S_2, &mldr;, S_{D-1})$. Informally, $M(r)$ is a bit string consisting from a concatenation of shuffling of all bits form $S_j$:</p><p>$M(r) = B(S_0, H-1) B(S_1, H-1) &mldr; B(S_{D-1}, H-1)| &mldr;B(S_0, H-1) B(S_1, H-1) &mldr; B(S_{D-1}, H-1)| &mldr; B(S_0, 0) B(S_1, 0) &mldr; B(S_{D-1}, 0)$.</p><p>The symbol $|$ is added to graphically separate $H$ <em>layers</em> of the multiscale representation from each other.</p><p>Example. Let $r = (100, 110, 001)$. Then $M(r) = 111|010|001$. Note, that in some sense, $M(r)$ is producing a point on a <a href=https://en.wikipedia.org/wiki/Z-order_curve>$Z$-order curve</a> for $r$. This correspondence may help in some applications.</p><p>So, multiscale decomposition of $T = M(R)$ converts a table with $D$ columns into a table with $H$ columns, which are called <em>layers</em> here.</p><p>Now, let&rsquo;s assume, that the table $T$ is sorted in a bit-lexicographic order.</p><figure><img src=tables.svg></figure><p>What is special about table $T$ is that every column $L_j$ contains some information form each column $D_j$ from table $R$. So, by searching in a column of $T$ we can search in all columns from $R$ at once. Table $T$ itself does not provide any speedup over the sorted $R$ because the number of rows is still the same as for table $R$. But quick look at the table will show that there are many <em>repetitions</em>. So, we can transform $T$ into a tree, by hierarchically (here, from left to right) collapsing repetitive elements in tables' columns. Now, a <strong>bucket</strong> is a list of all children of the same parent node sorted lexicographically. It can be shown, that there may be <em>at most</em> $2^D$ elements in a bucket. So, search in such data structure is $O(2^D H + M)$ <em>&ldquo;on average&rdquo;</em>. If $D$ is small, say, 16 or less, this may dramatically improve performance relative to linear scan of $R$ (even if it&rsquo;s sorted). See the <a href=#analysis>Analysis</a> section below for additional properties and limitations.</p><p>Note that each path from root to leaf in the tree encodes a single row in the table $T$, and after the inverse multiscale decomposition, in $R$.</p><p>Let&rsquo;s demonstrate how search works. Let we want to enumerate all rows in $R$ with $D_2$ = 0111, so $Q = (X, Y, 0111)$, where $X$ and $Y$ are <em>placehoders</em>. First, we need a multiscale representation of $Q$, $M(Q) = xy0|xy1|xy1|xy1$. Now, we need to traverse the tree from root to leafs, according to this pattern:</p><figure><img src=search.svg></figure><p>Having the multiscale query encoding, the tree traversal is straightforward. We are visiting sub-trees in-order, providing that current pattern matches the node&rsquo;s label. Visiting the leaf (+ its label is matched) means full match. The excess number of nodes visited by a range query is called <em>overwork</em>.</p><p>In this example, the selectivity of the query is pretty good. All leafs matching the query are being visited (visited nodes are drawn in the dotted-green line style). Note the nodes marked with (*). The traversal process visited some nodes <em>not leading to the full match</em>. This is why type complexity estimation for this data structure is logarithmic <em>&ldquo;on average&rdquo;</em>. Its performance depends on how data is distributed in the tree.</p><h2 id=using-louds-for-the-tree>Using LOUDS for the Tree<a href=#using-louds-for-the-tree class=anchor aria-hidden=true>#</a></h2><p>Table $T$ has the same size (in bits) as the table $R$, but the tree, if implemented using pointer-based structure, will add a lot to the table representation. Fortunately, we can use <a href=/docs/data-zoo/louds-tree>LOUDS Tree</a> to encode the tree. LOUDS tree has very small memory footprint, only 2 bits per node (+ a small overhead, like 10%, for rank/select indices). Search tree size estimation is at most $NH$ nodes, so the tree itself will take at most size of two columns of the table $R$ ($2NH$ + small overhead). In most cases LOUDS-encoded $T$ will take less space that original table $R$, including auxiliary data structures.</p><p>What is the most important for LOUDS Tree is that it&rsquo;s structured in memory in a linear order. So, if for some reason a spatial tree traversal degrades into linear search, the tree will be pretty good at this. We just need to read many layers of the tree in parallel. Such I/O operations can be efficiently prefetched.</p><h2 id=analysis>Analysis<a href=#analysis class=anchor aria-hidden=true>#</a></h2><p>It can be shown that the tree is similar to a trie-based Quad Tree (for high dimensions), so many expected (average-case) and worst-case estimations also apply. In worst case, for high-dimensional trees, traversal degenerates into linear a search. Fortunately for LOUDS, it&rsquo;s both memory-efficient for linear search <em>and</em> for tree traversal. But on average, overwork is moderate, if doesn&rsquo;t even tend to zero, so queries should perform pretty well.</p><p>Let&rsquo;s return to RDF triple stores and compare things to each other. Let&rsquo;s assume that $D = 3$ (number of dimensions) and $H = 32$ (resource identifier&rsquo;s size or search tree depth). So, in case of insertion of a triple, we have to perform 6 insertions into 6 triple tables (for each ordering) and 32 insertions in case of the tree (into each tree level). Reading is also slower: one lookup in a sorted table vs 32 lookups in the tree. It looks unreasonable to switch from triple tables (worst case logarithmic) to search tree (average case logarithmic), unless we are limited in memory and want to fit as many triples as possible into the available amount. But things start changing when we go into higher dimension ($D > 3$) and need more indices to speedup our queries.</p><p>So far&mldr;</p><ol><li><p>Point-like operations (to check if some row exists in the relation $R$) will take $O(D log(N))$ time.</p></li><li><p>Range search in the quad trees is logarithmic on average, but it&rsquo;s relatively easy to build a worst-case example, when performance degrades to a linear scan. For example, if $D = 64$, maximal bucket size is $2^{64}$, that is much larger than any practical $N$ (number of entries in $R$). Unless the data is distributed uniformly among <em>different levels</em> of the tree, we will end up having a few but very big buckets. So, special care must be taken on how we map our high-level data to dimensions of the tree. Random mapping is usually a safe bet, but always the best choice.</p></li><li><p>In the search tree &ldquo;Blessing of Dimensionality&rdquo; is fighting with &ldquo;Curse of Dimensionality&rdquo;, it&rsquo;s kind of $0 \cdot \infty$. In higher dimensions data tends to be extremely <em>sparse</em> because volume size grows exponentially with number of dimensions. So, normally, even in high dimensions, buckets will tend to have small number of elements. The bigger the number &ndash; the better, because it improves data compression and speeds up queries. But beware of the worst case, when the tree has one big bucket that all queries are visiting. It has also been observed for similar K-d trees, that with higher number of dimensions, <em>overwork</em> also tens to increase (the blessing vs curse situation, $0 \cdot \infty$, who wins?).</p></li><li><p>In case if LOUDS tree is dynamic and implemented using B-tree, <em>insert</em>, <em>update</em> and <em>delete</em> operations to the search tree have $O(H log(N))$ time complexity for point-like updates and $O(H(log(N) + M))$ for batch updates of size $M$.</p></li><li><p>The data structure representation in memory is very compact. It&rsquo;s the size of original table $R$ + (up to) the size of two columns from $R$ for LOUDS tree + 5-10% of the tree to auxiliary data structures. Overhead of the tree is constant and is amortizing with higher number of dimensions.</p></li><li><p>Even if we work with high-dimensional data, and we are losing to the curse of dimensionality, it&rsquo;s possible to perform approximate queries. Many applications where high-dimensional data analysis is required, like AI, are essentially approximate. LOUDS tree allows to compactly and efficiently remember additional bits of information with each node, to facilitate approximate queries (if necessary).</p></li></ol><h2 id=hardware-acceleration>Hardware Acceleration<a href=#hardware-acceleration class=anchor aria-hidden=true>#</a></h2><p>LOUDS tree-based Associative memory seems to be impractical specifically for RDF triple stores, but if hardware accelerated, can be cost-effective at scale, providing also many other benefits, not just memory savings (which are huge for higher dimensions). The bottleneck is on the update operations, where insertion and deletion may require tens of B-tree updates. Fortunately, this operation is well-parallelizable so we can use thousands of small RISC-V cores equipped with special command for direct and energy-efficient implementation of essential operations (partial/prefix sums, rank and select). An array or cluster of such cores can even be embedded into <a href=/subprojects/smart-storage>storage memory controller</a>.</p><p>Advanced data structures like LOUDS-based associative memory, considered to be impractical in the past, relative to more traditional things, like sorted tables and pointer-based data structures (trees, lists, graphs, &mldr;) for main memory. But progress in computer hardware makes task-specific hardware accelerator a much more viable options, opening the road to completely new applications.</p><p>In the <a href=/docs/data-zoo/associative-memory-2>next post</a> it will be shown how LOUDD-backed associative memory can be used for generic function approximation and inversion.</p><div class="docs-navigation d-flex justify-content-between"><a href=/docs/data-zoo/packed-allocator/><div class="card my-1"><div class="card-body py-2">&larr; Packed Allocator</div></div></a><a class=ms-auto href=/docs/data-zoo/associative-memory-2/><div class="card my-1"><div class="card-body py-2">Associative Memory (Part 2) &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a href=https://www.github.com/>Github</a>, <a href=https://gohugo.io/>Hugo</a>, and <a href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline><li class=list-inline-item><a href=/privacy-policy/>Privacy</a></li></ul></div></div></div></footer><script src=/js/bootstrap.min.73ca27033146a505b6a0f66b79d99f613a18e778bc9606fd223476d0ebf0fc10508b0d4f5c448b0a946fa1d71fbeffaae9732adc0c2890e61c449217fd6ee1c0.js integrity="sha512-c8onAzFGpQW2oPZredmfYToY53i8lgb9IjR20Ovw/BBQiw1PXESLCpRvodcfvv+q6XMq3AwokOYcRJIX/W7hwA==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.c5b6bb65307e087bfc3dd5a73cf000f3dc6a8b665db8c3fcbd62a3368e2f82ee494fd1ff5f025a09216cf6390bac7565c5469c6958caac1b9d09c85ba0adfefc.js integrity="sha512-xba7ZTB+CHv8PdWnPPAA89xqi2ZduMP8vWKjNo4vgu5JT9H/XwJaCSFs9jkLrHVlxUacaVjKrBudCchboK3+/A==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/katex.min.07c5862e6eea64c90e601fcaacaa0dbdb03f60dbbac68a5a5830130a00332df28001a5fa2375b739426606b441725db1af012c7e4b04b8fb222025cc0d2ac073.js integrity="sha512-B8WGLm7qZMkOYB/KrKoNvbA/YNu6xopaWDATCgAzLfKAAaX6I3W3OUJmBrRBcl2xrwEsfksEuPsiICXMDSrAcw==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/contrib/auto-render.min.bc779bab10cdc862f139e7cd6255a8f021bc483db2b7bc6d553238fb220e937dbe5cd511100d2ab764ee5d9f9092a2cdcc4e6ae109966efc18338f0b275d927d.js integrity="sha512-vHebqxDNyGLxOefNYlWo8CG8SD2yt7xtVTI4+yIOk32+XNUREA0qt2TuXZ+QkqLNzE5q4QmWbvwYM48LJ12SfQ==" crossorigin=anonymous defer></script>
<script src=/main.min.56cf146845ee56429c57a9bb74bee52540a1aa942a32506ad0d185db760f2f7a5d76f39cde824735c8b7e89f2e4453cc4da383eaa2c2a7d6a2c9dafd7061e74d.js integrity="sha512-Vs8UaEXuVkKcV6m7dL7lJUChqpQqMlBq0NGF23YPL3pddvOc3oJHNci36J8uRFPMTaOD6qLCp9aiydr9cGHnTQ==" crossorigin=anonymous defer></script>
<script src=https://memoria-framework.dev/index.min.4a5a13fdcf51744f9766c76d5ac06e1f558d5719c4faea3d46c7cce35e424bd45b01f6baa5f4ca7e73821db3f241f75b63da07733c88656c0c64d35aeb87e75b.js integrity="sha512-SloT/c9RdE+XZsdtWsBuH1WNVxnE+uo9RsfM415CS9RbAfa6pfTKfnOCHbPyQfdbY9oHczyIZWwMZNNa64fnWw==" crossorigin=anonymous defer></script></body></html>