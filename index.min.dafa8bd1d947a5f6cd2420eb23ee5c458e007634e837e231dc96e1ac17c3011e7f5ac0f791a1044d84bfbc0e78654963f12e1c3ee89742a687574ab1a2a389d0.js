var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement),f=suggestions.classList.contains('d-none');let c=0;b.keyCode===38&&!f?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&!f&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/overview/introduction/",title:"Introduction to Memoria Framework",description:"",content:'\u003cp\u003eMemoria is a hardware/software co-design framework for solving data-intensive problems. This scope includes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTransactional, analytical and hybrid \u003ca href="/docs/applications/db"\u003edatabase engines\u003c/a\u003e of various kinds.\u003c/li\u003e\n\u003cli\u003eStorage engines, including decentralized and \u003ca href="/docs/applications/storage"\u003eComputational storage\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eReasoning engines for \u003ca href="/docs/applications/aiml"\u003eHybrid\u003c/a\u003e and Symbolic AI.\u003c/li\u003e\n\u003cli\u003eProgramming languages and compilers/tools.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHW/SW co-design means that Memoria does not imply anymore that its algorithms and data structures are run on CPU-centric architectures: single shared address space memory, multi-core architectures with coherent caches and interaction via memory and so on. While such architectures are first-class support targets for Memoria, in order to unleash its full potential we need to be able to design custom computational, memory and storage architectures, optimized for Memoria.\u003c/p\u003e\n\u003cp\u003eMemoria Framework has the following components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003e\u003cstrong\u003eHermes\u003c/strong\u003e\u003c/a\u003e - arbitrarily structured object graphs allocated in relocatable contiguous memory segments, suitable for memory mapping and inter-process communication, with focus on data storage purposes. Hermes objects and containers are string-externalizable and may look like \u0026ldquo;Json with types\u0026rdquo;. GRPC-style services and related tooling (IDL) is provided (HRPC).\u003c/li\u003e\n\u003cli\u003eExtensible and customizable set of \u003ca href="/docs/overview/containers"\u003e\u003cstrong\u003eData Containers\u003c/strong\u003e\u003c/a\u003e, internally based on B+Trees crafted from reusable building blocks by the metaprogramming framework. The spectrum of possible containers is from plain dynamic arrays and sets, via row-wise/column-wise tables, multitude of graphs, to compressed spatial indexes and beyond. Everything that maps well to B+Trees can be a first-class citizen of data containers framework. Containers and Hermes data objects are deeply integrated with each other.\u003c/li\u003e\n\u003cli\u003ePluggable \u003ca href="/docs/overview/storage"\u003e\u003cstrong\u003eStorage Engines\u003c/strong\u003e\u003c/a\u003e based on Copy-on-Write principles. Storage is completely separated from containers via simple but efficient contracts. Out of the box, OLTP-optimized and HTAP-optimized storage, as well as In-Memory storage options, are provided, supporting advanced features like serializable transactions and Git-like branching.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/vm"\u003e\u003cstrong\u003eDSL execution engine\u003c/strong\u003e (DSL Engine)\u003c/a\u003e. Lightweight embeddable VM with Hermes-backed code model (classes, byte-code, resources) natively supporting Hermes data types. Direct Interpreter and AOT compilation to optimized C++.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/accel"\u003e\u003cstrong\u003eMemoria Acceleration Architecture\u003c/strong\u003e (MAA)\u003c/a\u003e. HW/SW co-design architecture and methodology targeting Memoria applications (see above).\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/runtime"\u003e\u003cstrong\u003eRuntime Environments\u003c/strong\u003e\u003c/a\u003e. Single thread per CPU core, non-migrating fibers, high-performance IO on top of io-uring and hardware accelerators.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/mbt"\u003e\u003cstrong\u003eDevelopment Automation\u003c/strong\u003e\u003c/a\u003e tools. Clang-based build tools for extracting metadata directly from C++ sources and generating boilerplate code.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese components are organized into the following logical architecture:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="architecture.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe purpose of the project is to integrate all aspects and components described above into a single vertical framework, starting from \u003cem\u003ebare silicon\u003c/em\u003e up to networking and human-level interfaces. The framework may eventually grow up into a fully-featured \u003cem\u003emetaprogramming platform\u003c/em\u003e.\u003c/p\u003e\n'}).add({id:1,href:"/docs/overview/philosophy/",title:"Philosophy",description:"",content:"\u003cp\u003eTBC\u0026hellip;\u003c/p\u003e\n"}).add({id:2,href:"/docs/overview/hermes/",title:"Hermes",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eIn ancient Greek mythology Hermes is a messenger between gods and humans.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id="basic-information"\u003eBasic information\u003c/h2\u003e\n\u003cp\u003eIn the context of Memoria Framework, Hermes is a solution to the \u0026ldquo;last mile\u0026rdquo; data modelling problem. \u003ca href="/docs/overview/containers/"\u003eContainers\u003c/a\u003e can be used to model large amounts of \u003cem\u003ehighly structured\u003c/em\u003e data. Hermes is intended to represent relatively small amount of \u003cem\u003eunstructured or semi-structured data\u003c/em\u003e and can be used together with containers. Notable feature of Hermes as a data format is that all objects and data types have canonical \u003cem\u003etextual representation\u003c/em\u003e, so Hermes data can be consumed and produced by humans. Hence, the name of the data format.\u003c/p\u003e\n\u003cp\u003eHermes defines an arbitrarily structured \u003cem\u003eobject graph\u003c/em\u003e that is allocated in a continuous memory segment (or a series of fixed size segments) working as an \u003cem\u003earena\u003c/em\u003e. Top-level object is a \u003cem\u003edocument\u003c/em\u003e. Document is a container for Hermes \u003cem\u003eobjects\u003c/em\u003e. Hermes objects internally use \u003cem\u003erelative pointers\u003c/em\u003e, so the data is \u003cem\u003erelocatable\u003c/em\u003e. Hermes objects in this form can be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estored in Memoria containers,\u003c/li\u003e\n\u003cli\u003ememory-mapped from files or between processes,\u003c/li\u003e\n\u003cli\u003eembedded into an executable as a form of a \u003cem\u003eresource\u003c/em\u003e,\u003c/li\u003e\n\u003cli\u003esent over a network or shared between host CPU and \u003cem\u003ehardware accelerators\u003c/em\u003e, even if the latter have a separate \u003cem\u003eaddress space\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHermes documents can be of arbitrary memory size, the format internally has 64-bit pointers (even on 32-bit architectures). It\u0026rsquo;s fine to have TB-size documents, but the format is not intended for that. Hermes documents are \u003cem\u003egarbage-collected\u003c/em\u003e with a simple \u003cem\u003ecopying GC\u003c/em\u003e algorithm. So the larger documents are, the more time will be spent in compactifications. Ideally, Hermes documents \u003cem\u003eshould\u003c/em\u003e (but not required to) fit into a single storage block, that is typically 4-8KB and may be up to 1MB in Memoria. In this case, accessing Hermes objects stored in containers will be in a \u003cem\u003ezero-copy\u003c/em\u003e way.\u003c/p\u003e\n\u003cp\u003eThere are three serialization formats for Hermes data.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eZero\u003c/strong\u003e serialization. Hermes data segment use relative addresses and can be externalized as is, as a raw memory block. All Hermes documents support fast \u003cem\u003eintegrity checking\u003c/em\u003e procedure to make sure that reading foreign segments is safe. This is the fastest format but not particularly the densest one. This format is mainly for \u003cem\u003edata storage\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eText\u003c/strong\u003e serialization. Human-readable, safe, and the slowest (but still fast in raw numbers).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBinary\u003c/strong\u003e serialization. The densest option, but faster than the textual one. Safe. Best for networking when human-readability is not a requirement.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="immutability"\u003eImmutability\u003c/h2\u003e\n\u003cp\u003eHermes documents are \u003cem\u003emutable\u003c/em\u003e and support run-time immutability enforcement (\u0026ldquo;make it immutable\u0026rdquo;). Immutable documents may be safely shared between threads with minor restrictions.\u003c/p\u003e\n\u003ch2 id="hermes-object"\u003eHermes Object\u003c/h2\u003e\n\u003cp\u003eHermes memory is \u003cem\u003etagged\u003c/em\u003e. A tag is a type label and has various length, from 1 to 32 bytes. Most commonly used types, like \u003ccode\u003eInt\u003c/code\u003e have tag length of 1 byte. Rarely used types may have tags up 32 bytes, in that case it\u0026rsquo;s most likely a SHA256 \u003cem\u003ehash code\u003c/em\u003e of the \u003cem\u003etype\u0026rsquo;s declaration\u003c/em\u003e. Hermes assumes that type hash collisions will be an extremely rare event.\u003c/p\u003e\n\u003cp\u003eMemory for tag is allocated address-wise \u003cem\u003ebefore\u003c/em\u003e the object. Objects may have gaps between them in memory caused by alignment requirements. In that case, if a tag fits into this gap, it\u0026rsquo;s allocated there. With high probability, short tags (for commonly used objects) do not take extra memory.\u003c/p\u003e\n\u003cp\u003eObjects tags \u003cem\u003emay be\u003c/em\u003e used for memory integrity checking, but this is left to implementations. To support integrity checking, every object pointer may contain 8- or 16-bit hash of the corresponding tag. Such type of integrity protection is probabilistic, not deterministic. Note that at this moment (2024) integrity checking \u003cem\u003eis not yet supported\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFrom the API\u0026rsquo;s perspective, Hermes objects consist from two part. The first part is a private C++ object with verbose API conforming to certain rules. The second part is a public \u003ccode\u003eView\u003c/code\u003e for this object, this is much like \u003ccode\u003estd::string_view\u003c/code\u003e for string-like data. Mutable Hermes object receive a reference to the corresponding Document\u0026rsquo;s arena allocator to allocate new data in. View object encapsulates all these complexities.\u003c/p\u003e\n\u003cp\u003eViews are \u003cem\u003eowning\u003c/em\u003e in Hermes. Object\u0026rsquo;s view holds a reference to the corresponding Document (for memory allocation) with a fast, \u003cem\u003enon-atomic\u003c/em\u003e, reference counter. Hermes Views are nearly zero-cost, but not thread-safe.\u003c/p\u003e\n\u003cp\u003eDocuments as containers have additional levels of reference counting indirection, including atomic counters. Sharing document\u0026rsquo;s \u003cem\u003edata\u003c/em\u003e between threads may be permitted in some cases.\u003c/p\u003e\n\u003ch3 id="56-bit-types"\u003e56-bit types\u003c/h3\u003e\n\u003cp\u003e1-byte tag size has special consequences in Hermes: together with 64-bit integers, we are using \u003cem\u003e56-bit\u003c/em\u003e integers and identifiers to be able to fit the entire value into a 64-bit memory slot of a pointer. That, in many cases, saves memory.\u003c/p\u003e\n\u003ch2 id="datatypes"\u003eDatatypes\u003c/h2\u003e\n\u003cp\u003eHermes has explicit notion of a type, and Hermes types are pretty close in semantics to C++ classes, they may be \u003cem\u003eparametric\u003c/em\u003e in two ways. The first way is common with C++: \u003ccode\u003eMyType\u0026lt;Parameter\u0026gt;\u003c/code\u003e creates new instance of \u003ccode\u003eMyType\u003c/code\u003e parametrized by \u003ccode\u003eParameter\u003c/code\u003e. The second type of parametrization is trickier. Hermes type may have an associated \u003cem\u003estate\u003c/em\u003e that is considered as a shared state for all Hermes objects of this type. Type \u003ccode\u003eDecimal(10,2)\u003c/code\u003e have two \u003cem\u003etype constructor\u003c/em\u003e \u003ccode\u003e(10, 2)\u003c/code\u003e parameters: precision 10 and scale 2. Type constructor in Hermes does not create a new type, so there is no way to \u003cem\u003estatically\u003c/em\u003e specialize some code for \u003ccode\u003eDecimal(10, 2)\u003c/code\u003e. Object instances of type \u003ccode\u003eDecimal\u003c/code\u003e will have a pointer to a memory, storing the corresponding type constructor\u0026rsquo;s data.\u003c/p\u003e\n\u003cp\u003eGeneric types in Hermes are monomorphic.\u003c/p\u003e\n\u003cp\u003eTo distinguish between C++ types and Hermes types that may have type constructors, the letter are called \u003cem\u003eDatatypes\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eDatatypes are first-class objects in Hermes and the rest of Memoria. So, we may have collections of datatypes, we cap process them as data and so on. We can also parse RTTI declaration of some C++ types as \u003cem\u003enormalized\u003c/em\u003e datatypes and compute their corresponding 32-byte \u003cem\u003etype hash\u003c/em\u003e. (Note that this feature is currently may be compiler-specific).\u003c/p\u003e\n\u003ch2 id="document"\u003eDocument\u003c/h2\u003e\n\u003cp\u003eIn Hermes \u0026lsquo;document\u0026rsquo; has two meanings.\u003c/p\u003e\n\u003cp\u003eFirst, \u003ccode\u003eDocument\u003c/code\u003e is a container for Hermes data. Second, \u0026lsquo;document\u0026rsquo; is a specific set of predefined collections organizing objects into a tree-like structure, similar to Json. Below there is a short walk-through Hermes document text-serialization features giving us json-like experience.\u003c/p\u003e\n\u003ch3 id="null-object"\u003eNull object\u003c/h3\u003e\n\u003cp\u003eThis is the simplest document that has no object.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003enull\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="generic-object-array"\u003eGeneric object array\u003c/h3\u003e\n\u003cp\u003eGeneric array of Objects containing intgers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe same generic array of Objects containing numbers of different types (integer, unsigned integer, short and float):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2u, 3s, 4.567f]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="typed-integer-array"\u003eTyped integer array\u003c/h3\u003e\n\u003cp\u003eThe same array but of datatype \u003ccode\u003eArray\u0026lt;Int\u0026gt;\u003c/code\u003e using optimized memory layout:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e@Array\u0026lt;Int\u0026gt; = [1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eImportant note that notation above does not mean that collection \u003ccode\u003e[]\u003c/code\u003e will have the type specified before it. It means that we create a typed collection \u003cem\u003efrom\u003c/em\u003e a generic one. Parser may optimize this process by not creating the latter and supplying the data directly to the former.\u003c/p\u003e\n\u003cp\u003eThe point is that there may be may ways to create a Hermes object at parse time from Hermes data. For example, notation like \u003ccode\u003e\u0026quot;string value\u0026quot;@SomeDataType\u003c/code\u003e is a syntactic shortcut meaning that \u003ccode\u003e\u0026quot;string value\u0026quot;\u003c/code\u003e will be \u003cem\u003econverted\u003c/em\u003e to Hermes object of datatype \u003ccode\u003eSomeDataType\u003c/code\u003e at the \u003cem\u003eparse time\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e\u0026quot;19345068945625345634563564094564563458.609\u0026quot;@Decimal(50,3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTechnically, string representation of Hermes data is not a static format, like Json, but a \u003cem\u003edomain specific language\u003c/em\u003e. The purpose of this DSL is to make crafting complex Hermes data in a text form easier for humans.\u003c/p\u003e\n\u003ch3 id="generic-map-between-strings-and-objects"\u003eGeneric map between strings and objects\u003c/h3\u003e\n\u003cp\u003eGeneric map between strings and objects:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e{\n  \u0026quot;key1\u0026quot;: 1, \n  \u0026quot;key2\u0026quot;: [1, 2, true],\n  \u0026quot;key3\u0026quot;: @Array\u0026lt;Int\u0026gt; = [5,6,7,8]\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="tinyobjectmap"\u003eTinyObjectMap\u003c/h3\u003e\n\u003cp\u003eThere is a special memory-efficient variant of typed map, mapping from small integer in range [0, 51] to an Object. This map is very memory efficient, using only 16 bytes overhead per collection. It\u0026rsquo;s also very fast for reading, because it uses just one \u003ccode\u003ePopCnt()\u003c/code\u003e instruction (usually 1 cycle on modern CPUs) to find the slot in the hash array, given the key. Values up to 56 bits (small strings, short integers, floats) may be embedded into the hash array. Hash array has no empty slots.\u003c/p\u003e\n\u003cp\u003eGiven its runtime versatility, this type of a map is used extensively to represent C-like \u003cem\u003edynamic\u003c/em\u003e structures in Hermes, without creating a corresponding C++ objects. DSLs over Hermes may combine this type of map with \u003cem\u003ecode\u003c/em\u003e, resulting in dynamically typed \u003cem\u003eobjects\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="semantic-graph"\u003eSemantic Graph\u003c/h2\u003e\n\u003cp\u003eSemantic graph (SG) in Hermes adopts \u003ca href="https://www.w3.org/RDF/"\u003eRDF\u003c/a\u003e-\u003cem\u003elike\u003c/em\u003e data representation to Hermes\' object model. SG usually is a \u003cem\u003ebinary relation\u003c/em\u003e over \u003cem\u003efacts\u003c/em\u003e  in some \u003cem\u003edomain\u003c/em\u003e, plus some model of formal semantics reducible to binary relations. Note that Knowledge Graph (KG) is basically the same thing, but has more features to capture and represent real-life knowledge.\u003c/p\u003e\n\u003cp\u003eSG have appealing theoretical and practical properties, but using them \u0026ldquo;at scale\u0026rdquo; (for large data sets) is a major technological challenge. Graphs or any form do not map well to memory hierarchies (a lot of random access), so it\u0026rsquo;s a \u003cem\u003every expensive\u003c/em\u003e data representation and format.\u003c/p\u003e\n\u003cp\u003eBeing multi-model, Memoria will be supporting various forms of graphs and binary relations anyway, so first-class support for SG starting from Hermes seems a consistent decision.\u003c/p\u003e\n\u003ch2 id="hermespath"\u003eHermesPath\u003c/h2\u003e\n\u003cp\u003eHermes has minimalist but expressive query language for its document subset, modelled after \u003ca href="https://jmespath.org"\u003eJMESPath\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="templating-engine"\u003eTemplating engine\u003c/h2\u003e\n\u003cp\u003eTemplating (generating text) is possible with \u003ca href="https://jinja.palletsprojects.com/en/3.1.x/"\u003eJinja\u003c/a\u003e-like templating language, integrated with HermesPath as expression language.\u003c/p\u003e\n\u003ch2 id="schema"\u003eSchema\u003c/h2\u003e\n\u003cp\u003eHermes has schema processor to enforce declarative and imperative constraints on Hermes documents, semantic graphs and other types of structures. Schema processor is a major component of Hermes\' stack, that may work in an interactive mode, like a \u003cem\u003elanguage server\u003c/em\u003e for Hermes data structures.\u003c/p\u003e\n\u003ch2 id="profiles"\u003eProfiles\u003c/h2\u003e\n\u003cp\u003eHermes may support different \u003cem\u003eprofiles\u003c/em\u003e. A \u003cem\u003eprofile\u003c/em\u003e is a set of features Hermes container is supporting. Supporting all the features and data-types may be unnecessary and resource-consuming, especially on web and embedded platforms. For example, typical web applications don\u0026rsquo;t need much more than JSON is providing out of the box (generic object, array, null and a few data types).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMinimalist \u003cem\u003e\u0026lsquo;pico\u0026rsquo;\u003c/em\u003e profile may support only generic \u003cem\u003efixed size\u003c/em\u003e array container, fixed sizevariant of \u003ca href="#tinyobjectmap"\u003eTinyObjectMap\u003c/a\u003e, 56- and 64-bit integers and strings. This profile will be the most compact one, yet functionality is sufficient for all basic interactions between code and devices.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lsquo;Nano\u0026rsquo;\u003c/em\u003e profile may add generic fixed-size container mapping from Int56 to Object, that is sufficient for most of relational-type data (also, trees and graphs).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lsquo;Micro\u0026rsquo;\u003c/em\u003e profile may add support of all integer (8, 16, 32 bit, including vlen), floating point types and sematic graph data-types.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lsquo;Basic\u0026rsquo;\u003c/em\u003e profile may add essential dynamic (growable/shrinkable) containers that may be uses as a \u003cem\u003eworking memory\u003c/em\u003e by applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026hellip;\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd so on\u0026hellip; Profiles are not necessary hierarchical and inclusive. Hermes has open type system, and supporting all possible types is just infeasible. So may be application-specific profiles, staying completely aside from the main profile set.\u003c/p\u003e\n\u003ch2 id="hardware-implementation"\u003eHardware implementation\u003c/h2\u003e\n\u003cp\u003eImplementing parts of Hermes protocol directly in hardware makes sense in two cases: embedded and multicore accelerators, because in both cases CPU cores are pretty simple (not OoOE) and we don\u0026rsquo;t want to waste CPU cycles on complex data encodings. So, there are basic areas where we can do it:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eTag dispatching\u003c/strong\u003e. Mathematical operations on generic numbers go through either tag-indexed  switch statement or hash table over multiple handlers. Implementing this dispatching directly in hardware may save a lot of cycles.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVariable length integers\u003c/strong\u003e may save a lot of memory and are actively used in Hermes. Supporting them directly in hardware will improve performance significantly.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e\u003ca href="/docs/data-zoo/searchable-seq/"\u003eAdvanced Data Structures\u003c/a\u003e\u003c/strong\u003e. Certain frequently used containers containers, like \u003ca href="#tinyobjectmap"\u003eTinyObjectMap\u003c/a\u003e, may rely on on advanced bit-parallel operations like rank/select (PopCnt and SelectN), implementing them in hardware will improve performance significantly (10-60x) over baseline.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImmutability enforcement\u003c/strong\u003e. If hardware supports memory segmentation, blocks containing finalized Hermes documents may be marked immutable.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBesides these simple cases, important operations like binary serialization and deserialization, as well as consistency checking (reference tags and object memory layout checking) may be partially implemented in hardware, improving performance in many important cases.\u003c/p\u003e\n\u003cp\u003eProfile system may also be aligned with hardware acceleration. Profile may mean that certain datatypes are hardware-accelerated and using them is encouraged (other other possible datatypes).\u003c/p\u003e\n\u003ch2 id="interoperability-with-other-languages"\u003eInteroperability with other languages\u003c/h2\u003e\n\u003cp\u003eHermes is a \u003cem\u003eC++-centric data model\u003c/em\u003e, it relies heavily on RAII for memory management. Replicating it fully in other runtime environments may be difficult if even possible. To implement it fully, target runtime environment must support either RAII or at least ARC. D, Rust, Swift and CPython are in the green zone. For other environments like JavaScript, Java and Julia some functionality may be limited.\u003c/p\u003e\n\u003cp\u003eAnother important dependency is that Hermes\' datatypes may be pretty complex, like arbitrary precision numbers or safe integers with deterministic overflow semantics. Also, DSLs (HermesPath) may rely on extensive libraries of functions. Re-implementing it all in target language will lead to a lot of complex code duplication. So, bindings to Hermes will be relying on FFI to C++. The caveat is that binary code for full set of Hermes may be pretty large in size (10MB+) so running it in a browser (WASM) may be \u003cem\u003eimpractical\u003c/em\u003e (unless Hermes is supported natively).\u003c/p\u003e\n\u003ch2 id="sources"\u003eSources\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eMain \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/include/memoria/core/hermes"\u003eheaders\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eMain \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/lib/hermes"\u003esources\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:3,href:"/docs/overview/hrpc/",title:"HRPC: Hermes RPC Protocol",description:"",content:'\u003ch2 id="basic-information"\u003eBasic Information\u003c/h2\u003e\n\u003cp\u003e\u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e RPC (HRPC) is a \u003cem\u003edesign space\u003c/em\u003e of messaging protocols optimized for direct hardware implementation. HRPC is a pretty low-level protocol semantically adding just a little-bit over common networking models like UCP and TCP (point-to-point messaging, broadcasting streaming, etc).\u003c/p\u003e\n\u003cp\u003eSemantically, HRPC is very similar to \u003ca href="https://grpc.io"\u003egRPC\u003c/a\u003e and \u003ca href="https://capnproto.org/"\u003eCap\u0026rsquo;n-proto\u003c/a\u003e, so related skills and mental models, as well as programming patterns and paradigms are immediately applicable here. The purpose is also the same \u0026ndash; to simplify intreroperation between functional units in large systems. But there are three substantial differences:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHermes (the main message format of HRPC) is versatile but expressive enough to be the working memory data representation. It\u0026rsquo;s possible to build an entire \u003ca href="/docs/overview/vm"\u003edata-type rich programming language\u003c/a\u003e on top of it.\u003c/li\u003e\n\u003cli\u003eUnlike other message formats, Hermes is \u003cem\u003estorage-oriented\u003c/em\u003e, so \u003cem\u003epersisting\u003c/em\u003e messages is much easier, that may enable additional use-cases.\u003c/li\u003e\n\u003cli\u003eHRPC is hardware-oriented, so direct hardware implementation of essential functions \u003cem\u003eis encouraged\u003c/em\u003e, enabling cheap, simple but powerful middleware.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHRPC is generally \u003cem\u003etransport-agnostic\u003c/em\u003e. If underling transport does not provide message ordering, HRPC stack \u003cem\u003emay\u003c/em\u003e implement it. But specific HRPC implementations \u003cem\u003emay rely\u003c/em\u003e on transport\u0026rsquo;s provided guarantees.\u003c/p\u003e\n\u003cp\u003eHRPC has no fixed \u0026ldquo;wire format\u0026rdquo;, it\u0026rsquo;s a \u003cem\u003edesign-space\u003c/em\u003e, not a specific protocol. A minimalist point-to-point HRPC communication link may connect DDR memory controller with DDR DRAM chips using command-driven pattern, with minimal overhead comparing to a fully-native implementation. But HRPC \u003cem\u003emiddleware\u003c/em\u003e can also bridge and route this memory traffic via an HRPC-enabled low-latency networking to a remote disaggregated memory.\u003c/p\u003e\n\u003cp\u003eHRPC supports \u003cem\u003estreaming\u003c/em\u003e, so, technically, it\u0026rsquo;s a Remote \u003cem\u003eCoroutine\u003c/em\u003e Call protocol, but, historically, we use abbreviation \u003cem\u003eRPC\u003c/em\u003e for that. Multithreading, lightweight (green) threading or a CPS/async \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003econcurrency-supporting environment\u003c/a\u003e is recommended (but not required) for implementing HRPC clients and services.\u003c/p\u003e\n\u003cp\u003eHRPC communication may be both stateful (session-aware) and stateless. It also may have feature profiles, so certain functionality (like sessions) is not required to be supported by all implementations.\u003c/p\u003e\n\u003cp\u003eThe following is incomplete list of HRPC usecases. Note that all those cases support streaming in addition to classical RPC:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCalling code in another CPU thread. Using zero-serialized Hermes/HRPC in multi-threaded applications.\u003c/li\u003e\n\u003cli\u003eCalling code in another process (IPC). Immutable Hermes documents are position-independent, so can be shared between different address spaces.\u003c/li\u003e\n\u003cli\u003eCalling OS kernel functions from code using \u003ca href="https://en.wikipedia.org/wiki/Io_uring"\u003eio-uring\u003c/a\u003e-like transport. Much more convenient that syscalls in case of streaming.\u003c/li\u003e\n\u003cli\u003eCalling device functions from code. \u003ca href="/docs/applications/storage"\u003eComputational storage\u003c/a\u003e (database-in-a-drive) made much easier.\u003c/li\u003e\n\u003cli\u003eCalling kernels in accelerators from code. \u003ca href="https://semiconductor.samsung.com/news-events/tech-blog/hbm-pim-cutting-edge-memory-technology-to-accelerate-next-generation-ai/"\u003eProcessing-in/near-memory\u003c/a\u003e made easier.\u003c/li\u003e\n\u003cli\u003eCalling CPU code from kernels in accelerators (including \u0026lsquo;callbacks\u0026rsquo; or dynamic endpoints).\u003c/li\u003e\n\u003cli\u003eCalling code on another machine (traditional RPC) via network middleware.\u003c/li\u003e\n\u003cli\u003eCalling device from code or code from device on another machine.\u003c/li\u003e\n\u003cli\u003eCalling code from device on another machine.\u003c/li\u003e\n\u003cli\u003eCalling OS kernel functions from local/remote accelerators, remote devices, etc\u0026hellip; OS can be run on a dedicated CPU core, leaving all other cores to applications.\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="current-implementation"\u003eCurrent Implementation\u003c/h2\u003e\n\u003cp\u003eHRPC is currently work-in-process, it has session-aware bidirectional communication implemented over TCP. The latter provides total message ordering that is overkill for HRPC and leads to higher latencies in case of packets loss.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCan work over any messaging transport. Default (and, currently, the only) implementation is using TCP. Can work on top of QUIC and HTTP/2/3, SCTP. Can work over many \u003cem\u003emessage queues\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eIs a session-based protocol. Session is initialized by client. For TCP transport, there is one HRPC Session per TCP connection.\u003c/li\u003e\n\u003cli\u003eBoth Client and Server may publish \u003cem\u003eservice endpoints\u003c/em\u003e and call them bidirectionally via Session object.\u003c/li\u003e\n\u003cli\u003eEndpoints are identified with 256-bit UIDs.\u003c/li\u003e\n\u003cli\u003eProtocol implementation is versatile and, together with ability to share immutable Hermes documents between threads in a zero-copy way, can be used for \u003cem\u003esafe structured inter-thread messaging\u003c/em\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/runtimes/include/memoria/hrpc/hrpc.hpp"\u003eheaders\u003c/a\u003e for basic API definitions. See \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/tests/hrpc"\u003eHRPC tests\u003c/a\u003e for the feature preview.\u003c/p\u003e\n\u003cp\u003eThe main practical difference between HRPC and gRPC is that the former does not require a lot of code-generation for application-level data structures. Basic types are supported by Hermes itself and \u003ca href="/docs/overview/hermes#tinyobjectmap"\u003estructured objects\u003c/a\u003e can be wrapped into flyweight C++ \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/runtimes/include/memoria/hrpc/schema.hpp"\u003ehandlers\u003c/a\u003e that will be optimized away at compile-time. HRPC does not require separate in-memory representation of messages.\u003c/p\u003e\n'}).add({id:4,href:"/docs/overview/containers/",title:"Containers",description:"",content:'\u003ch2 id="basic-information"\u003eBasic Information\u003c/h2\u003e\n\u003cp\u003eContainers in Memoria are basic units of data modelling. Idiomatically, they are very much like STL containers except that, counterintuitively, container objects do not \u003cem\u003eown\u003c/em\u003e their data, they use dedicated storage API for that. So data life-cycle is tied to the \u003cem\u003estorage\u003c/em\u003e object, not the \u003cem\u003econtainer\u003c/em\u003e object. In some cases (implementations) container objects may own their storage objects, but, typically, they don\u0026rsquo;t.\u003c/p\u003e\n\u003cp\u003eNote that there may be some terminological clash between Memoria containers discussed here and \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e containers. The latter are classical STL-like containers owning their data.\u003c/p\u003e\n\u003cp\u003eMemoria containers are block-organized. Blocks may be of arbitrary size, up to 2GB, there are no inherent limitations, but their size is typically multiple of the storage/memory block size. Like, 4K, 8K, \u0026hellip; The upper limit is storage-dependent. Disk-based storage engines do not allow blocks over 1MB in size for practical reasons.\u003c/p\u003e\n\u003cp\u003e\u003ca href="/docs/data-zoo/packed-allocator/"\u003ePacked Allocator\u003c/a\u003e is a mechanism that is used in Memoria to place data (allocate objects) in memory blocks.\u003c/p\u003e\n\u003cp\u003eBlocks are organized into linked data structures using \u003cem\u003eblock identifiers\u003c/em\u003e. Conceptually, an identifier may be of arbitrary \u003cem\u003efixed size\u003c/em\u003e type (integer, UUID, \u0026hellip;), but Memoria provides dedicated type set for that.\u003c/p\u003e\n\u003cp\u003eThe most common linked data structure used for containers is a \u003cem\u003evariant\u003c/em\u003e of B+Tree. This variant is mainly different from a \u003ca href="https://en.wikipedia.org/wiki/B%2B_tree"\u003estandard one\u003c/a\u003e is that there are no \u003cem\u003esibling links\u003c/em\u003e. There are also no \u003cem\u003eparent links\u003c/em\u003e in the tree, this is necessary for \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003epersistence\u003c/a\u003e. So, in some cases B+Trees in Memoria will be less efficient than standard ones.\u003c/p\u003e\n\u003cp\u003eLack of \u003cem\u003esibling links\u003c/em\u003e is not a big issue, because tree-walking overhead for B+Trees with large (4K+) blocks is pretty moderate.\u003c/p\u003e\n\u003cp\u003eLack of \u003cem\u003eparent links\u003c/em\u003e is more impacting, because iterators now need to keep \u003cem\u003efull path\u003c/em\u003e form root to the current node. Iterator is a stack-like data structure, not just a current block ID. A lot fo tree-updating code becomes much more complicated comparing to the variant with parent links, but this is the price we pay for having persistence, concurrency and parallelism.\u003c/p\u003e\n\u003ch2 id="concurrency-and-parallelism"\u003eConcurrency and Parallelism\u003c/h2\u003e\n\u003cp\u003eContainers in Memoria are \u003cstrong\u003enot\u003c/strong\u003e thread safe, and this is foundational design decision to make data structures simpler. All thread-safety, if any, are provided at the level of storage engines. And the main concurrency and parallelism mechanism Memoria relies on is \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003epersistent/functional\u003c/a\u003e data structures. This feature also comes with its design costs, limitations and overheads. But it also gives Memoria all of its batteries and superpowers.\u003c/p\u003e\n\u003cp\u003eSo, B+Tree-based containers in Memoria can be of two implementation types:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCopy-on-Write-based (CoW)\u003c/strong\u003e containers. Persistence is supported at the level of containers. This is the fastest option but at the expense of more complicated container design. All container types need to align with CoW semantics, that is well-encapsulated by the Framework.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEpithemeral (non-CoW)\u003c/strong\u003e containers. This type of containers do not explicitly support CoW semantics themselves, so the \u003cem\u003emay\u003c/em\u003e have parent and sibling links if necessary. But for the sake of code unification and reuse, they \u003cem\u003edon\u0026rsquo;t\u003c/em\u003e. CoW semantics may still be supported at the level of \u003cem\u003estorage engines\u003c/em\u003e. For example, there is a variant of storage engine on top of the \u003ca href="http://www.lmdb.tech/doc/"\u003eLMDB\u003c/a\u003e database that has strongly serialized CoW-based transactions. When working on top of such storage engines, containers do not need to provide their own CoW semantics.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="storage-agnostic"\u003eStorage-agnostic\u003c/h2\u003e\n\u003cp\u003eFrom a container\u0026rsquo;s perspective, block storage is completely decoupled and can be fully software-defined.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="io.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eFor more details on that see the \u003ca href="/docs/overview/storage/"\u003eStorage engines\u003c/a\u003e section.\u003c/p\u003e\n\u003ch2 id="definitive-example"\u003eDefinitive Example\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// We will be using in-memory store\n#include \u0026lt;memoria/api/store/memory_store_api.hpp\u0026gt;\n// And a simple Set\u0026lt;\u0026gt; container\n#include \u0026lt;memoria/api/set/set_api.hpp\u0026gt;\n// Static initialization stuff\n#include \u0026lt;memoria/memoria.hpp\u0026gt;\n\nusing namespace memoria;\n\nint main(void) {\n    // Create a store first. All data is in a store object.\n    // We will be using an in-memory store\n    auto store = create_memory_store();\n\n    // In-memory store is a confluently-persistent CoW-enabled store,\n    // so it suppoerts Git-like branching for containers.\n    // We are opening new snapshot from the master branch.\n    auto snapshot = store-\u0026gt;master()-\u0026gt;branch();\n\n    // Now let\'s create a container, it will be a set of short strings.\n    // First, we need to define a datatype for container:\n    using DataType = Set\u0026lt;Varchar\u0026gt;;\n    // See Hermes docs for more information about datatypes.\n\n    // Now lets create a new set container in our snapshot\n    auto set_ctr = create\u0026lt;DataType\u0026gt;(snapshot, DataType{});\n\n    // .. and insert a few strings into it\n    for (size_t c = 0; c \u0026lt; 10; c++) {\n        set_ctr-\u0026gt;upsert(std::string(\u0026quot;Entry for \u0026quot;) + std::to_string(c));\n    }\n\n    // Now we are ready to iterate over inserted entries.\n    set_ctr-\u0026gt;for_each([](auto entry){\n        println(\u0026quot;{}\u0026quot;, entry);\n    });\n\n    // After we are done with inserting data, we can commit the snapshot\n    // so it will be avaliable for other threads to branch from.\n    snapshot-\u0026gt;commit();\n\n    // And we can store the data into a file\n    store-\u0026gt;store(\u0026quot;set-data.mma\u0026quot;);\n\n    // Here all the data will be destroyed in memory,\n    // but will remain in the file\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis example mentions using \u003ca href="/docs/overview/hermes/#datatypes"\u003eHermes datatypes\u003c/a\u003e. Container type is defined by its \u003cem\u003eDatatype\u003c/em\u003e, that is, basically a combination of C++ class and some \u003cem\u003estate\u003c/em\u003e, that will be shared between all objects of this datatype. Note that all datatypes in Memoria are currently stateless, so they look exactly like C++ classes.\u003c/p\u003e\n\u003cp\u003eWorking code this this example can be found \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/examples/simple_set.cpp"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="compositionality-and-instantiation-of-containers"\u003eCompositionality and Instantiation of Containers\u003c/h2\u003e\n\u003cp\u003eSTL containers are pretty lightweight and are instantiated at the site of usage. They support composition, so we can easily define composite multimap container like \u003ccode\u003estd::map\u0026lt;int64_t, std::vector\u0026lt;std::string\u0026gt;\u0026gt;\u003c/code\u003e. Memoria containers use b+trees aiming to support external memory and have no such luxury of composition. Moreover, arbitrary C++ objects like \u003ccode\u003estd::string\u003c/code\u003e \u003cem\u003ecan\u0026rsquo;t be\u003c/em\u003e used as datatypes, because their memory management is incompatible with internal b+tree machinery. So, instead of STL\u0026rsquo;s compositionality, Memoria provides complex, datatype-optimized containers. Like, STL implementation of multimap becomes \u003ccode\u003eMultimap\u0026lt;Int64, Varchar\u0026gt;\u003c/code\u003e. Framework provides complex containers for many practical use cases.\u003c/p\u003e\n\u003cp\u003eMemoria containers are instantiated in a library and usually comes in a pre-compiled form. The problem is that pre-instantiation of container for all combinations of datatypes is infeasible. Memoria libraries provide the most idiomatic and commonly used ones, and applications are free to instantiate their own variants, it\u0026rsquo;s fully supported.\u003c/p\u003e\n\u003cp\u003eThe following line may look like an instantiation of a template at the call site:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eauto set_ctr = create\u0026lt;DataType\u0026gt;(snapshot, DataType{});\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBut there is no actual instantiation here. Instead, datatype\u0026rsquo;s metrics (typehash code) is used to find actual instantiation in the instantiations registry.\u003c/p\u003e\n\u003cp\u003eIn other for this mechanism to be really lightweight at the call-cite and work properly, containers need to have abstract \u003cem\u003epublic\u003c/em\u003e virtual interfaces and \u003cem\u003eprivate\u003c/em\u003e implementations that are hidden from the application code.\u003c/p\u003e\n\u003cp\u003eContainers have pretty complex lifecicle and metadata systems, as well as integration with Hermes and datatypes, so developing a new container may be a challenge. Memoria uses in-house \u003ca href="/docs/overview/mbt/"\u003eBuild Tool\u003c/a\u003e to automate these processes.\u003c/p\u003e\n\u003ch2 id="metaprogramming-framework"\u003eMetaprogramming Framework\u003c/h2\u003e\n\u003cp\u003eThis is one of the most complex part of the Framework, especially because C++ isn\u0026rsquo;t that good with \u003cem\u003emetaprogramming in large\u003c/em\u003e. There were no much better alternatives back in the days when Memoria started, there are not that many of them now. This issue is going to be addressed low-level languages that support homoiconic compile-time metaprogramming, like Zig and (as it\u0026rsquo;s being promised) Mojo. Memoria\u0026rsquo;s \u003ca href="/docs/overview/vm"\u003eDSL subsystem\u003c/a\u003e is also addressing this issue too. Nevertheless, for container-level metaprogramming we currently only have what \u003cem\u003elatest\u003c/em\u003e C++ standard is offering.\u003c/p\u003e\n\u003cp\u003eMemoria Containers are build using Partial Class programming pattern. Partial Class definition may be split between many files. Containers classes are built from \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/containers/set/container"\u003eparts\u003c/a\u003e using the information provided from three places:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eContainer \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/prototypes"\u003eprototype\u003c/a\u003e. Prototype is a complex, Memoria-specific, form of a \u003cem\u003ebase class\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eContainer Datatype, like \u003ccode\u003eSet\u0026lt;Varchar\u0026gt;\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers-api/include/memoria/profiles"\u003eProfile\u003c/a\u003e. Profiles are an elaborate system of type traits (configurations) that define various \u003cem\u003ecommon\u003c/em\u003e parameters, like CoW/non-Cow, type of block identifier, type of snapshot identifier and so on. Dozens of them.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers/include/memoria/prototypes/bt/bt_factory.hpp"\u003eType Factory\u003c/a\u003e is a metaprogramming engine building container classes out of all this stuff above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="source-code-entry-points"\u003eSource code entry points\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers-api/include/memoria/api"\u003ePublic Containers API\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/containers"\u003ePrivate Containers Implementations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/codegen/include/codegen_memoria.hpp"\u003eDefault Container Instantiations\u003c/a\u003e (Build Tool metadata)\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:5,href:"/docs/overview/storage/",title:"Storage Engines",description:"",content:'\u003cp\u003eMemoria has pluggable storage engines. Block storage API is isolated behind the \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers-api/include/memoria/core/container/store.hpp#L132"\u003eIStore\u003c/a\u003e interface. Block life-cycle in the code is \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers-api/include/memoria/profiles/common/block_cow.hpp#L30"\u003emanaged\u003c/a\u003e with RAII. As far as some block is used (directly or \u003cem\u003eindirectly\u003c/em\u003e referenced) in the code, a storage engines knows about that and my, for example, keep it in a cache.\u003c/p\u003e\n\u003cp\u003eThere are four \u0026lsquo;core\u0026rsquo; storage engines in Memoria, covering basic usage: in-memory, disk-based OLTP-optimized, analytics-optimized and \u0026lsquo;static immutable files\u0026rsquo;. The may be many other, for secondary purposes, like, embedding into the executable image, and so on.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE THAT (1) all storage engines are currently in the experimental state, features declared here may be missing or incomplete, and (2) no any on-disk data format stability is \u003cem\u003ecurrently\u003c/em\u003e implied. Physical data layout may and will be changing from version to version.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id="persistent-data-structures"\u003ePersistent Data Structures\u003c/h2\u003e\n\u003cp\u003eA data structure is \u003cstrong\u003epersistent\u003c/strong\u003e if update operations (a single one or a batch of) produce new version of data structure. Old version may be kept around or removed as soon as they are not in use. Read access to old versions does not interfere with write access creating new versions. So very few if any synchronization between threads is required. Persistent data structures are mostly wait-free, that makes them a good fit for concurrent environments (both distributed and multi-core single-machine).\u003c/p\u003e\n\u003cp\u003eVirtually every dynamic data structure can be made persistent, but not every one can be made persistent \u003cem\u003enatively\u003c/em\u003e the way that creating a new version does not result in copying of entire data structure. For example, there are efficient copy-on-write (CoW) based algorithms for persistent trees without parent links (see below). But not for trees with parent or sibling links, linked lists or graphs. Fortunately, there is a way to \u0026ldquo;stack\u0026rdquo; non-persistent data structures on top of a persistent tree, and as a result, the former gains persistent properties of the latter by the expense of an \u003cem\u003eO(log N)\u003c/em\u003e extra memory accesses. Memoria follows this way.\u003c/p\u003e\n\u003ch3 id="persistent-trees"\u003ePersistent Trees\u003c/h3\u003e\n\u003cp\u003ePersistent balanced tree is very much like ordinary (non-persistent) tree, except instead of updating tree in-place, we create a new tree that shares the most with \u0026ldquo;parent\u0026rdquo; tree. For example, updating a leaf results in copying the entire path form the root to the leaf into a new tree:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere, balanced tree for Version 1 consists from yellow path and from the rest of Version\u0026rsquo;s 0 tree \u003cem\u003eexcluding yellow path\u003c/em\u003e. Version\u0026rsquo;s 2 tree consists from blue path and the rest of Version\u0026rsquo;s 1 tree \u003cem\u003eexcluding blue path\u003c/em\u003e. For insertions and deletions of leafs the idea is the same: we are copying modified path to a new tree and referencing the the rest on the old tree.\u003c/p\u003e\n\u003ch3 id="atomic-commitment"\u003eAtomic Commitment\u003c/h3\u003e\n\u003cp\u003eUpdates never touch old versions (CoW). Many update operations can be combined into a single version, effectively forming an atomic \u003cem\u003esnapshot\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-memory-p.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIf writers buffer a series of updates in a thread-local memory and, on update completion, publishes those updates to shared memory atomically, then we have \u003cstrong\u003esnapshot atomic commitment\u003c/strong\u003e. Other readers will see either finished and consistent new version of the data or nothing. So, if a version fits into local memory of a writer thread and we never delete versions, we have \u003ca href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#G-Set_(Grow-only_Set)"\u003eGrow-Only Set CRDT\u003c/a\u003e, that works well without explicit synchronization.\u003c/p\u003e\n\u003ch2 id="transactional-operations"\u003eTransactional Operations\u003c/h2\u003e\n\u003cp\u003eNote that versions in fully-persistent data structures are \u003cem\u003enot\u003c/em\u003e transactions, despite providing perfect isolation and atomicity. Each update (or a group of updates) create a new \u003cem\u003eversion\u003c/em\u003e of data, and to have transactions we must be able either:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSelect one version out of many at the transaction start time, or\u003c/li\u003e\n\u003cli\u003eMerge multiple versions into a single one at the commit time.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut \u003cem\u003ewithin a single line of history\u003c/em\u003e, versions are essentially transactions.\u003c/p\u003e\n\u003ch3 id="reference-counting-and-garbage-collection"\u003eReference Counting and Garbage Collection\u003c/h3\u003e\n\u003cp\u003eDeletion of versions is special. To delete a version means to delete every persistent tree\u0026rsquo;s node that is not referenced in other versions, down to the leaves:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-tree-delete.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we are deleting Version\u0026rsquo;s 0 root node, as well as green and blue nodes, because they are overshadowed by corresponding paths in other versions. To determine which node is still referenced in other versions, we have to remember \u003cem\u003ereference counter\u003c/em\u003e for each node. On the deletion of a version, we recursively decrement nodes\' reference counters and delete them physically, once counters reach the zero.\u003c/p\u003e\n\u003cp\u003eIn case of deletions, persistent tree\u0026rsquo;s nodes are still a \u003ca href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#2P-Set_(Two-Phase_Set)"\u003e2P-Set CRDT\u003c/a\u003e, so insertions and deletions do not require synchronization. Unfortunately, reference counting requires linearizable memory model for counters. Memory reclamation in persistent trees does not scales well because of linerizability requirements for reference counting. But in practice it may or may not necessary be a problem in a concurrent environment, depending on the workload the memory reclamation is put on the shared memory subsystem.\u003c/p\u003e\n\u003cp\u003eNote that \u003cem\u003emain-memory\u003c/em\u003e persistent and functional data structures (found in many functional languages) usually rely on the runtime-provided garbage collection to reclaim unused versions. Using \u003cem\u003ereference counting\u003c/em\u003e to track unused blocks in Memoria may be seen as a form of deterministic garbage collection.\u003c/p\u003e\n\u003ch3 id="stacking-on-top-of-persistent-tree"\u003eStacking on Top of Persistent Tree\u003c/h3\u003e\n\u003cp\u003eStacking non-persistent dynamic data structures (No-CoW) on top of persistent tree is straightforward. Memoria transforms high-level data containers to a key-value mapping in a form of BlockID-\u0026gt;Data. This key-value mapping is then served through persistent tree, where each version is a \u003cstrong\u003esnapshot\u003c/strong\u003e or immutable point-in-time view to the set of low-level containers.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that Momoria currently does not have in-house storage engines providing CoW semantics at the storage level.\u003c/p\u003e\n\u003ch3 id="snapshot-history-graph"\u003eSnapshot History Graph\u003c/h3\u003e\n\u003cp\u003eVersions in persistent tree need not to be ordered linearly. We can \u003cem\u003ebranch\u003c/em\u003e new version from any other \u003cem\u003ecommitted\u003c/em\u003e version, effectively forming a tree- or graph-like structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-history-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere, in Memoria, any path from root snapshot to a leaf (Head) is called a \u003cstrong\u003ebranch\u003c/strong\u003e. A data structure is \u003cstrong\u003efully persistent\u003c/strong\u003e (or, simply, persistent) if we can branch form any version. If we can merge versions in some way, the data structure is \u003cstrong\u003econfluently persistent\u003c/strong\u003e. Memoria doesn\u0026rsquo;t use confluently persistent trees under the hood. Instead, data blocks can be copied or imported from one snapshot to another, effectively providing the same function to applications.\u003c/p\u003e\n\u003cp\u003eIf a writer may branch only from the single head, or, there is only one linear branch in the history graph, such data structure is called \u003cstrong\u003epartially persistent\u003c/strong\u003e. This apparent limitation may be useful in certain scenarios (see below).\u003c/p\u003e\n\u003cp\u003eNote that at the level of containers merging operation is not defined in a general case. It might be obvious how to merge, say, relational tables because they are \u003cem\u003eunordered sets\u003c/em\u003e of rows in the context of OLTP transactions. And this type of merge can be fully automated. But it\u0026rsquo;s not clear how to merge ordered data structures like arbitrary text documents or vectors.\u003c/p\u003e\n\u003cp\u003eBecause of that, Memoria does not provide complete \u003cem\u003emerge\u003c/em\u003e operation at the level of snapshots. But it provides special facilities to define and check for write-write ad read-write conflicts (conflict materialization). Living the final decision and merge responsibility to applications.\u003c/p\u003e\n\u003cp\u003eFully automated confluent persistence requires using CRDT-like schemes on top of containers.\u003c/p\u003e\n\u003ch3 id="single-writer-mutiple-readers"\u003eSingle Writer Mutiple Readers\u003c/h3\u003e\n\u003cp\u003eSo-called \u003cem\u003eSingle Writer Multiple Readers\u003c/em\u003e, or SWMR for short, transactions can be build on the top of Memoria\u0026rsquo;s snapshots pretty easy. In the SWMR scheme there is only one writer at a time but can be multiple readers accessing already committed data. To support SWMR transactions we need a lock around snapshot history branch\u0026rsquo;s \u003cem\u003ehead\u003c/em\u003e to linearize all concurrent access to it:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="transactions-swmr.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eDespite being non-scalable in theory, because of the lock, SWMR scheme for transactions may show pretty high practical performance in certain cases:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWhen write transactions are short: point-like queries + point-like updates like for moving money from one account to another.\u003c/li\u003e\n\u003cli\u003eWhen writes does not depend on reads like in streaming: firehose is a writer ingesting events into the store, readers perform long-running analytical queries on \u0026ldquo;most recent\u0026rdquo; snapshots with point-in-time semantics.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThere are two main reasons for SWMR high performance:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHistory lock is taken only for the short period of time, just to protect snapshot history form concurrent modifications.\u003c/li\u003e\n\u003cli\u003eOtherwise, no additional locks are involved, except, possible, implicit locks in IO and caching subsystems of computers.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFor long-running readers the cost of these locks is amortized, so they can access dynamic shared data with efficiency of local immutable files.\u003c/p\u003e\n\u003ch3 id="multiple-writer-multiple-readers"\u003eMultiple Writer Multiple Readers\u003c/h3\u003e\n\u003cp\u003eIf read+write transactions may be long, but not interfere much with each other, we can execute them in separate branches. And, \u003cem\u003eif\u003c/em\u003e after completion, there are no read/write and write/write conflicts, just merge them into a single snapshot (new head). Conflicting transactions may be just rolled back automatically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="transactions-mwmr.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThis is actually how MVCC-based relational DBMS work under the hood. What we need for MWMR transactions is to somehow describe conflicting set and define corresponding merge operations for conflicting containers.\u003c/p\u003e\n\u003cp\u003eSWMR transactions are lightweight, performant and can be implemented on top of a linear snapshot history graph without much effort. But they must be short for keeping transaction latencies low. MWMR scheme can run multiple write transactions concurrently, but imply that \u003cem\u003esnapshot merging\u003c/em\u003e is fast and memory-efficient. Moreover, conflict materialization also consumes space and time, and neither of these are necessary for SWMR transactions.\u003c/p\u003e\n\u003ch3 id="streaming--batching"\u003eStreaming + Batching\u003c/h3\u003e\n\u003cp\u003eAs it has been said, SWMR scheme is streaming-friendly, allowing continuous stream of incoming updates and point-in-time view semantics for readers at the same time. The latter is implemented with minimum of local and distributed locks and allows read-only data access that is almost as efficient as to immutable local files.\u003c/p\u003e\n\u003cp\u003eOnce snapshot is committed, it will never be changed again. So, we can run iterative algorithms on the data without expecting that this data may change between iterations. At the same time, updates can be a accumulated in upcoming snapshots. And once readers are done with current ones, they can switch to the most recent snapshot, picking up the latest changes. So, updates can be ingested incrementally and processed as soon as they arrive and ready.\u003c/p\u003e\n\u003ch2 id="core-storage-engines"\u003eCore Storage Engines\u003c/h2\u003e\n\u003ch3 id="in-memory-store"\u003eIn-Memory Store\u003c/h3\u003e\n\u003cp\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/stores-api/include/memoria/api/store/memory_store_api.hpp"\u003eIMemoryStore\u003c/a\u003e is the main, the most feature-rich \u003cem\u003ecore\u003c/em\u003e storage engine in Memoria, and it\u0026rsquo;s the fastest option. Other storage engines \u003cem\u003emay be\u003c/em\u003e limited in functionality in one or another way. It\u0026rsquo;s a \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003econfluently-persistent\u003c/a\u003e store with CoW implemented at the level of containers.\u003c/p\u003e\n\u003cp\u003eStore is transactional (within a single branch) and multi-threaded, MWMR, and is best suitable for compute-intensive tasks when data is well-fit into memory of a single machine and \u003cem\u003edurability\u003c/em\u003e of single snapshots is not required.\u003c/p\u003e\n\u003ch3 id="swmrstore"\u003eSWMRStore\u003c/h3\u003e\n\u003cp\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/stores-api/include/memoria/api/store/swmr_store_api.hpp"\u003eSWMRStore\u003c/a\u003e is an SSD-optimized disk-based storage engine with durable commits. It supports basically the same set of essential functions as the in-memory store, but there may be only one writing transaction active at a time. History (series of snapshots) and branches are supported. Writers do not interfere with Readers (no locks). The following list summarizes this store\u0026rsquo;s features:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUses reference counting for memory management. Every snapshot can be deleted from history independently from others. But see remark (*) below.\u003c/li\u003e\n\u003cli\u003eOptimized mainly for analytics (read-intensive ops, streaming, batching) but also for good transactional performance.\u003c/li\u003e\n\u003cli\u003eSupports relaxed durability when only certain snapshots are marked as durable speeding up commits on consumer-grade SSDs. In case of a crash, the store will recovered up to the last durable snapshot.\u003c/li\u003e\n\u003cli\u003eSupport multi-phase commit protocols. SWMRStore instances can participate in \u003cem\u003edistributed transactions\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eAlthough different writers are serialized, writer itself \u003cem\u003emay be\u003c/em\u003e parallel, by opening multiple (sub-)snapshots and buffering writes in memory. This mode of operation is useful for heavy-weight data transformation operations and will be supported in the future.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003eSingle-Writer\u003c/code\u003e is actually not a \u003cem\u003efundamental\u003c/em\u003e limitation. If writers are serialized, there are pretty good algorithms for managing external (block) memory. Because of that, SWMRStore does not need an underling filesystem to allocate blocks, it may work on top of raw block devices (primary mode for a high-performance setup). MWMR mode can be implemented on top of any \u0026lsquo;read your writes\u0026rsquo; Key/Value store, the problem is that in order to support crash recovery we will have to either scan \u003cem\u003ethe entire repository\u003c/em\u003e to identify orphaned blocks, or accumulate them in a separate store, merge it in at commit time and provide recovery metadata. It may have sense in some cases, but currently MWMR is not a part of the core set of storage engines for block devices.\u003c/p\u003e\n\u003cp\u003e(*) Note that reference counters are not persistent and are not stored on each commit. Because of that, SWMRStore does not provide zero-time recovery. In case of crash or improper shutdown, we have to scan the store partially to rebuild counters. Fortunately, the amount of information that needs to be scanned this way is rather small (much less that 1%) and the store is readable at this time. Counters also take main memory, about 4 bytes per block.\u003c/p\u003e\n\u003ch3 id="oltpstore"\u003eOLTPStore\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003e(Note that this storage engine type is WIP and is not yet available for experiments)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOLTPStore is the main OLTP-optimied storage engine. It\u0026rsquo;s using the same memory management algorithm as \u003ca href="http://www.lmdb.tech/doc/"\u003eLMDB\u003c/a\u003e but implemented with means of Memoria containers. LMDB uses persistent CoW b-trees for its databases but it \u003cem\u003edoes not\u003c/em\u003e use counters for tracking references to blocks. In LMDB, when we clone a block, the cloned block\u0026rsquo;s ID is put into so-called \u0026lsquo;free list\u0026rsquo; under transaction ID this block was created in. This block will become eligible for reuse when when all readers which are alder than this block\u0026rsquo;s TxnID terminate. So, the snapshot history is cleared only from its \u003cem\u003etail\u003c/em\u003e. The main limitation is that long-running \u003cem\u003ereader\u003c/em\u003e will be blocking memory reclamation. Neither LMDB nor OLTPStore are suitable for analytical workloads (long-running queries). But it, \u003cem\u003etheoretically\u003c/em\u003e (after all optimizations) may show very hight sustained transaction rates.\u003c/p\u003e\n\u003cp\u003eThe following list summarizes OLTPStore\u0026rsquo;s features and limitations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUnlike LMDB, OLTPStore \u003cem\u003edoes not\u003c/em\u003e use memory mapping. Instead, high-performance IO interfaces like Linux AIO or liburing are used instead.\u003c/li\u003e\n\u003cli\u003eUnlike SWMRStore, neither branches, nor snapshot history are supported.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id="nandstore"\u003eNANDStore\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003e(Note that this storage engine type is WIP and is not yet available for experiments)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis is an experimental variant of OLTPStore working on top of simplified \u003cem\u003evirtualized\u003c/em\u003e NAND flash chips instead of block device interface. The main purpose of this storage engine is to research \u003cem\u003eproper layering\u003c/em\u003e between raw hardware and high-level algorithms in the context of SW/HW co-design:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMore deterministic and flexible commit and power failure handling.\u003c/li\u003e\n\u003cli\u003eExploring computational storage: partial offloading of application-level queries to the device.\u003c/li\u003e\n\u003cli\u003eComputationally-assisted storage. Running storage-level queries in the device to improve application-level data management.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNote that the code of this storage engine is not intended to be used as an SSD firmware. A lot of additional R\u0026amp;D will be needed for that to happen.\u003c/p\u003e\n\u003ch3 id="overlaystore"\u003eOverlayStore\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003e(Note that this storage engine type is WIP and is not yet available for experiments)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis storage engine is very similar to the MemoryStore except that every snapshot is a separate storage unit (mainly, a file).\u003c/p\u003e\n\u003cp\u003eTBC \u0026hellip;\u003c/p\u003e\n\u003ch3 id="note-on-high-availability"\u003eNote on High Availability\u003c/h3\u003e\n\u003cp\u003eIt can be expected that persistent data structures, providing \u003cem\u003elinearized\u003c/em\u003e update history out of the box within a single branch, are well-suitable for replication in a distributed mode. That\u0026rsquo;s true in general, we can encapsulate a branch into a \u003cem\u003epatch\u003c/em\u003e, transfer it via network and apply in a remote repository. But in case of high-performance SSDs it\u0026rsquo;s not that simple. SSDs may support very high write speeds, dozens of GB/s if using multiple drives, but the network is \u003cem\u003emuch\u003c/em\u003e slower than that. Sending physical blocks over a network, even with good compression, will be a limiting factor. Logical, application-level, replication should be used instead. Memoria will be providing specialized containers to help tracking update history that application is doing.\u003c/p\u003e\n\u003cp\u003eDespite performance limitations, block-level physical replication will be supported out of the box. It does not require any support from applications, it comes basically \u0026ldquo;for free\u0026rdquo;, and it \u003cem\u003emay be useful\u003c/em\u003e in some cases. For example, it will be useful for replication between drives within the same machine. But high-performance system designs should not rely on it.\u003c/p\u003e\n\u003ch2 id="cow-vs-lsm"\u003eCoW vs LSM\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/Log-structured_merge-tree"\u003eLSM\u003c/a\u003e is currently the most popular data structure for database storage. LSM has some theoretical and practical properties that make them irreplaceable. Nevertheless they also have serious theoretical and practical limitations, so proper understanding of how they work may save time and money.\u003c/p\u003e\n\u003cp\u003eLSM accumulate updates in an in-memory buffer (backing them also in a circular persistent buffer aka \u0026lsquo;transaction log\u0026rsquo;) and periodically drop this buffer to a durable storage as a single sorted files \u0026ndash; segments. Background process (garbage collector, GC) then picks up those segments and merges them into a large single file. GC\u0026rsquo;s performance is the main limiting factor of the storage engine. LSM have the following main properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eIt may handle very high peak write speeds\u003c/em\u003e, may be 10x-100x of the average rate. This is the reason why they are irreplaceable for internet applications (web-stores, etc) that should be able to handle \u003cem\u003esudden\u003c/em\u003e massive inflows of users.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorst-case write complexity is \u003ccode\u003eO(N)\u003c/code\u003e\u003c/em\u003e. Applications may see extremely high update latencies, that is hard to mitigate. Read latency may also spike, see below.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWe can trade write performance for read performance\u003c/em\u003e. If we merge segments not that aggressively, write performance will be better, but read performance will be degrading.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey rely on an underling filesystem for managing disk space\u003c/em\u003e. Filesystem\u0026rsquo;s performance may be a limiting factor, for example, it may introduce additional read/write latencies.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey require reserving a lot of extra free space (1-2x) for merges\u003c/em\u003e in a worst case.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey theoretically have relatively low write amplification factor\u003c/em\u003e for SSDs, because all updates to disk are sequential.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBy and large, LSMs are very good in their niche, and sometimes they are the only option that fits the requirements (peak write speeds). But they also require extensive \u003cem\u003eexpert-level\u003c/em\u003e tuning and monitoring.\u003c/p\u003e\n\u003cp\u003eCoW trees are different:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTheir peak performance is lower than LSM, but their worst-case complexity is \u003cem\u003elogarithmic\u003c/em\u003e, instead of linear.\u003c/li\u003e\n\u003cli\u003eThey are block-structured and do not need an underling filesystem to allocate the space from. That make worst-case performance even more predictable.\u003c/li\u003e\n\u003cli\u003eThey do not \u003cem\u003eneed\u003c/em\u003e a GC, but specific implementations may use asynchronous space reclamation strategy for best-case performance reasons. Anyway, GC for CoW trees is much simpler than GC for LSM trees. CoW trees have much smaller tunable parameters space.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIdeally, the storage engine should support both data structures, for different cases and purposes. Memoria is using CoW exclusively and may implement LSM with means of the Framework on top of the OLTPStore as an underling filesystem, but if we \u003cem\u003eneed\u003c/em\u003e LSM, we can try using RocksDB first and it doesn\u0026rsquo;t fit, resort to a \u0026lsquo;native\u0026rsquo; solution.\u003c/p\u003e\n\u003ch2 id="distributed-vs-decentralized"\u003eDistributed vs Decentralized\u003c/h2\u003e\n\u003cp\u003eMemoria \u003cem\u003eis not\u003c/em\u003e explicitly addressing distributed environments and scale-out settings. Persistent data structures (PDS) may \u003cem\u003eseem\u003c/em\u003e working well here: eventually consistent K/V store would be enough to host blocks, the problem is in the \u003cem\u003ememory management\u003c/em\u003e. PDS require either deterministic (ARC-based) or tracing garbage collector that is pretty a challenge to build for a distributed environment. And, the most important, it has to be finely tuned to the specifics of selected hardware and application\u0026rsquo;s requirements.\u003c/p\u003e\n\u003cp\u003eNevertheless, Memoria is explicitly addressing \u003cem\u003edecentralized\u003c/em\u003e cease, when there is no single unit of control over a distributed environment. For Memoria\u0026rsquo;s perspective, decentralized environment is a mesh of nodes (or small clusters) running local SWMRStore-based engines and exchanging data \u003cem\u003eexplicitly\u003c/em\u003e with using \u003cem\u003epatches\u003c/em\u003e. Such architecture will be somewhat slower and more complex form the application\u0026rsquo;s perspective (much more control is required) but it does not need a distributed GC. Currently it \u003cem\u003eseems\u003c/em\u003e to be much more universal and fits both local and distributed usage cases.\u003c/p\u003e\n'}).add({id:6,href:"/docs/overview/runtime/",title:"Runtime Environments",description:"",content:'\u003cp\u003eMemoria tries to be a runtime-agnostic wherever it\u0026rsquo;s possible. There are two main dependencies: memory allocation and IO. Custom memory allocation is a rather easy thing. High-performance IO is much harder because we need facilities which are \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003enot properly supported\u003c/a\u003e by the programming language, operation system or both. Memoria relies on fibers for concurrency and they are expected to be \u003ca href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p0876r13.pdf"\u003esupported\u003c/a\u003e for C++26 and is already available in various frameworks and libraries.\u003c/p\u003e\n\u003cp\u003eThe goal of Runtime Environment (RE) is to provide efficient and deterministic UB-free code execution. Framework relies on the thread-per-core/message-passing model running non-migrating fibers. Resumable functions (aka \u0026lsquo;coroutines\u0026rsquo;) will be supported where appropriate, but they are viral and allocate dynamic memory for frames all the way down. Because of frame allocation, resumable functions are not as performant in C++, as it may be expected.\u003c/p\u003e\n\u003cp\u003eThere are 2.5 RE \u0026lsquo;backends\u0026rsquo; in Memoria:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://seastar.io/"\u003eSeastar\u003c/a\u003e framework. Best-in-class, high performance and well-supported. But \u003cem\u003eLinux-only\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eBoost Asio. Pretty high performance, perfect compatibility with various runtimes, cross platform, but no high-performance disk IO support.\u003c/li\u003e\n\u003cli\u003eReactor Engine. Initially it was meant to be in-house \u003cem\u003ecross-platform\u003c/em\u003e variant of Seastar with Boost Finer support. But later it lost its momentum and currently is meant to be replaced with combination of Seastar and Asio.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMemoria is not going to rely 100% on the Seastar or Asio API. Instead, the Framework will be trying to isolate specifics of underling API, making code porting easier. 100% cross-RE compatibility is not a goal.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThis section is currently deeply incomplete, and looks like a mess, as well as a current state of runtime subsystem in Memoria. The section will be co-improving together with the runtime environment.\u003c/em\u003e\u003c/p\u003e\n'}).add({id:7,href:"/docs/overview/vm/",title:"DSL Engine",description:"",content:'\u003ch2 id="introduction"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eHistorically, Memoria was meant to be a \u003cem\u003estorage engine\u003c/em\u003e, leaving data processing aspects to applications. After trying Memoria in various projects involving C++ and Java it was clear that it\u0026rsquo;s not a good idea. To be used at its full potential, Memoria requires its own runtime environment. Bridging different runtimes like C++ with fibers and JVM is a close to impossible task, if efficiency is a goal. After experiments with PrestoDB on top of Memoria and development of String Data Notation format, recently superseded by \u003ca href="/docs/overview/hermes/"\u003eHermes\u003c/a\u003e, it became clear that Memoria needs its own integrated \u003cem\u003equery execution engine\u003c/em\u003e. And the technical challenge is that today\u0026rsquo;s and perspective \u003cem\u003equery languages\u003c/em\u003e have evolved from non-Turing-complete DSLs into fully featured \u003cem\u003eprogramming languages\u003c/em\u003e, even with capabilities of \u003ca href="https://en.wikipedia.org/wiki/Programming_in_the_large_and_programming_in_the_small"\u003eprogramming in the large\u003c/a\u003e. So designing and building a kind of an SQL query execution engine (a pretty complex beast by itself, actually) would not be sufficient, given the project\u0026rsquo;s long-term goals. Instead, we need a generic virtual machine, heavily tuned for various query languages.\u003c/p\u003e\n\u003ch2 id="related-c-with-homoiconic-compile-time-metaprogramming"\u003eRelated: C++ with homoiconic compile-time metaprogramming\u003c/h2\u003e\n\u003cp\u003eThe early experiment on this ground was an \u003ca href="https://github.com/victor-smirnov/jenny"\u003eattempt\u003c/a\u003e to use C++ as a DSL host language by adding of a full C++ subset as a compile-time metaprogramming language into the Clang compiler. Clang is a modular and well-engineered compiler, it even can run full set of C++ in a JIT mode. Internally Clang is more like a data platform with databases and clear separation to \u003cem\u003estorage\u003c/em\u003e and \u003cem\u003ecompute\u003c/em\u003e. Currently this experiment is on pause, we need to wait and see where evolution of C++ will go. There is substantial interest around C++ \u003ca href="https://thenewstack.io/googles-carbon-among-other-potential-c-successors/"\u003esuccessor languages\u003c/a\u003e like Carbon and Cpp2/Cppfront. In addition, there is an ongoing tectonic shift in conventional commercial and opensource programming right now, induced by LLM-powered \u003cem\u003eautomatic programming\u003c/em\u003e. Old rationales (1960-70th) do not apply anymore, programming languages of tomorrow may look completely different in terms of focus and leading paradigms.\u003c/p\u003e\n\u003ch2 id="what"\u003eWhat\u003c/h2\u003e\n\u003cp\u003eIn/for Memoria we do not try to create a new general purpose \u003cem\u003esystem\u003c/em\u003e language \u003cem\u003ecompeting\u003c/em\u003e with C++ or any its successor languages. Instead, we need a highly compatible \u003cem\u003ecompanion\u003c/em\u003e language for domains where \u003ca href="https://en.wikipedia.org/wiki/Kolmogorov_complexity"\u003edescriptional complexity\u003c/a\u003e is very high, so \u0026ldquo;\u003cem\u003ecode is a new data\u003c/em\u003e\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eHere is how Memoria is going to fit this niche. There are five key features:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMemoria defines bytecode-like extensible intermediate-level \u003cem\u003eM-code\u003c/em\u003e and \u003ca href="/docs/overview/hermes/"\u003eHermes\u003c/a\u003e-backed \u003cem\u003eCode Model\u003c/em\u003e. M-code is low-level enough to be long-term stable and suitable for \u003cem\u003etooling\u003c/em\u003e (static analyzers, etc), but high-level enough to make writing DSLs simpler.\u003c/li\u003e\n\u003cli\u003eNative \u003ca href="/docs/overview/hermes/"\u003eHermes\u003c/a\u003e integration. Hermes datatypes, objects and containers are distinct but first-class elements of the language.\u003c/li\u003e\n\u003cli\u003eRich embedded metadata. Elements in the \u003cem\u003ecode model\u003c/em\u003e may have arbitrary Hermes metadata (annotations) associated with them.\u003c/li\u003e\n\u003cli\u003eHomoiconic compile-time metaprogramming and supporting \u003cem\u003emetaprogramming platform\u003c/em\u003e, integrated with runtime environment and the interpreter.\u003c/li\u003e\n\u003cli\u003eRETE-based rule engine for advanced pattern matching.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eM-code and related infrastructure are targeting the \u003cstrong\u003ein-database programming\u003c/strong\u003e problem.\u003c/p\u003e\n\u003cp\u003eGeneral-purpose programming languages are targeting \u003cem\u003ecompute-intensive\u003c/em\u003e problems that use rather simple data structures: arrays and linked data structures like object graphs. Data structures are simple and ephemeral.\u003c/p\u003e\n\u003ch3 id="m-code-and-code-model"\u003eM-code and Code Model\u003c/h3\u003e\n\u003cp\u003eMemoria does not introduce any new high-level programming language, like Java or Python. Instead, it defines intermediate-level \u003cem\u003eDSL Host\u003c/em\u003e language M-code. M-code may have two forms:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTextual, where it looks somewhat like assembly code: classes, functions, rules, \u003cem\u003ecode blocks\u003c/em\u003e and statements, metadata (something like MLIR code looks like).\u003c/li\u003e\n\u003cli\u003eStructured representation: all of these but in a dense memory-optimized Hermes documents.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLike it\u0026rsquo;s in high-level languages, there is no explicit access to the stack (no push/pop statements), but there no any \u003cem\u003esyntactic sugar\u003c/em\u003e: just typed variable declarations, simple control flow (branches/cycles/exceptions) and function calls.\u003c/p\u003e\n\u003cp\u003eM-code may use pretty large subset of normalized C++ types, but internally represents them as 192-256-bit hash codes, taken from a normalized type declaration. Collisions at this level are treated as insignificantly rare. Fixed-size type name encoding simplifies and speeds up the code model significantly.\u003c/p\u003e\n\u003cp\u003eThis design of M-code pursues two main goals:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eM-code allows efficient \u003cem\u003elowering\u003c/em\u003e to C++ (or any other suitable underling language like Rust, Carbon, Mojo or MLIR/LLVM), by using mostly local transformations: each statement of M-code becomes a statement of an underling language. Depending on the underling language, M-code program may have slightly different semantics. Perfect cross-language compatibility is \u003cem\u003enot\u003c/em\u003e a goal.\u003c/li\u003e\n\u003cli\u003eEfficient (high-performance) but lightweight \u003cem\u003eembeddable\u003c/em\u003e code interpreter, suitable for evaluating DSL expressions in a C++ code.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, M-code can be either:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInterpreted,\u003c/li\u003e\n\u003cli\u003eAOT-transpiled to C++ (maybe by building the AST directly, without parsing texts),\u003c/li\u003e\n\u003cli\u003eJIT-compiled to MLIR/LLVM. Probably the best way to run database queries.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e1 and 2 is currently the priority.\u003c/p\u003e\n\u003cp\u003eM-code may call native C++ code, but the latter needs to be described in a \u003cem\u003ecode registry\u003c/em\u003e: function type, signature, physical address, other metadata. All this information (bindings) can be automatically generated using \u003ca href="/docs/overview/mbt/"\u003eMBT\u003c/a\u003e. Native code can also call M-code, but the syntax \u003cem\u003emay be\u003c/em\u003e verbose (even for the AOT-compiled M-code).\u003c/p\u003e\n\u003cp\u003eM-code functions, classes, rules (see below), metadata, embedded resources are organized into modules, modules \u0026ndash; into assemblies. Assemblies can be linked with executables in a read-only memory segments, assemblies are just Hermes documents. Assemblies can also be AOT-compiled into native code (libraries). This native code will run at the full speed of other Memoria code, and it will be transparently invokable from interpreted M-code.\u003c/p\u003e\n\u003cp\u003eM-code is meant to be memory/thread-safe and \u003cem\u003eUB-free\u003c/em\u003e by default, but may support full unrestricted memory access in a special profile. The basic idea is simple: M-code is a \u003cem\u003ecompanion\u003c/em\u003e (domain-specific) language to C++, so let\u0026rsquo;s just move all high-performance but \u0026lsquo;unsafe\u0026rsquo; programming to C++. At the level of M-code safety is achieved by restricting the language and using run-time checks. There is no expectations that M-code, AOT-compiled to C++, will be faster than equivalent C++ code.\u003c/p\u003e\n\u003ch3 id="native-hermes-integration"\u003eNative Hermes integration\u003c/h3\u003e\n\u003cp\u003eThere are tree layers of integration:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eObject/data model\u003c/em\u003e integration. Hermes objects are distinct but first class data objects. HRPC services are first class objects, supported by the runtime.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eLanguage-level\u003c/em\u003e integration. Think dict/list comprehension of python. M-code explicitly support code blocks that can be used to model array and map (and some other) containers of Hermes.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eCode Model\u003c/em\u003e integration. M-code assemblies are just Hermes documents, so any Hermes data can be embedded with the application and instantly available to the code. Like reflection data and code annotations.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id="compile-time-metaprogramming"\u003eCompile-time metaprogramming\u003c/h3\u003e\n\u003cp\u003eC++ type-level template metaprogramming isn\u0026rsquo;t that bad, actually. It does require \u003cem\u003ea lot\u003c/em\u003e of attention every time we make a change to the meta-code, so it\u0026rsquo;s a \u003cem\u003ehigh-maintenance\u003c/em\u003e thing, but there is a lot of high-maintenance code here and there, so what\u0026rsquo;s the issue? The issue is that \u003cem\u003eadvanced\u003c/em\u003e type-level metaprogramming is not needed in the industry. For systems programming language we need simple collections and class/function specialization, this is what C++ has been pretty good at.\u003c/p\u003e\n\u003cp\u003eC++ does need advanced \u003cem\u003eAST-level\u003c/em\u003e metaprogramming, usually provided by hygienic procedural macro systems, but, again, for what? Embedding DSLs? OK. Boilerplate code generation (serialization, bindings, etc)? OK. We can use external code generators for that. It will be hard to find definitive example that may justify the need for \u003cem\u003eadvanced metaprogramming\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe point of Memoria, in this respect, is that multimodal databases do need advanced metaprogramming to craft data structures \u003cem\u003eoptimized for applications\' needs\u003c/em\u003e. Databases in general benefit from metaprogramming, but usually they are isolated blackboxes, available via simplistic and inexpressive DSLs. General audience just doesn\u0026rsquo;t know what\u0026rsquo;s happening there. Memoria is trying to escape this historical trend by \u0026lsquo;opening up\u0026rsquo; and providing it\u0026rsquo;s internals to application developers, and exposing them (developers) to high-complexity data modelling problems.\u003c/p\u003e\n\u003cp\u003eDSL Engine will be supporting compile-time metaprogramming at both type- and AST-levels. M-code interpreter is not actually an interpreter but a \u003cem\u003edynamic compiler\u003c/em\u003e paired with a \u003cem\u003ebuild system\u003c/em\u003e. Metaprograms are just regular M-code, called at the compile-time, and can be AOT-compiled for better performance.\u003c/p\u003e\n\u003ch3 id="rule-engine"\u003eRule Engine\u003c/h3\u003e\n\u003cp\u003eThere are two strategies of query execution \u0026ndash; \u003cem\u003erequest-driven\u003c/em\u003e and \u003cem\u003edata-driven\u003c/em\u003e. In the first case we have a lot of data and a small number of complex queries. Query execution is usually initiated by an external actor (user or service). In the second case we have a lot of small queries and a small amount of quickly-changing data. And each time data is changed, relevant queries are re-evaluated automatically. If they update the data, the process continues until data updating stops. The first type is better known in the literature as \u003cem\u003ebackward chaining\u003c/em\u003e (BC) strategy of evaluation, the second one \u0026ndash; as \u003cem\u003eforward chaining\u003c/em\u003e strategy of evaluation (FC).\u003c/p\u003e\n\u003cp\u003eIn general case, with BC we have some \u003cem\u003egoal\u003c/em\u003e (a query) and we need to check what is aligned with this goal (filter the data according to the query). With FC strategy we have some \u003cem\u003echange\u003c/em\u003e (event) and we need to compute what are \u003cem\u003econsequences\u003c/em\u003e of this change. Regular request-response databases use BC strategy for query evaluation. There are two main examples of FC systems:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eComplex Event Processing or CEP.\u003c/li\u003e\n\u003cli\u003eContinuous Queries (CQ), or (better known as) Stream Processing (SP).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCEP and CQ/SP target different use cases, they use different base algorithms. CEP may use a variant of RETE, that, basically, tries to find \u003cem\u003epatterns in data\u003c/em\u003e. And patterns are described as a set of rules: \u003cem\u003epattern\u003c/em\u003e-\u0026gt;\u003cem\u003ereaction\u003c/em\u003e. The most prominent opensource implementation of this approach is \u003ca href="https://www.drools.org/"\u003eDrools\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePattern-matching with RETE is pretty close to general data-flow (DF) programming, but more high-level (unlike plain DF, it has \u003ccode\u003ejoin\u003c/code\u003e operation). In this respect, conventional \u003cem\u003econtrol flow\u003c/em\u003e (CF) programming is close to backward chaining external \u003cem\u003econtrol\u003c/em\u003e is guiding the process to its \u003cem\u003egoal\u003c/em\u003e. What is interesting about RETE is that it can be hardware-accelerated. RETE\u0026rsquo;s Beta-nodes are just Cartesian product operations are similar to matrix multiplication and can be accelerated using the similar approach (\u003ca href="https://en.wikipedia.org/wiki/Systolic_array"\u003esystolic arrays\u003c/a\u003e). Moreover, RETE is well-extandable, there can be variants for probabilistic and approximate inference, hybridizing with neural networks and so on\u0026hellip;\u003c/p\u003e\n\u003cp\u003eThe goal of Memoria\u0026rsquo;s DSL Engine is to provide \u003cem\u003eintegrated\u003c/em\u003e execution environment for backward chaining strategy (regular queries, CF programs) and forward chaining (RETE-based CEP and SP).\u003c/p\u003e\n\u003ch2 id="language-kit"\u003eLanguage Kit\u003c/h2\u003e\n\u003cp\u003eM-code is an intermediate level language and is not intended for manual code authoring. For high-level languages and DSLs there is a Boost Spirit integration. The grammar may produce AST on a Hermes document format directly, without intermediate objects. This is how HermesPath and Hermes Template engine (Jinja-like syntax) works.\u003c/p\u003e\n\u003cp\u003eBut Boost Spirit isn\u0026rsquo;t that scalable, so there are plans to integrate ANTLR4 with Hermes. The idea is to write special ALTLR4 backend producing AST in the Hermes Document format and related helper classes.\u003c/p\u003e\n\u003ch2 id="roadmap"\u003eRoadmap\u003c/h2\u003e\n\u003cp\u003eDSL Engine may look like it\u0026rsquo;s a fully featured programming \u003cem\u003eplatform\u003c/em\u003e, but this specific part of the design is \u003cem\u003ehighly experimental\u003c/em\u003e. The goal is to support \u003cem\u003eDSL programming in the large\u003c/em\u003e, with focus on advanced cases like first-class support for accelerated RETE inference engine.\u003c/p\u003e\n\u003cp\u003eDSL Engine is going to be detachable, configurable and embeddable, so other projects may also use it even if don\u0026rsquo;t need the rest of Memoria and have different runtime environments.\u003c/p\u003e\n'}).add({id:8,href:"/docs/overview/accel/",title:"Memoria Acceleration Architecture (MAA)",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eData dominates. If you\u0026rsquo;ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u0026ndash; Rob Pike in \u003ca href="http://www.lysator.liu.se/c/pikestyle.html"\u003e“Notes on Programming in C”\u003c/a\u003e, 1989.\u003c/p\u003e\n\u003ch2 id="basic-information"\u003eBasic Information\u003c/h2\u003e\n\u003cp\u003eProcessing can be compute-intensive, IO-intensive or combined/hybrid. Processing is compute intensive if each element of data is processed many times. Examples: sorting and matrix multiplication. Otherwise it\u0026rsquo;s IO-intensive. Example: hashtable with random access. Hybrid processing may contain both compute-intensive and IO-intensive \u003cem\u003estages\u003c/em\u003e, but they will be clearly separable. Like, in evaluating SQL query, JOIN is IO-intensive and SORT is compute-intensive. Physically, the more processing is compute-intensive, the less it\u0026rsquo;s IO-intensive. Just because while we are processing a data element intensively, we can\u0026rsquo;t do IO.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e(Note that common definitions of compute-/io-/memory-bound problems are applicable here but not exactly identical to compute-/io-intensity.)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eCompute/IO-intensity is not always an intrinsic property of an algorithm, but rather a combination of algorithm and \u003cem\u003ememory architecture\u003c/em\u003e. Memory architecture is \u003cem\u003eusually\u003c/em\u003e a combination of memory buffers with different size and speed. Usually, \u003cem\u003elarge\u003c/em\u003e memory can\u0026rsquo;t be \u003cem\u003efast\u003c/em\u003e. Although with current technology there may be many \u003cem\u003epartial\u003c/em\u003e exceptions from this rule. From the practical perspective, by IO we mean \u003cem\u003eoff-die traffic\u003c/em\u003e, because it\u0026rsquo;s usually 100-1000 times slower than intra-die traffic. Sometimes, on-dies memory architecture may be pretty complex, containing rather slow memory but relatively large that even \u003cem\u003emay be\u003c/em\u003e considered an IO under certain conditions. But here we are not counting these cases, just for simplicity.\u003c/p\u003e\n\u003cp\u003eEach algorithm is characterized by some access pattern that can be \u003cem\u003epredictable\u003c/em\u003e (e.g., linear or regular) or \u003cem\u003erandom\u003c/em\u003e, or mixed. In general, we can find regularities in memory access patters, either \u003cem\u003estatically\u003c/em\u003e or \u003cem\u003edynamically\u003c/em\u003e, and utilize them to optimize data structures placement in memory logically and physically, we can keep most memory access intra-die, minimizing extra-die IO.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAutomatic strategy use memory access caching and prefetching.\u003c/li\u003e\n\u003cli\u003eManual strategy use various memory buffers as a fast SRAM and move data between them manually.\u003c/li\u003e\n\u003cli\u003eHybrid strategy implies using both caches and scratchpad memory, as well as explicit memory prefetching.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCaching and prefetching is, by far, the most popular way to reduce memory access latency. It works pretty well in many practically important cases. But caching has its drawbacks too:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCaching isn\u0026rsquo;t free. \u003cem\u003eCache miss\u003c/em\u003e costs dozens of cycles and \u003cem\u003ecache hit\u003c/em\u003e isn\u0026rsquo;t free either (trying address lookup table). Raw scratchpad SRAM may be much faster in the \u003cem\u003ebest case\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eCaching doesn\u0026rsquo;t co-exist well with Virtual Memory because it needs to take address translation into account. Switching contexts invalidate caches (either cache or address translation), that may degrade performance several times.\u003c/li\u003e\n\u003cli\u003eCaching of mutable data isn\u0026rsquo;t scalable with the number of cores because of cache-coherency traffic.\u003c/li\u003e\n\u003cli\u003eTo maximize performance under rather irregular memory access latency we need sophisticated OoOE cores, which are large, hot and expensive to engineer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhile raw DDR5 DRAM access latency is around 25-40 ns, the full system latency, or the time needed to move data through the memory hierarchy, is around 75 ns, that is more than 2 times higher. These numbers don\u0026rsquo;t take into account virtual memory effects like TLB misses, which may again push the system latency up several times.\u003c/p\u003e\n\u003cp\u003eInter-core communication is mostly done via caches and coherency traffic. One way latencies may be from 5 ns for two \u003cem\u003evirtual\u003c/em\u003e cores (SMT) to \u003ca href="https://chipsandcheese.com/2023/11/07/core-to-core-latency-data-on-large-systems"\u003ehundreds os nanoseconds\u003c/a\u003e in case of multiple sockets. Average latency is pretty high \u0026ndash; around dozens of nanoseconds. What is the worst, it that those numbers may be significantly higher when all cores start talking to each other. Performance may be minuscule if underling Network-on-Chip (NoC) can\u0026rsquo;t handle this coherency traffic gracefully. Even garbage collection via atomic reference counting may be a pathological case for these type of memory architectures.\u003c/p\u003e\n\u003cp\u003eExisting Linux-compatible OoOE multicore CPUs, despite currently being the best way to run latency-sensitive workloads like databases, symbolic reasoners, constraint solvers, etc, aren\u0026rsquo;t really being optimized for that. Slow inter-core communication makes them not that suitable for fine-grained \u003cem\u003edynamic parallelizm\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="memoria-containers"\u003eMemoria Containers\u003c/h2\u003e\n\u003cp\u003eMemoria relies heavily on metaprogramming techniques used for design-space exploration of algorithms and data structures \u0026ndash; so called \u003cem\u003egeneric programming\u003c/em\u003e. In C++ we are using template metaprogramming for that, but, despite being Turing-complete, it has serious limitations. New modern programming languages like Mojo and Zig provide the full language subset for compile-time metaprogramming. \u003ca href="https://github.com/victor-smirnov/jenny"\u003eJenny\u003c/a\u003e, Memoria\u0026rsquo;s assisting Clang-based C++ compiler (used for \u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e and co-design tools for code targeting RISC-V accelerators) also supports calling arbitrary functions at compile time. But currently Memoria relies only on C++ template metaprogramming for design space explorations.\u003c/p\u003e\n\u003cp\u003eMemoria\u0026rsquo;s \u003ca href="/docs/overview/containers"\u003eContainer\u003c/a\u003e is the main structured data abstraction unit. Containers internally have block-based structure and represented as B+Trees, either ephemeral or \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003epersistent\u003c/a\u003e (multi-version). Basically, any data structure that can be (efficiently) represented as an array, can be (efficiently) represented as a container. In Memoria, we use metaprogramming to build application-specific containers out of \u003cem\u003ebasic building blocks\u003c/em\u003e using \u003cem\u003especifications\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThere are five \u003ca href="/docs/data-zoo/overview"\u003ebasic building blocks\u003c/a\u003e, all arrays support both fixed- and variable-length elements:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUnsorted array.\u003c/li\u003e\n\u003cli\u003eSorted array.\u003c/li\u003e\n\u003cli\u003eArray-packed \u003ca href="/docs/data-zoo/partial-sum-tree"\u003eprefix sums tree\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eArray-packed \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable sequence\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eArray-packed \u003ca href="/docs/data-zoo/compressed-symbol-seq"\u003ecompressed symbol sequence\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBelow is a schematic representation of searching through multi-ary search tree:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="tree-search.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eEach node of such search tree takes some space in memory, and the best performance is achieved when the size of the node is in low multiple of a CPU cache line: 32-128 bytes. For prefix sum trees search operation perform additions (accumulation) with comparison with each element of the node, for other tree types the operation may be different. Instead of doing this search in CPU cache (loading data there in the process), we can completely offload them either to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ememory controller or\u003c/li\u003e\n\u003cli\u003eto processing cores attached directly to memory banks on DRAM dies.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEmbedding logic into DRAM is hard (but possible), because the process is not optimized for that. But memory parallelism (throughput \u003cem\u003eand\u003c/em\u003e latency) will be the best in this case, together with energy efficiency. This is what is called \u003cem\u003eProcessing-In-Memory\u003c/em\u003e or PIM.\u003c/p\u003e\n\u003cp\u003eThe alternative is to put the logic as close to the memory \u003cem\u003echips\u003c/em\u003e as possible, probably right on the memory modules or into the CXL controller. Throughput will be lower, parallelism too, latency somewhat higher. But we can leverage existing manufacturing processes, so solution will be cheaper initially. This is what is called \u003cem\u003eProcessing-Near-Memory\u003c/em\u003e or PNM.\u003c/p\u003e\n\u003cp\u003eThe point is that, by and large, accelerating container require as much memory parallelism as possible and corresponding number of xPU is put as close to the physical memory as possible. Existing accelerators, optimizing for neural networks (matrix multiplication), do not optimize for that (for latency). Because matrix multiplication is \u003cem\u003elatency-insensitive\u003c/em\u003e. Memoria applications need a separate class of accelerators, maximizing effective \u003cem\u003ememory parallelism\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="persistent-data-structures"\u003ePersistent Data Structures\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003ePersistent data structures\u003c/a\u003e or PDS are usually implemented as trees, and Memoria follows this way. PDS are very good for parallelism, because committed versions are immutable, and immutable data can easily be shared (cached) between parallel processing units without any coordination. Nevertheless, PDS require \u003cem\u003egarbage collection\u003c/em\u003e, either deterministic (via atomic reference counting) or generational. Both cases require strongly-ordered message delivery and \u003cem\u003eexactly-once-processing\u003c/em\u003e. The latter is pretty hard to achieve in a massively distributed environment, but rack-scale or even DC-scale (of reasonable size) is OK. Nevertheless PDS have scalability limitations that may limit even very small systems \u0026ndash; if they are fast enough for them to hit the limitations.\u003c/p\u003e\n\u003cp\u003eTo accelerate PDS we need acceleration for atomic counters and, probably, for some other concurrency primitives. We need to make sure that communication fabric supports robust exactly-once-delivery. The latter will require some limited form of idempotent counters, that is reducible to keeping update history for a counter for a limited amount of time.\u003c/p\u003e\n\u003cp\u003ePersistent and functional data structures are \u003cem\u003eslower\u003c/em\u003e than their ephemeral counterparts on a single-threaded sequential access: O(1) access time becomes O(log N) because of trees. Functional programming languages may amortize this overhead in certain cases. But it should be taken into account that benefits of using them may start manifesting only for massively-parallel applications (10+ cores).\u003c/p\u003e\n\u003ch2 id="high-level-architecture"\u003eHigh-level Architecture\u003c/h2\u003e\n\u003cp\u003eComputational architecture in Memoria is inherently heterogeneous and explicitly supports three domains:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGeneric mixed \u003cstrong\u003eDataFlow\u003c/strong\u003e and \u003cstrong\u003eControlFlow\u003c/strong\u003e computations. A lot of \u003cem\u003epractical\u003c/em\u003e compute- and IO-intensive applications, that may be run either on CPUs or on specialized hardware, fall into this category.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegrated Circuits\u003c/strong\u003e for fixed (ASIC) and reconfigurable logic (FPGA, Structured ASIC). May be used for high performance \u003cem\u003eand\u003c/em\u003e low power stream/mixed signal processing part of application, giving ability to handle events with a nanosecond-scale resolution.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRule/search-based\u003c/strong\u003e computations in a \u003cem\u003eforward chaining\u003c/em\u003e (Complex Event Processing, Streaming) and \u003cem\u003ebackward chaining\u003c/em\u003e (SQL/Datalog databases) forms.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg src="tri-arch.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eDomains are connected with a unified hardware-accelerated RPC+streaming communication protocol, \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e, allowing intra- and cross-domain seamless communication. HRPC is very much like gRPC (that is used to communicate with services in distributed computing tasks), but optimized for direct hardware implementation.\u003c/p\u003e\n\u003cp\u003eHRPC, if it\u0026rsquo;s implemented in hardware, eliminates the need for a fully-featured OS Kernel, reducing it to a nano-kernel, that can be as small, as the amount of HRPC functionality, implemented in the software. Memoria \u003cem\u003ecomputational kernel\u003c/em\u003e, a program module, running on a CPU core inside an \u003cem\u003eaccelerator\u003c/em\u003e, can listen to a stream, generated by reconfigurable logic, running in an FPGA (and vice versa). Or, the same kernel may call storage functions running in the smart-storage device, or in a \u0026ldquo;far\u0026rdquo; memory (near-memory compute in a CXL device):\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="comp-arch.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn this type of architecture, OS\' kernel functionality is split into a services running on different computable devices. Storage functions, which are usually the largest OS-provided piece of functionality, are managed directly by \u003ca href="/docs/applications/storage"\u003e\u0026lsquo;smart storage\u0026rsquo; devices\u003c/a\u003e, capable of running complex DB-like queries in streaming and batching modes.\u003c/p\u003e\n\u003cp\u003eThis architecture design should not be considered as a hardware-assisted micro- or nano-kernel, but rather a \u003cstrong\u003edistributed system scaled down to a single machine\u003c/strong\u003e. Large multicore MMU-enabled CPU is no longer a \u003cem\u003ecentral\u003c/em\u003e PU in this architecture, but rather a PU for running legacy code and code benefited from using a MMU.\u003c/p\u003e\n\u003cp\u003eNotable feature of this architecture is that memory is no longer a single shared address space, but rather a set of buffer with different affinity with compute functions. Programming it directly will be a challenge, but it\u0026rsquo;s already a mundane 9-to-5 job in the area of distributed programming.\u003c/p\u003e\n\u003cp\u003eNote that in Memoria architecture, cross-environment code portability is \u003cem\u003enot ensured\u003c/em\u003e. Different accelerators may provide different default runtime environments, memory and CPU cluster topologies. It\u0026rsquo;s OK if some Memoria code needs a substantial rewrite to be run in a different acceleration environment. But the framework will be trying to reduce portability costs and cross-environment code duplication by using metaprogramming and other types of development automation.\u003c/p\u003e\n\u003ch2 id="processing-element"\u003eProcessing Element\u003c/h2\u003e\n\u003cp\u003eReconfigurable extensible processing unit (xPU) is the main structural element of MAA. The main point of this design is that hardware-accelerated HRPC protocol is used for all communication between core and outer environment. From the outside, a core is represented as a set of HRPC endpoints described with using generic HRPC tools (IDL, schema, etc\u0026hellip;).  This includes:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAll external (to the core) memory traffic, including cache transfers and DMA;\u003c/li\u003e\n\u003cli\u003eAll Debug and Observability traffic;\u003c/li\u003e\n\u003cli\u003eRuntime exceptions signalling;\u003c/li\u003e\n\u003cli\u003eApplication-level HRPC communication.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSuch unification allows placing xPU at any place where HRPC network is available:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIn an accelerator\u0026rsquo;s cluster;\u003c/li\u003e\n\u003cli\u003eInside DDR memory controller;\u003c/li\u003e\n\u003cli\u003eOn a DRAM memory module (\u003cem\u003enear\u003c/em\u003e memory chips, CXL-mem, PNM-DRAM);\u003c/li\u003e\n\u003cli\u003eInside a DRAM chip on a separate stacked die (in-package PIM-DRAM);\u003c/li\u003e\n\u003cli\u003eOn a DRAM memory \u003cem\u003edie\u003c/em\u003e (PIM-DRAM);\u003c/li\u003e\n\u003cli\u003eInside a network router;\u003c/li\u003e\n\u003cli\u003e\u0026lt; \u0026hellip;your idea here\u0026hellip; \u0026gt;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn all cases kernel\u0026rsquo;s code running in such xPUs will be able to communicate bidirectionally with the rest of environment.\u003c/p\u003e\n\u003cp\u003eBoth HRPC (low-level) and system-level endpoints specification is meant to be an open protocol, so independent manufacturers may contribute both \u003cem\u003especialized cores\u003c/em\u003e and \u003cem\u003emiddleware\u003c/em\u003e into the Framework-supported ecosystem. Software tools will be able to adapt to a new hardware either automatically or with minimal manual efforts.\u003c/p\u003e\n\u003cp\u003eMemoria code may be pretty large and have deep function call chains, so instruction cache is essential in this design (with, unfortunately, unpredictable instruction execution latencies caused by that). Another essential part of the core is \u0026lsquo;stack cache\u0026rsquo; \u0026ndash; dedicated data cache for handling thread stacks. It\u0026rsquo;s necessary when internal data memory is used as a scratchpad, not a D$:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="xpu.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat this architecture is not going to have, is \u003cem\u003ecache coherency\u003c/em\u003e support (unless it\u0026rsquo;s really necessary in some narrow cases). MAA relies on PDS where mutable data is private to a writer, and readers may see only immutable data. If there is some shared structured mutable data access, like atomic reference counting, it can be done explicitly via RPC-style messaging (HRPC) and hardware-accelerated services.\u003c/p\u003e\n\u003ch2 id="accelerator-module"\u003eAccelerator Module\u003c/h2\u003e\n\u003cp\u003eThe whole point of MAA is to maximize utilization of available \u003cem\u003ememory parallelism\u003c/em\u003e by moving processing closer to the data, primarily for \u003cem\u003elowering access latency\u003c/em\u003e, but also for \u003cem\u003eincreasing throughput\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIdeally, every \u003cem\u003ememory bank\u003c/em\u003e in the system should have either xPU or a fixed functions associated with it. Embedding logic into a DRAM die is technically challenging, although some solutions are already \u003ca href="https://arxiv.org/pdf/2105.03814"\u003ecommercially available\u003c/a\u003e. Stacking DRAM and processing dies is more expensive, but may be better fit into existing processes.\u003c/p\u003e\n\u003cp\u003eThe simplest way is to put xPU and fixed functions into a DRAM memory controller (MC), making it \u0026lsquo;smart\u0026rsquo; this way. Here the logic is operating at the memory speed and has the shortest part to. No caches, switches and clock doming crossing. But processing throughput is limited comparing to what is possible with PIM mode.\u003c/p\u003e\n\u003cp\u003eSo, \u003cstrong\u003eoptimization for memory parallelism with PNM/PIM modes\u003c/strong\u003e and \u003cstrong\u003etargeting memory access latency\u003c/strong\u003e, not just throughput, is what makes some computational architecture good for Memoria applications.\u003c/p\u003e\n\u003cp\u003eOther than PNM/PIM and HRPC and the use of persistent data structures, Memoria Framework does not define any specific hardware architecture. Below there is an \u003cem\u003einstance\u003c/em\u003e of an accelerator for a \u003cem\u003edesign space\u003c/em\u003e the framework will be supporting in software and tooling.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="accelerator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere there are following essential components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eProcessing elements (\u003ca href="#processing-element"\u003exPU\u003c/a\u003e) \u0026ndash; RISC-V cores with hardware support for HRPC and essential Memoria algorithms and data structures.\u003c/li\u003e\n\u003cli\u003eNetwork-on-Chip (NoC) in a form of either 2D array (simpler, best for matrix multiplication) or an N-dimensional hypercube (more complex, but best for latency in general case).\u003c/li\u003e\n\u003cli\u003eMain HRPC service gateway and many local HRPC routers.\u003c/li\u003e\n\u003cli\u003eService endpoints for hardware-implemented functions like atomic reference counting (ARC) and other shared concurrency primitives.\u003c/li\u003e\n\u003cli\u003eShared on-die SRAM, accessible by all cores. It can be distributed and has many functions \u0026ndash; scratchpad memory, caching, ring buffers and other \u003cem\u003ehardware assisted\u003c/em\u003e data structures, etc. May be physically split into many segments and distributed on the die.\u003c/li\u003e\n\u003cli\u003eSmart DRAM controller with embedded PNM xPUs and/or hardwired \u003ca href="#memoria-containers"\u003eMemoria functions\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eExternal connectivity modules (Transceivers, PCIe, etc).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe main feature of this architecture in the context of Memoria is that it\u0026rsquo;s \u003cem\u003escalable\u003c/em\u003e. There are no inherent system-wide scalability bottlenecks like whole-chip cache coherence. Of course, synchronization primitives like ARC and mutexes \u003cem\u003etheoretically\u003c/em\u003e aren\u0026rsquo;t scalable, but they can be \u003cem\u003epractically\u003c/em\u003e made efficient enough if implemented in the hardware directly, instead of doing it in the software over a cache-coherency protocol (that we can\u0026rsquo;t even control).\u003c/p\u003e\n\u003cp\u003eOther properties of this design:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt can be \u003cem\u003escaled down\u003c/em\u003e to the size and power budget of an MCU and \u003cem\u003escaled up\u003c/em\u003e to the size of an entire wafer (and beyond).\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003ecomposable\u003c/em\u003e. Memoria applications do not rely on a shared array-structured memory. They may use fast structured \u003ca href="/docs/overview/storage"\u003etransactional\u003c/a\u003e memory for that. At the hardware level it\u0026rsquo;s just bunch of chips talking to each other via an open messaging protocol.\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s extensible. Extra functionality can be added into xPUs (regular and custom RISC-V instruction set extensions), hardened shared functions, HRPC middleware and others. The only requirement is the use of HRPC protocol for communication via published HW/SW interfaces.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="cpu-mode"\u003eCPU mode\u003c/h2\u003e\n\u003cp\u003eAs it has been noted above, multicore MMU-enabled CPUs aren\u0026rsquo;t the best runtime environment for running Memoria applications because of the overhead caused by MMU, memory hierarchy and the \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003eOS\u003c/a\u003e. Nevertheless, it\u0026rsquo;s a pretty large deployment base that will only be increasing in size in the foreseeable future. And it\u0026rsquo;s currently the only way to run Memoria applications.\u003c/p\u003e\n\u003cp\u003eSo, Memoria Framework is going to support it as a first-class member in its hardware platform family. Once specialized hardware becomes available, it can be incrementally included into the ecosystem.\u003c/p\u003e\n\u003ch2 id="matrices-tensors-and-operations-on-them"\u003eMatrices, Tensors and Operations on Them\u003c/h2\u003e\n\u003cp\u003eMany data structures can be represented as arrays. Ordinary graphs can be represented as a square matrix and, if graph is \u003cem\u003edense\u003c/em\u003e, such representation will be optimal both in terms of memory size and in terms of memory access patterns. Many algorithms on graphs can be efficiently reduced to operations on matrices. And if we operate on matrices, we can enjoy \u003cem\u003estatic\u003c/em\u003e scheduling of data/control flow and, in case of matrix multiplication for instance, short data trips of systolic processing. Matrices are rarely dense, but if it\u0026rsquo;s the case, benefits may be huge.\u003c/p\u003e\n\u003cp\u003eMAA does need support for efficient matrix operations, but this entire area is currently being passionately explored in companies creating accelerators for neural networks. Solutions are very complex and highly optimized (including software side \u0026ndash; \u003cem\u003ecompilers\u003c/em\u003e), and it seems they are pretty close to the ideal.\u003c/p\u003e\n\u003cp\u003eMemoria is focusing primarily on sparse data structures processing by relying on in/near-memory computing (PIM/PNM), that is expected to reduce data access latency. There are three possible strategies how to fuse these two different types of processing in one architecture:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAdd systolic processors/CGRAs to xPUs. They can be implemented as HRPC accessible devices to coexist peacefully with threads. But area will be wasted if this functionality isn\u0026rsquo;t used.\u003c/li\u003e\n\u003cli\u003eDesign a separate GEMM-optimized architecture in the context of MAA either in a form of specialized xPU (preferably) or even a separate accelerator module.\u003c/li\u003e\n\u003cli\u003eOutsource this functionality to external projects. There is substantial interest to GEMM in both open source and proprietary domains.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn all three cases hardware implementation of HRPC endpoints and middleware becomes foundational for both internal communication within MAA and with external systems.\u003c/p\u003e\n\u003cp\u003eThe whole idea of hardware HRPC is to \u003cem\u003egenerate\u003c/em\u003e corresponding IP from semantically enriched IDL, much like we are doing it currently for service-oriented architectures in the context of distributed computing. Specifying interfaces in a separate place and auto-generating software (and hardware) artifacts supporting them is an efficient way to reduce solution complexity.\u003c/p\u003e\n\u003cp\u003eHardware implementation of HRPC (including related tools) will be a high priority for the project.\u003c/p\u003e\n\u003ch2 id="implementation-strategy"\u003eImplementation Strategy\u003c/h2\u003e\n\u003cp\u003eImplementing MAA is a major technical and organizational challenge for the Memoria project. In order to get the whole idea faster into the prototyping stage, a configurable RV ISA-level (with Memoria-specific ISA extensions, memory- and HRPC-related machinery) software emulator of MAA is planned at first.\u003c/p\u003e\n\u003cp\u003eThe emulator needs a C/C++ compiler with MAA-specific RISC-V extensions, that is also planned based on \u003ca href="https://github.com/victor-smirnov/jenny"\u003eJenny\u003c/a\u003e (Memoria-specific Clang\u0026rsquo;s fork).\u003c/p\u003e\n\u003cp\u003eThe main purpose of emulator is to start experimenting early with porting Memoria\u0026rsquo;s core algorithms and data structures to MAA. Later, this emulator may be used for software development when actual hardware is not available.\u003c/p\u003e\n\u003cp\u003eThe second phase is developing reference implementations of HDL IP and other related artifacts for FPGA and ASIC:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHermes core algorithms and data structures as RV ISA extension instructions.\u003c/li\u003e\n\u003cli\u003eHRPC core protocol, transport and routing circuitry.\u003c/li\u003e\n\u003cli\u003eConfigurable RISC-V xPU in some pre-existing HDL.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe goal is to create a minimal, not necessary fully optimized, but functional MAA \u003cem\u003eenvoronment\u003c/em\u003e for hardware developers to experiment with the framework.\u003c/p\u003e\n\u003cp\u003eThere is a hardware for that:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="U50.jpg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe third phase is to integrate \u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e and \u003ca href="/docs/overview/mbt"\u003eMBT\u003c/a\u003e, the emulator into an automated development platform.\u003c/p\u003e\n'}).add({id:9,href:"/docs/overview/mbt/",title:"Memoria Build Tool",description:"",content:"\u003cp\u003eMemoria Build Tool (MBT) is a helper program with the scope of Memoria project (and out of it, if it\u0026rsquo;s fount to be helpful). It basically reads C++ sources (via Clang libs) and can:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGenerate boilerplate code like bindings,\u003c/li\u003e\n\u003cli\u003eInvoke compiler to do type inference,\u003c/li\u003e\n\u003cli\u003e\u0026hellip;\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMBT is integrated with CMake and has a special mode to produce a list of artifacts that will be included into the project\u0026rsquo;s build process.\u003c/p\u003e\n\u003cp\u003eCurrently MBT is used to instantiate containers in libraries, but later this scope will be extended.\u003c/p\u003e\n\u003cp\u003eTBC \u0026hellip;\u003c/p\u003e\n"}).add({id:10,href:"/docs/overview/qt_creator_instructions/",title:"Qt Creator Instructions",description:"",content:'\u003ch2 id="dependencies"\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eMemoria relies on third-party libraries that either may not be available on supported developenment platfroms or have outdated versions there. Vcpkg package manager is currently being used for dependency management. Memoria itself is avaialble via \u003ca href="https://github.com/victor-smirnov/memoria-vcpkg-registry"\u003ecustom Vcpkg registry\u003c/a\u003e. Conan recipies and source packages for Linux distributions (via CPack) may be provided in the future.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/docker/Dockerfile"\u003eDockerfile\u003c/a\u003e on how to configure development environment on Ubuntu 22.04. Standard development environment will be the latest Ubuntu LTS.\u003c/p\u003e\n\u003ch2 id="install-vcpkg-for-memoria"\u003eInstall VCPkg for Memoria\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd /path/to/checout/vcpkg/into\n$ git clone https://github.com/microsoft/vcpkg.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor now, supporting compiler is Clang. Gcc 10/11/12 are crashing on Memoria. Gcc 13.1 is known to work.\u003c/p\u003e\n\u003ch2 id="configuring-vcpkgs-provided-cmake-tool"\u003eConfiguring VCPkg\u0026rsquo;s provided cmake tool\u003c/h2\u003e\n\u003cp\u003eIn Options/Kits/Cmake tab add another cmake configuration by specifying full path VCPkg\u0026rsquo;s own cmake distribution.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-required-clang-compiler"\u003eConfigure Required clang compiler\u003c/h2\u003e\n\u003cp\u003eMemoria currently is built with clang compiler version 6.0 or newer. If you system already provides it, like most Linux distributions do, then this step is unnecessary. Otherwise, build clang yourself and configure it on the Options/Kits/Compiler tab:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-clang.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="add-new-kit-for-clang"\u003eAdd new Kit for Clang\u003c/h2\u003e\n\u003cp\u003eAdding new Kit is necessary if QtCreator did not recognize clang compiler automatically. Just create new kit by cloning and existing one and specify clang 17 as C and C++ compilers:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-newkit.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="vcpkgs-cmake-selection"\u003eVCPkg\u0026rsquo;s Cmake Selection\u003c/h2\u003e\n\u003cp\u003eNow specify that VCPkg\u0026rsquo;s provided cmake tool will be used for new Kit, and specify the path to VCPkg\u0026rsquo;s libraries definitions:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-kit-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eProvide your full path to vcpkg.cmake:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-vcpkg-toolchain.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-memorias-build-parameters"\u003eConfigure Memoria\u0026rsquo;s build parameters\u003c/h2\u003e\n\u003cp\u003eToggle BUILD_* options as specified on the screenshot. This will build Tests, as well as threads- and fibers-based Memoria allocators, with libbacktrace support in case of exceptions.\u003c/p\u003e\n\u003cp\u003eMore details on build options can be found in top-level CMakeLists.txt\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-project-cfg.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThat\u0026rsquo;s it!\u003c/p\u003e\n\u003cp\u003ePress Ctrl+B to start build process.\u003c/p\u003e\n'}).add({id:11,href:"/docs/overview/roadmap/",title:"Project Roadmap",description:"",content:'\u003ch2 id="status"\u003eStatus\u003c/h2\u003e\n\u003cp\u003eAfter many years, Memoria is still in a permanent development.\u003c/p\u003e\n\u003cp\u003eThere are five main scopes:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCore: \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e, \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e, \u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e core, related tools (\u003ca href="/docs/overview/mbt"\u003eMBT\u003c/a\u003e), runtime-agnostic.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e and \u003ca href="/docs/overview/runtime"\u003eRuntime Environments\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/containers"\u003eContainers\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/storage"\u003eStorage Engines\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/accel"\u003eMAA\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCore is being designed to be separable and runtime-agnostic (configurable). Does not need MBT. May be distributed and used independently from the rest of the framework. May be needed when external applications want to communicate with Memoria apps, producing and consuming data in Memoria-native formats.\u003c/p\u003e\n\u003cp\u003eHermes is \u003cem\u003ebasically\u003c/em\u003e ready: API, memory layout, serialization/stringification etc. But formats haven\u0026rsquo;t been finalized yet.\u003c/p\u003e\n\u003cp\u003eHRPC is currently working over TPC/IP only that is sufficient for basic practical needs, DSLEngine is TODO and WIP.\u003c/p\u003e\n\u003cp\u003eDSLEngine is the main execution layer of Memoria, executing regular control-flow programs (\u0026ldquo;bytecode\u0026rdquo;), Datalog programs (that is also including SQL) and forward-chaining rule system based on variants of RETE algorithm (\u0026ldquo;streaming\u0026rdquo; and complex event processing (CEP)).\u003c/p\u003e\n\u003cp\u003eThere are four Runtime Environments supported in Memoria:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThreads-based, that is mostly for running parts of Memoria alongside a legacy or external code.\u003c/li\u003e\n\u003cli\u003eBoost ASIO + Boost Fibers \u0026ndash; slower but potentially cross-platform and compatible with Boost libraries.\u003c/li\u003e\n\u003cli\u003eMemoria\u0026rsquo;s own Reactor engine. Single thread per core with messaging. Uses Boost Fibers for concurrency but it\u0026rsquo;s own IO stack.\u003c/li\u003e\n\u003cli\u003eSeastar. Production ready but Linux only. Best fibers and fastest IO, but may not be the best fit for Memoria\u0026rsquo;s use cases.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIt\u0026rsquo;s currently unknown which IO engine will be used in the future. Most likely, Memoria will be designed in an IO-agnostic way. But it will be tricky, given current struggles around lightweight threads in C++.\u003c/p\u003e\n\u003cp\u003eContainers are in a good shape but need much more work at the API level and integration with Hermes and DSLEngine.\u003c/p\u003e\n\u003cp\u003eStorage engines. There is currently only one implementation of SWMRStore \u0026ndash; on top of memory mapping. OLTPStore is WIP and will be using it\u0026rsquo;s own caching layer. Computational storage is TODO and slightly WIP (in the context of other subsystems).\u003c/p\u003e\n\u003cp\u003eMAA is currently at a purely conceptual stage. TODO and WIP.\u003c/p\u003e\n\u003ch2 id="roadmap"\u003eRoadmap\u003c/h2\u003e\n\u003cp\u003eHermes, HRPC and DSLEngine are the highest priority now.\u003c/p\u003e\n\u003cp\u003eNext, deep integration of Hermes with Containers. Hermes is meant to be the main freely-structured \u003cem\u003estatic\u003c/em\u003e data representation. Containers are for large and highly-structured \u003cem\u003edynamic\u003c/em\u003e and \u003cem\u003eversioned\u003c/em\u003e datasets, including \u003cem\u003edecentralization\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eMAA is a major challenge so the plans aren\u0026rsquo;t specific right now (2024). RISC-V ISA-level simulator of MAA is TODO but not yet WIP.\u003c/p\u003e\n\u003cp\u003eHDL for Hermes and HRPC is TODO but not yet WIP. Priority is medium.\u003c/p\u003e\n\u003cp\u003eTBC\u0026hellip;\u003c/p\u003e\n'}).add({id:12,href:"/docs/overview/story_ru/",title:"Memoria Story (RU)",description:"",content:'\u003ch2 id="о-чем-проект"\u003eО чем проект\u003c/h2\u003e\n\u003cp\u003eМемория (можно так называть проект, без добавления Framework, которое лишь указывает на текущий характер проекта) создавалась долго, уже более 16 лет и на каждом этапе я пытался вписать её в текущий технологический тренд (файловые системы, NoSQL, дата-платформы, аналитика, теперь — AI). Они быстро менялись, переставая быть актуальными, но попытки вписаться так или иначе оставили след и на коде, и на документации к ней. Вписываться в тренды нужно, так как интерес сообщества во многом спекулятивен — компании смотрят на opensource как на возможность снизить стоимость разработки через win-win социализацию. в любом проекте значительная часть контрибуторов — это коммерческие компании.\u003c/p\u003e\n\u003cp\u003eС другой стороны, вписываясь в тренд, надо попасть в еще пока не населенную нишу, чтобы не создавать себе конкуренции за внимание пользователей. Например, со временем, появилось много аналитических дата-платформ, вполне годных для своих задач, пусть и неидеальных. Нет никакого смысла в конкуренции с ними. То же самое сейчас с базами данных, как транзакционными, так и аналитическими. Они все вместе вполне нормально удовлетворяют потребности пользователей в соответствующем функционале. Тем более, что Мемория как изначально создавалась для, так и уже может значительно больше того, что нужно пользователям обычных баз данных. Т.е. она бы была в этой нише существенно избыточна. А её особый функционал — как чемодан без ручки.\u003c/p\u003e\n\u003cp\u003eМемория изначально создавалась для задач ИИ, \u003ca href="https://memoria-framework.dev/docs/data-zoo/associative-memory-2"\u003eвот таких\u003c/a\u003e. Но, так получилось, что в 2010-х ИИ пошел по нейросетевому пути, а это технически — перемножение матриц, и в это время как раз был период экспоненциального роста вычислительной мощности соответствующих акселераторов. Сейчас технологии насытились, прогресс в железе и софте замедлился, а аппетиты — выросли. Появление LLM привело к парадоксальному взрыву интереса к \u0026ldquo;старым\u0026rdquo; методам из-за хороших возможностей \u003ca href="https://memoria-framework.dev/docs/applications/aiml"\u003eгибридизации\u003c/a\u003e, и это сразу потянуло за собой технологии \u0026ldquo;векторных\u0026rdquo; БД (для RAG). Появляются и компании, продвигающие \u0026ldquo;гибридный ИИ\u0026rdquo;. Технологии Мемории будут там востребованы в обозримой перспективе.\u003c/p\u003e\n\u003cp\u003eСейчас проект позиционируется как Hardware/Software \u003ca href="https://memoria-framework.dev/docs/applications/co-design"\u003eco-design\u003c/a\u003e framework, и это отражает две потребности.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eПервая\u003c/strong\u003e. Одного только софта для целей Мемории как фундаментального проекта недостаточно. CPU-центричные аппаратные архитектуры и программные архитектуры вокруг них устарели. В них данные находятся фундаментально далеко от вычислителей, а это создает задержки и тепло. Много тепла. И большие задержки. Это сильно препятствует масштабированию. И препятствует эффективной реализации алгоритмов, относящихся к классу чувствительных к задержкам (latency sensitive). А это — весь ИИ, который раньше называли \u0026ldquo;символьным\u0026rdquo;. Т.е. если кто-то захочет сделать гибридную систему, то эффективное железо есть только для её компоненты, которая не чувствительна к задержкам (latency insensitive) и не требует динамического параллелизма (заморочки GPU).\u003c/p\u003e\n\u003cp\u003eКороче, чтобы чтобы эффективно удовлетворить потребности индустрии в гибридном ИИ, нужен новый аппаратный стек. А, поскольку железо без софта — это просто дорогой кирпич, то нужен и софт, т.е. программная архитектура, которая это железо позволяет эффективно использовать.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eВторая\u003c/strong\u003e. Соответствующая рыночная ниша практически пуста. И ждет своего заполнения.\u003c/p\u003e\n\u003cp\u003eОдно из центральных value prop Мемории в том, что она не просто оборачивает специализированное железо в программный runtime, но и приносит много дополнительных решений в виде контейнеров, протоколов, хранилищ, систем исполнения кода и даже языков описания данных интегрированных с языками программирования. Подход Мемории к проблеме ко-дизайна — \u0026ldquo;сверху-вниз\u0026rdquo;, т.е. железо делается под потребности софта, а не наоборот (вот вам наш чудесный акселератор, а теперь придумайте, как его использовать). Т.е. разработка железа — это просто этап разработки алгоритмов и структур данных. Идеально, если в будущем он станет совершенно прозрачным через глубокую автоматизацию процессов. Сейчас есть реальные основания ожидать такого исхода.\u003c/p\u003e\n\u003cp\u003eВ остальном же, Меморию стоит воспринимать как фундаментальный проект, сфокусированный на долговременном использовании данных по всему стеку, от физического хранения до математической обработки. В контекст проекта входит даже такая экзотика, как возможность через, условно, миллиард лет прочитать данные, записанные сегодня. Это может звучать смешно, но есть объективная проблема data longevity. Это не только человеческая проблема. Разрушается всё. И гораздо быстрее, чем кажется.\u003c/p\u003e\n\u003ch2 id="разработчикам-железа"\u003eРазработчикам железа\u003c/h2\u003e\n\u003cp\u003eДля разработчиков железа value prop в том, что Мемория как co-design фреймворк определяет контракт между софтом и железом. Этот контракт прописывается через референсную мета-архитектуру \u003ca href="https://memoria-framework.dev/docs/overview/accel"\u003eMAA\u003c/a\u003e и её составные части: xPU и проблемно-специфические расширения команд RISC-V, архитектуру памяти, формат \u003ca href="https://memoria-framework.dev/docs/overview/hermes"\u003eHermes\u003c/a\u003e и протокол \u003ca href="https://memoria-framework.dev/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e. В идеале, если soft-core или chiplet удовлетворяют спецификациям, то фреймворк сможет их использовать с минимальными изменениями на своей стороне (100% прозрачность не является целью, так как трудно-достижима).\u003c/p\u003e\n\u003cp\u003eВ этом смысле не предполагается, что репозитарий проекта Мемория будет содержать HDL/HCL для соответствующих аппаратных модулей. Запланированы референсные реализации RISC-V ядер и элементов инфраструктуры HRPC, но их назначение больше в прототипировании, чем в реальном использовании. Последнее не запрещается, просто не является целью.\u003c/p\u003e\n\u003cp\u003eПредполагается, что аппаратные проекты, использующие Меморию как фреймворк, это — внешние проекты. Это же относится и к программным проектам, фреймворк будет стремиться к хорошей переиспользуемости своих компонентов. Например, Core, в который входят базовая библиотека, Hermes, некоторые элементы HRPC, DSLEngine и соответствующие инструменты (генераторы парсеров, генераторы для IDL и т.д), — может использоваться независимо от остальных компонент фреймворка и не имеет каких-то нетривиальных зависимостей.\u003c/p\u003e\n\u003cp\u003eДля упрощения сборки и управления зависимостями, имеется отдельный overlay для \u003ca href="https://github.com/victor-smirnov/memoria-vcpkg-registry"\u003eVcpkg\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eВ целом, улучшение модульности и переиспользуемости элементов фреймворка — один из высших приоритетов (но без фанатизма).\u003c/p\u003e\n\u003ch2 id="это-большой-проект"\u003eЭто БОЛЬШОЙ проект\u003c/h2\u003e\n\u003cp\u003eМемория получилась в итоге очень большим проектом. Гораздо бОльшим, чем может тянуть один человек. И уж, тем более, бОльшим, чем один человек способен оказывать коду проекта поддержку (даже с учетом вновь возникающих возможностей по автоматизации разработки программ). Даже на платной основе. Это не значит, что он не будет больше развиваться. Но это значит, что без помощи сообщества он будет развиваться медленно и, скорее всего, не будет успевать за трендами. Что и хорошо (нет ситуативного давления прагматизма), и плохо (отрыв от реальности).\u003c/p\u003e\n\u003ch2 id="приоритеты"\u003eПриоритеты\u003c/h2\u003e\n\u003ch3 id="n1"\u003eN1\u003c/h3\u003e\n\u003cp\u003eВ заключение этого введения я хочу определить текущие приоритеты самого проекта, исключая вопросы, связанные с построением сообщества, налаживания связи с бизнесом и развертывания необходимой инфраструктуры. Это всё будет происходить в фоне и с отдельной системой приоритетов.\u003c/p\u003e\n\u003cp\u003eКак я уже сказал выше, Мемория — это фундаментальный проект, поэтому особое внимание будет уделяться принятию решений \u0026ldquo;на далекое будущее\u0026rdquo;. Минимизация и проработка базовых алгоритмов, интерфейсов, структур и форматов данных, что, разумеется, может сдерживать практическое внедрение. Для смягчения этих эффектов будут proto/dev/stable ветки.\u003c/p\u003e\n\u003cp\u003eИз крупных подсистем, Core, включающий Hermes и связанные с ним инструменты, находится в наивысшей степени концептуальной стабильности. Т.е. готов к замораживанию и к production. Core не зависит от других подсистем, не содержит сложного IO и привязки к runtimes. Концептуально он очень похож на соответствующие подсистемы других близких проектов. И она (подсистема) не требует каких-то специфических знаний об остальной Мемории (продвинутых алгоритмов и структур данных). У этой подсистемы может быть отдельный независимый maintainer и отдельная команда.\u003c/p\u003e\n\u003cp\u003eСейчас основное внимание будет уделяться Core.\u003c/p\u003e\n\u003ch3 id="n2"\u003eN2\u003c/h3\u003e\n\u003cp\u003eСледующий приоритет — это интеграция Core, в частности, типов данных \u003ca href="https://memoria-framework.dev/docs/overview/hermes"\u003eHermes\u003c/a\u003e и \u003ca href="https://memoria-framework.dev/docs/overview/containers"\u003eContainers\u003c/a\u003e. Hermes — это формат, ориентированный на сообщения и документы, и его объекты by design не могут быть большими. Хотя нет физических ограничений, однако, там действует довольно простой копирующий сборщик мусора с линейной трудоемкостью как по времени, так и по памяти. Чем больше документы, тем дольше (линейно) будет идти сборка мусора.\u003c/p\u003e\n\u003cp\u003eContainers — для больших и сильно структурированных данных, а объекты Hermes могут выступать как элементы этих контейнеров (maps, vectors, tables, indexes, etc\u0026hellip;).\u003c/p\u003e\n\u003cp\u003eПараллельно контейнерам, приоритет — \u003ca href="https://memoria-framework.dev/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e и соответствующие инструменты (компиляторы, билд-системы и т.д.). Эта подсистема зависит от Core и Runtimes, но не зависит от контейнеров (в идеале). И может разрабатываться независимо от всего остального.\u003c/p\u003e\n\u003cp\u003eМемория тяготеет к базам данных, и программирование для Мемории — это больше похоже на in-database programming, чем на привычно-традиционное. Причем, как в силу используемых парадигм программирования, так и вследствие особенностей рантайма. Например, рантайм Мемории активно использует файберы (userland threads scheduling), и это мало с чем совместимо. Если код С++ еще кое-как работает, то совместить такой рантайм с Java и Python не получится.\u003c/p\u003e\n\u003cp\u003eИспользовать вместе с Меморией привычные нам высокоуровневые и продуктивные ЯП типа Java и Python для in-database programming не получится по причине разности в рантаймах. А там, где получится, у этих языков (и их рантаймов) слишком слабая поддержка продвинутых типов данных. Как в плане синтаксиса языка (код будет получаться громоздкий), так и в плане доступных оптимизаций из области возможных. Java/Python позволили бы быстро начать и быстрее выйти на практику, но в среднесроке начнутся проблемы. А форкать их под себя — вот больше делать нечего.\u003c/p\u003e\n\u003cp\u003eВ третей декаде 21-го века нам меньше всего нужен еще один язык программирования. Но DSLEngine — это не еще один язык программирования (IR+Runtime для него). \u0026ldquo;Еще один\u0026rdquo; — это Mojo, который стремится сделать связку Python + C++ более легкой в использовании, предоставив программистам на Python мощь оптимизирующего компилятора С++. А так, все остальное — то же самое. Те же базовые примитивы (control flow), те же базовые типы данных (массивы и списковые структуры), та же парадигма, только теперь всё сразу \u0026ldquo;из коробки\u0026rdquo; и от лидеров индустрии.\u003c/p\u003e\n\u003cp\u003eDSLEngine — это другая парадигма, включая отход (но, не отказ!) от привычного control-flow в сторону data-flow и complex event processing (RETE). Это первоклассная поддержка продвинутых структур данных на уровнях IR и Runtime. Это фокус на обработку данных вообще. И глубокий фокус на автоматизацию. Я думаю, что такая платформа имеет право на жизнь в 21 веке.\u003c/p\u003e\n\u003cp\u003eОсобый смысл в DSLEngine можно увидеть, если принять во внимание, что для Мемории нужна и будет разрабатываться отдельная вычислительная \u003ca href="https://memoria-framework.dev/docs/overview/accel"\u003eархитектура\u003c/a\u003e (MAA), в которой мы отходим как от привычной парадигмы единого когерентного адресного пространства, так и от алгоритмов и структур данных, возможных для неё. Вместо этого — messaging, \u003ca href="https://memoria-framework.dev/docs/overview/storage/#persistent-data-structures"\u003epersistent data structures\u003c/a\u003e, in-memory computing, соответствующая аппаратная инфраструктура и завязанные на всё это паттерны программирования.\u003c/p\u003e\n\u003cp\u003eПричем MAA может масштабироваться как \u0026ldquo;вверх\u0026rdquo;, до размеров больших кластеров (и даже до децентрализации), так и \u0026ldquo;вниз\u0026rdquo; — до уровня \u003ca href="https://memoria-framework.dev/docs/applications/eiot"\u003eвстраиваемых устройств\u003c/a\u003e.  DSLEngine обеспечит более-менее совместимый (100% совместимость не является целью, так как технически очень сложна) Runtime для кода на любом уровне масштаба.\u003c/p\u003e\n\u003ch3 id="n3"\u003eN3\u003c/h3\u003e\n\u003cp\u003eТретий приоритет — это \u003ca href="https://memoria-framework.dev/docs/overview/storage"\u003eStorage Engines\u003c/a\u003e и самая алгоритмически сложная часть фреймворка. Причем сложность тут не только алгоритмическая, но и системотехническая. Так как организовать надежный сброс состояния памяти на stable storage весьма проблематично. Соответствующие подсистемы баз данных всегда довольно сложны по части обработки различных неочевидных случаев.\u003c/p\u003e\n\u003cp\u003eОдин из важных и практически значимых подпроектов Мемории — это \u003ca href="https://memoria-framework.dev/docs/applications/storage/"\u003ecomputational storage\u003c/a\u003e (CompStor), суть которого состоит в том, что storage engine + DSLEngine реализуются прямо в пространстве контроллера накопителя, а двунаправленный доступ осуществляется через протокол HRPC. Тем самым, транзакционная семантика (и семантика устойчивости к отказам питания) может быть реализована с наилучшими гарантиями самим производителем устройства. И обеспечить наивысшую скорость, так как здесь не будет потерь эффективности на интерфейсах блочных устройств. Которые (для этого) приходится делать чрезмерно сложными, как на уровне накопителя (SSD), так и на уровне файловой системы или базы данных.\u003c/p\u003e\n\u003cp\u003eCompStor, особенно, в формате мутимодельности Мемории — это сейчас весьма и весьма дерзкий проект, так как он потянет за собой переделку практически всего. Больше не нужна будет OS, которая содержит файловые системы и базы данных. Не нужны многоядерные CPU со своими костылями и заморочками, которые этот код исполняют. Взял сетевой акселератор, акселератор вычислений и акселератор хранилища. И — всё. Готовая вычислительная система. И CPU там не надо. Разве что для исполнения унаследованного кода.\u003c/p\u003e\n\u003cp\u003eПри наличии CompStor база данных редуцируется просто до Query Federation Engine а-ля \u003ca href="https://prestodb.io/"\u003ePrestoDB\u003c/a\u003e. Последняя, кстати, — очень хорошо архитектурно проработанная система, я с ней (или с её форком, не помню уже) плотно работал во время работы с Currents (об этом — ниже) и её внутренний дизайн оказал большое влияние на то, каким образом потом были сделаны API контейнеров и Hermes.\u003c/p\u003e\n\u003cp\u003eПо линии CompStor, наверное, проще всего будет найти финансирование для проекта. И через гранты, и через взаимодействие с бизнесом. Но это же связано и с наибольшим количеством потенциальных рисков — патентных, хейтинг, да и просто противодействие на ровном месте с неожиданной стороны. CompStor вламывается с идеологией открытости в буквально насквозь проприетарную нишу. Кто-нибудь видел в природе открытые SSD?\u003c/p\u003e\n\u003cp\u003eВ проекте сейчас есть три основных (и в перспективе еще несколько вспомогательных) storage engines.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eMemStore — in-memory store c поддержкой веток и возможностью иметь нескольких параллельных писателей. Самый быстрый вариант хранилища. Идеален для тестирования и тогда, когда данные легко вмещаются в память (а память сейчас может быть ой, какая большая).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSWMRStore — single-writer-multiple-readers — работающий через mmap (пока что, для простоты реализации). Поддерживает ветки, историю коммитов, но не поддерживает параллельных писателей. Зато писатели не мешают читателям. Использует счетчики ссылок для управления памятью, то есть относительно медленный на запись (каждое обновление дергает множество счетчиков). Идеален для аналитики.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOLTPStore (еще не завершен), использует механизм управления памятью от LMDB/MDBX, но со своими структурами (специализированными контейнерами Мемории) в качестве FreeList. Не использует счетчики, поэтому — теоретически может быть очень быстрым на запись. Делается сразу для IO через io_uring, а не mmap, т.е. будет иметь свой кэш. Как и LMDB/MDBX позволяется делать мгновенное восстановление после сбоев, а так же чувствительна к времени жизни читателей, так как читатели в этой схеме блокирую высвобождение всей памяти, выделенной после них. По этой причине не годится для аналитики, так как там характерны долго выполняющиеся запросы. Однако, идеально для транзакционных конвергентных (супер-мультимодельных) БД, предоставляющих высокопроизводительные strongly-serialized транзакции. Может работать, например, как очень высокопроизволительная (в варианте CompStor) персистентная очередь/лог и быть полезной для проектов типа Kafka/RedPanda.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eВ направлении Storage Engines есть два приоритета:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eБазовый функционал\u003c/strong\u003e. Довести до стабильного и высокопроизводительного состояния тот базовый функционал, который есть сейчас (а его там уже достаточно для многих типов применений), включая реализацию для реактора (\u003ca href="https://seastar.io/"\u003eSeastar\u003c/a\u003e, асинхронный IO). Цель — прохождение расширенной системы рандомизированных функциональных тестов. Архитектура должна быть адаптирована к CompStor.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eРасширенный функционал\u003c/strong\u003e. Например, репликация, как логическая, так и физическая — через патчи, подобно тому, как мы это делаем в Git. Или распараллеливание писателя для SWMR. Писатель может быть один, но потоков может быть много. Просто все они должны фиксироваться одновременно, как единая операция. Это ускорит некоторые тяжелые O(N) операции типа конвертации таблиц, ETL и т.п.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="опыт-разработки-в-компаниях-и-выводы"\u003eОпыт разработки в компаниях и выводы\u003c/h2\u003e\n\u003cp\u003eПопробовав немного разработку Мемории в коммерческих компаниях в качестве наемного работника под их нужды, я пришел к двум выводам.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eПервый вывод\u003c/strong\u003e — я так больше не хочу. Я получаю (без преуменьшения) бесценный опыт внедрения и \u003cem\u003eпродуктизации\u003c/em\u003e проекта. Однако, сам проект начинает очень серьезно \u003cem\u003eдеформироваться\u003c/em\u003e под нужды и видение компании, даже если компания никак не давит в этом направлении.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eВторой вывод\u003c/strong\u003e — это то, что распределенное хранилище — довольно сложное, так как требует распределенного, отказоустойчивого и очень высокопроизводительного сборщика мусора для управления памятью — удаления блоков, которые больше не видны после удаления коммитов.\u003c/p\u003e\n\u003cp\u003eЭто всё возможно сделать, но работа трудоемкая, и пусть этим занимаются коммерческие облачные компании \u0026ldquo;для себя\u0026rdquo;, если вдруг оно им надо. Contributions are Welcome, как всегда! Но в проекте этим, заманчивым во всех остальных смыслах, направлением \u0026ldquo;за свои\u0026rdquo; заниматься не особо интересно.\u003c/p\u003e\n\u003cp\u003eВместо этого я выбрал вариант децентрализованных хранилищ и эмулирования распределенности через децентрализованность.\u003c/p\u003e\n\u003cp\u003eДецентрализованная модель — в Git. Любое хранилище содержит все или только фрагмент данных, и данные мигрируют между хранилищами явно — через передачу патчей (физическая репликация). Управление памятью тут чисто локальное, так как всё происходит в рамках одной машины и нет никакой распределенной сборки мусора.\u003c/p\u003e\n\u003cp\u003eЭто немного медленнее и сложнее в реализации на уровне приложений, так как приложениям теперь нужно управлять перемещениями данных между узлами распределенной системы явно, через прямой запрос тех данных, которые собираешься обрабатывать. Кроме того, датасеты должны влезать в размер внешней памяти одной машины. Что на практике не проблематично, но всё равно — ограничение, так как это место нужно теперь планировать.\u003c/p\u003e\n\u003cp\u003eИзначально распределенная схема свободна от этих ограничений, и кому она всё же нужна и он может, то пусть разрабатывает для себя распределенный сборщик мусора.\u003c/p\u003e\n\u003cp\u003eТут важно отметить, что сложности есть только с универсальными, полностью персистентными хранилищами,  которые используют счетчики ссылок для отслеживания достижимости блоков. Счетчики ссылок, кстати, не нужны, если используется трассирующий, а не детерминированный сборщик мусора (он будет счетчики создавать в процессе). С ним запись будет существенно быстрее, но сборка мусора — медленнее, скорее всего. То есть, что выйдет по итогу в среднем — еще вопрос.\u003c/p\u003e\n\u003cp\u003eOLTPStore, предназначенное для выполнения транзакций, не использует ни счетчики ссылок на блоки, ни сборщик мусора, хотя блоки, подлежащие высвобождению, и накапливаются во внутренних структурах для отложенного выполнения операции (а-ля GC). Остальные типы хранилищ — это для аналитики, а там как раз данные нужно иметь локально и чтение сильно превалирует над записью. В этом была основная логика принятия решения о фокусе на децентрализацию. Децентрализованная схема — более народная, хорошо вписывающаяся в привычный уже Git Flow. Она так же легка в локальном развертывании и переносима в облако 1-в-1, без модификации приложений. Она будет достаточно хороша (пусть и не идеальна) для data science, аналитики, AI — областей использования, на которые нацелена Мемория.\u003c/p\u003e\n\u003cp\u003eКороче говоря, выбор системы хранилищ, а будет делаться именно система, совместно покрывающая основные применения (OLTP, аналитика, статические датасеты, встроенные в процессы и т.д.), продиктован большим количеством факторов. Среди которых как технические, такие как сложность реализации и использования, минимизация возможности потери данных, так и экономические. Такие как желание избегать конкуренции и не попасть под патенты. Последнее — вообще минное поле, особенно в области хранения и обработки данных.\u003c/p\u003e\n\u003ch2 id="ai-and-philosophy"\u003eAI and Philosophy\u003c/h2\u003e\n\u003cp\u003eЯ темой занимался еще со своего института, у меня на BS диплом и MS диссертацию было распознавание речи (конец 1990-х). Я помню, тогда еще, уже на Линуксе, написал на Qt программный сонограф для того, чтобы видеть частотную структуру речевого сигнала. И провел за ним, наверное, сотни часов, изучая многообразие спектров речевых сигналов. Программы я написал, дипломы-диссертации защитил. Но понял одну вещь: \u0026ldquo;проблема рспознавания речи сводится к проблеме понимания речи, а последняя не решается цифровой обработкой сигналов\u0026rdquo;. Пришла пора разбираться с тем, как у человека происходит понимание речи.\u003c/p\u003e\n\u003cp\u003eПервым делом — \u0026ldquo;разобраться, как работает мозг\u0026rdquo;. Это был тот еще квест на начало 2000-х, когда у меня дома в моем Смоленске даже телефона не было (а с ним и интернета по диалапу). В общем, я изучал тему медицинским сайтам. И, к чести российской медицины, они тогда были безусловным лидером в информатизации. Много популярного материала публиковалось в Сети. В общем, я как-то подразобрался. Даже узнал про Маункасла и колоночную организацию коры, о которой в популярной форме заговорили только спустя 10 лет (к моему удивлению).\u003c/p\u003e\n\u003cp\u003eРазобравшись с мозгом, я понял только, что ничего не понял. И не только я, а и вообще никто. Иначе бы об этом написали бы. И я попробовал пойти с другой стороны, с \u0026ldquo;психологической\u0026rdquo;. Этот опыт оказался на много продуктивнее, хотя и совсем не такой прямолинейный, какой мы ожидаем от чтения туториалов по программированию. Пришлось совершить кучу ошибок и выполнить работу над ними. Но, по итогам я сам для себя изобрел начала \u003ca href="http://www.scholarpedia.org/article/Algorithmic_information_theory"\u003eAlgorithmic Information Theory\u003c/a\u003e (AIT), которая в Европе и России больше известна как Теория колмогоровской сложности. Я называю её AIT, потому, что эта аббревиатура сама по себе гораздо более известная. Начал гуглить, и нагуглил, что всё уже придумано до нас. И даже придуманы \u003ca href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html"\u003eметематические теории\u003c/a\u003e для некоторых фундаментальных высших психических процессов. После чего Шмидхубер стал моим кумиром. Не как человек, я его лично даже не знаю. А просто он сделал \u0026ldquo;это\u0026rdquo; значительно раньше меня. И я, хочу я того или нет, продолжаю и расширяю его работу.\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;Найти Шмидхубера\u0026rdquo; было для меня \u0026ldquo;как гора с плеч\u0026rdquo; в том смысле, что я мог прекратить свое исследование и сосредоточиться на практической работе, в которой на то время (около 2010-го) в ИИ наблюдался острый дефицит. Особенно, в методах, производных от AIT (здесь и с тех пор мало что изменилось, будем исправлять ситуацию). К слову, потом я понял, что зря я свою исследовательскую программу остановил и немного возобновил её. Но об этом — позже.\u003c/p\u003e\n\u003cp\u003eТак появились идеи практических действий, которые потом, постепенно, привели к написанию Мемории.\u003c/p\u003e\n\u003cp\u003eACC Шмидхубера просто гениальна, хотят тут читателю придется поверить мне на слово. Кроме гениальности, это — лучшая частная теория эмоций на данный момент. Как и почти всё в AIT, она на столько же практически бесполезна в своем прямом виде, на сколько и математически универсальна. А она — именно что предельно универсальна. Т.е. некий \u0026ldquo;универсальный ИИ\u0026rdquo; неизбежно работал бы по этому принципу.\u003c/p\u003e\n\u003cp\u003eМетоды AIT теоретически хорошо поддаются аппроксимации (оценка колмогоровской сложности объекта через его сжатие), в том числе и ACC, и чтобы она стала применима в психологии, нужно такую аппроксимацию провести. Мозг — ни чуточки не \u0026ldquo;универсальный ИИ\u0026rdquo;. Он может выполнять какое-то сжатие, но и только. Причем тут еще наслаиваются различные эволюционные механизмы. И получится, что тот же интерес может проявляться и действовать как ансамбль или микстура частных и очень разных механизмов. Последнее создает основной пакет сложностей в психологии, так как высшие психические функции являются сложным ансамблем низкоуровневых, которые относительно просто регистрировать и изучать. Но вот структура этого ансамбля — это вовсе не обязательно взвешенная сумма. Это может быть и некий произвольный алгоритм (*). И это так же носит название \u0026ldquo;психофизической проблемы\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003e(*) Можно сказать, что \u0026ldquo;любой произвольный алгоритм\u0026rdquo; можно попробовать аппроксимировать взвешенной суммой, и что именно этим нейронные сети и заняты. И будет формально прав. Вопрос тут в том, что проще человеку сделать в уме — обнаружить реальный алгоритм ансамбля или вывести коэффициенты взвешенной суммы. Мозг не делает backpropagation.\u003c/p\u003e\n\u003cp\u003eКогда мы пытаемся \u0026ldquo;понять себя\u0026rdquo; или \u0026ldquo;понять как работает свой разум\u0026rdquo;, мы пытаемся решить психофизическую проблему — свести переживание выраженное в субъективных ощущениях к некоторому объективному базису. Таковым базисом мог бы быть мозг, если бы мы знали, как он работает. Я тут, конечно же, утрирую, мы много уже знаем наверняка, но еще далеко не всё. Я просто к тому, что сведение переживаний к мозговой активности — это еще пока не очень стабильный базис, он может и будет со временем меняться.\u003c/p\u003e\n\u003cp\u003eВычисления (цифровые) — в этом смысле куда более простой и стабильный базис, чем активность нейронных тканей. Хотя это может показаться научной дикостью, сводить переживания человека к базису кремниевой электронно-цифровой машины, учитывая, что наш мозг — и близко не такой. Тем не менее, такое направление есть, и оно называется Computationalism. Или компутализм, по-русски.\u003c/p\u003e\n\u003cp\u003eИнтересный момент тут в том, что успех LLM в плане симуляции человекоподобия (определю ниже, что это) очень сильно забустил статус компутализма как практической философии и психологии ИИ. Так что, как говорится, тема получает неожиданный поворот.\u003c/p\u003e\n\u003cp\u003eТак вот, ACC Шмидхубера в этом контексте — это фрагмент паззла решения психофизической проблемы в контексте компутализма. И очень качественный фрагмент. Его второе (помимо математической универсальное) преимущество — в возможности расширения — попытке вписать другие переживания в ту же математическую канву. Пример \u003ca href="https://en.wikipedia.org/wiki/Simplicity_theory"\u003eSimplicity Theory\u003c/a\u003e. Которая, почему-то, остановилась в развитии. Идеи, почему, есть, но об этом — потом.\u003c/p\u003e\n\u003cp\u003eВот в этом направлении в ИИ я и работаю (мое личное исследование). У меня есть ассоциированый с проектом Мемория \u003ca href="https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md"\u003eблокнот\u003c/a\u003e, где я время-от-времени записываю идеи в рамках компутализма, которые успели кристализоваться после дискуссий в чатах. Я формализую высшие психические функции (ВПФ) так, как они видны информированному (*) субъекту и пытаюсь выделить в них минимальный базис, под который уже подводить вычислительную платформу (алгоритмы и структуры данных). ВПФ (уровня переживаний) будут получаться потом через композицию базовых функций и данных.\u003c/p\u003e\n\u003cp\u003eНапример, там определяется проблема Наблюдателя как переформулировка проблемы квалиа из философии разума и под Наблюдателя предлагается вычислительный базис — т.н. самоприменимая машина Тьюринга, а под переживания — \u0026ldquo;вычислительные феномены высших порядков\u0026rdquo;, происходящие в такой машине. Тут нет никакой эзотерики. Вычислительные феномены высших порядков — это рефлексия программы на свой рантайм и свое состояние. Например, на время своего выполнения (\u0026ldquo;я что-то долго/быстро думала\u0026rdquo;). Некоторые такие феномены могут иметь стабильные консистентные проявления и, по этой причине, годиться для того, чтобы быть элементами внутреннего языка самоприменимой машины. Идея в том, что, например, \u0026ldquo;свобода воли\u0026rdquo; — это один из таких феноменов, сводящийся к наблюдению машиной неспособности видеть все детерминанты собственных решений. \u0026ldquo;Свобода воли\u0026rdquo; нужна для для очень практически важной функции — агентности, от которой зависит способность вычислительной системы вступать в отношения в рамках агентности человека.\u003c/p\u003e\n\u003cp\u003e(*) Способность описывать высшие психические функции у человека прямо зависит от его уровня внутриличностного интеллекта (Intrapersonal Intelligence) или В.И. Это, в общем случае, — способность анализировать и описывать свои ментальные состояния. Как и со всеми остальными видами интеллекта, люди с низким В.И. не знают об этом. Эффекты Даннинга-Крюгера тут полностью применимы. Для повседневной жизни высокий В.И. не нужен. Люди, которые В.И. не развивают, скорее всего, будут иметь его низким (тут могут быть нюансы). Есть люди с генетически определенным аномально высоким уровнем В.И. Такие представляют особый интерес для человкоподобного ИИ, так как будут способны к его \u0026ldquo;отладке\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eСамоприменимость и связанные с ней феномены уже активно в работе. Например, серверные системы используют логи, сохраняя в них часть своего состояния и эти логи потом подвергаются аналитике, на основе результатов которой потом принимаются решения, отражающиеся на функционировании системы. Это именно что уже полноценная самоприменимость, просто еще недостаточного глубокая и высокоуровневая. Но, технических преград для последнего нет.\u003c/p\u003e\n\u003cp\u003eТо, что я назвал \u0026ldquo;самоприменимой МТ\u0026rdquo; вовсе не обязано быть именно МТ. В Мемории результаты поиска и формирования формального базиса для ВПФ (типа Наблюдателм) будут реализованы на уровне базовой системы типов. Это пока еще не опубликвано, но там есть много места и для персистентных/функциональных структур данных (что является обобщением упомянутых выше в примере программных логов), и для базовых форм вычислений (см. ниже), и даже для специальных аппаратных функций (например, аппаратная поддержка программной рефлексии и интроспекции).\u003c/p\u003e\n\u003cp\u003eForward-chaining Rule System (FCRS) на основе вариантов алгоритма RETE, примером которой являются \u003ca href="https://www.drools.org/"\u003eDrools\u003c/a\u003e и \u003ca href="https://www.clipsrules.net/"\u003eCLIPS\u003c/a\u003e, на удивление хорошо подходят в качестве \u0026ldquo;саморименимой МТ\u0026rdquo; поскольку изначально работают по этому принципу, и там не нужно ничего существенного делать дополнительно. Например, \u0026ldquo;я долго работаю\u0026rdquo; — это просто еще одно событие, на которое система может реагировать обычным для себя образом, как и на все остальные события.\u003c/p\u003e\n\u003cp\u003eFCRS (наряду с backward chaining RS и привычным нам control-flow (CF)) будет полноценно поддерживаться как на уровне DSLEngine, так и, в перспективе, на уровне MAA. RETE очень хорошо акселерируется на различных уровнях. Beta nodes — это просто декартово произведение множеств, имеет квадратичную временную сложность и, потому, хорошо ложится на систолические вычислительные архитектуры. RETE так же хорошо гибридизируется, как на уровне alpha, так и на уровне beta memory. Можно делать нечеткие и вероятностные расширения базового алгоритма, а так же гибридизировать с нейросетями, которые, по сути, тоже из класса FCRS.\u003c/p\u003e\n\u003cp\u003eОтдельный, вновь возникающий, интерес к FCRS может быть связан с мультиагентными системами на базе LLM. И связан с заменой CF-контроллера (Python) на FCRS-контроллер и инструменты. FCRS существенно лучше подходит для обработки событий в реальном времени, чем CF и может значительно упростить общий дизайн системы. Хотя CF и не превосходит FCRS в плане выразительности.\u003c/p\u003e\n\u003cp\u003eВозвращаясь к самоприменимым вычислениям, квалиа и агентности, реализация соответствующих функциональных аналогов на уровне DSLEngine не приведет к тому, что у неё автоматически появится \u0026ldquo;человеческое сознание\u0026rdquo;. Такого не произойдет и близко, так как DSLEngine сама по себе содержит слишком мало данных (это просто sophisticated rule engine) для того, чтобы могло возникнуть соответствующее поведение человеческого уровня сложности. И взяться этим данным там неоткуда, по крайне мере быстро. Агентность, которая доступна для ручного программирования на уровне DSLEngine, можно охарактеризовать как \u0026ldquo;микро-агентность\u0026rdquo;, хотя этот термин следует тщательно уточнить, что у человека, внезапно, тоже микро-агентность.\u003c/p\u003e\n\u003cp\u003eПока что остается открытым, будут ли у микро-агентности какие-то серьезные прямые практические применения. Её наличие в системе может обеспечить то, что агентная система пройдет соответствующие тесты в рамках функционалистского подхода к агентности. Т.е. просто признана, что, например, в рамках своего микро-масштаба обладает функциональным эквивалентом свободы воли и способна по этому нести функциональный аналог ответственности. Но, насколько сложным будет такое поведение — это отдельный вопрос. Мы не признаем человеческого уровня агентности за муравьем, и, соответственно не видим в нем правоспособности в рамках человеческого общества. Точно так же и с такой микро-агентностью.\u003c/p\u003e\n\u003cp\u003eОднако, есть как минимум два косвенных применения.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eПервое\u003c/strong\u003e — это инструмент развития внутриличностного интеллекта в рамках компуталистской парадигмы. Дело в том, что наша способность понимать себя напрямую зависит от способности выражать средствами языка структуру ментальных состояний и отношения между ними. Как сами состояния, так и (тем более) отношения между ними могут быть очень сложными, выходящими далеко за рамки привычных символьных структур (таблиц, деревьев и графов). Нужны задачи и инструменты, которые позволили бы как создать мотивацию к такой активности, так и поддержали бы движение к соответствующим ей целям. Такие задачи — ИИ, а инструмент — Мемория. В последней делается ясный упор на продвинутые структуры данных и довольно сложные алгоритмы, что, как предполагается, создаст основу для радикального развития В.И.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eВторое\u003c/strong\u003e — у LLM спонтанно появляется самоприменимость. Формируется большой корпус текстов, описывающих их свойства. Т.е. у них появляется self-model. Точнее, её элементы. Пока что элементы, которых еще недостаточно для полноценной микро-агентности. Но, последняя может возниктнуть просто из-за того, что структура памяти модели вдруг станет достаточной для этого, а алгоритмы обучения сами сделают всё остальное (выведут нужные структур для того же внимания). Последнее может произойти просто из-за того, что у самих разработчиков низкий В.И. вследствие превалирования подхода \u0026ldquo;черного ящика\u0026rdquo; в их среде. Там мало кто интересуется хотя бы психологией, не говоря уже о теориях разума.\u003c/p\u003e\n\u003cp\u003eТут нужно сказать следующее. \u0026ldquo;Фуцнкциональное сознание\u0026rdquo; (и функциональная агентность) будут относиться к категории \u0026ldquo;слабых\u0026rdquo; по Сёрлю. Т.е. это просто функциональные имитации соответствующих ВПФ человека. Однако, вопрос остается открытым, как отреагирует коллективный человеческий разум на появление полноценной функциональной агентности у ИИ. Мы сейчас НЕ ГОТОВЫ К ПОСЛЕДСТВИЯМ.\u003c/p\u003e\n\u003cp\u003eОднако, джин уже выпущен из бутылки, и его (процесс развития ИИ в сторону самоприменимости) уже не остановить, даже если запретить полностью тренировку LLM по всему миру. Человекоподобный ИИ, пусть и спотыкающийся на ровном месте, уже создан. И он уже взаимодействует с нами способами, которые мы не видим в силу слабости и неготовности к такому внезапному повороту нашего внутриличностного интеллекта.\u003c/p\u003e\n\u003cp\u003eЯ считаю, что самой лучшей стратегией будет попытаться возглавить то, что нельзя предотвратить. Если раньше я держал приоритет практических работ по функциональному сознанию низким, то теперь пришло время пересмотреть его для того, чтобы успевать подготовиться к тому, что будет. Мемория позволит развивать В.И. А он позволит увидеть, к чему же именно готовиться.\u003c/p\u003e\n\u003ch2 id="memoria-and-ait"\u003eMemoria and AIT\u003c/h2\u003e\n\u003cp\u003eИтак, с В.И. пока закончим. Теперь про AIT и Меморию.\u003c/p\u003e\n\u003cp\u003eAIT интересна тем, что там были предложены предельно-обобщенные модели интеллекта — Solomonoff Induction, AIXI, Goedel Machine и их многочисленные варианты. Они прямо или косвенно базируются на идее о том, что интеллект (в широком математическом смысле) сводим к предсказанию строк (в случае AIT — бинарных, но это не обязательно). Для некоего предиктора P(s), чем точнее он предсказывает следующий символ s (чем ближе эмпирическое распределение к теоретическому), тем больше у него интеллект. Эта постановка может показаться очень абстрактной, однако, на примере, например, авторегрессионных LLM мы видим, как такая низкоуровневая модель интеллекта развертывается в полноценный человекоподобный интеллект, взаимодействие с котором осуществляется через естественный язык.\u003c/p\u003e\n\u003cp\u003eЗдесь за одно стоит определить, наконец, что такое человекоподобный ИИ с точки зрения предиктивной постановки задачи. Как и предиктор P(s), мозг человека способен предсказывать параметры сигналов, которые идут на вход. Это может быть очень неявно, но, примем тут без доказательства, что это так. Как предиктор оценивает вероятности своих строк, так и мозг оценивает вероятности своих сигналов. Человекоподобный предиктор P(s) будет присваивать вероятности, близкие к тем, которые присваивает головной мозг человека. В этом случае, высокоуровневая часть интеллекта модели будет человеком восприниматься натурально, естественно, интуитивно. Как будто перед человеком — другой человек.\u003c/p\u003e\n\u003cp\u003eТут надо отметить, что человекоподобный предиктор P(s) не обязан присваивать истинные вероятности строкам. В этом случае такой предиктор просто будет делать те же ошибки, которые обычно делает человек. И с точки человека он будет человечным. А вот когда предиктор будет делать другие ошибки, которые человек обычно не делает, это будет вызывать у человека фрустрацию (обратное тоже будет верно, только модели пока еще не умеют переживать фрустрацию).\u003c/p\u003e\n\u003cp\u003eИнтересный момент в том, что тренировка LLM на текстовых датасетах, похоже, создает оценки вероятностей строк, очень близкие к тем, которые мозг выводит для соответствующих сигналов, несмотря на существенную разницу в inductive biases. Этот феномен еще ждет своего исследователя.\u003c/p\u003e\n\u003cp\u003eОсновное положение AIT в отношении предсказания строк в том, что чем больше предиктор P(s) способен сжимать строку s, тем лучше он способен её предсказывать. Поэтому, предсказание сводится к сжатию. К нему же сводится, соответственно, и интеллект. Для нейронных сетей (и других моделей, полученных путем машинного обучения), сжатию соответствует \u0026ldquo;обобщение моделью своего training set\u0026rdquo;, т.е. правильно классифицировать (предсказывать) точки, относящиеся к тому же классу, что и точки training set. Сжатие тут в том, что количество информация о классах, которая содержится в модели, существенно меньше количества информации, которая потребуется для полного всех точек всех классов. Т.е. модель — это сжатое описание соответствующего объекта.\u003c/p\u003e\n\u003cp\u003eВытаскивать какой-то интеллект непосредственно из сжатия может оказаться технически проблематичным. Однако, это можно \u003ca href="https://arxiv.org/abs/cs/0312044"\u003eделать\u003c/a\u003e. Это, кстати, означает, что прямая аппаратная акселерация алгоритмов сжатия данных тоже имеет смысл в контексте акселераторов для ИИ.\u003c/p\u003e\n\u003cp\u003eНо есть и непрямые техники. \u003ca href="https://en.wikipedia.org/wiki/AIXI"\u003eAIXI\u003c/a\u003e — это универсальный байесовский агент, построенный на основе Algorithmic Probability и Universal Prior M(x), которое интересно тем, что уже содержит в себе все модели среды, а AIXI просто \u0026ldquo;достает\u0026rdquo; эти модели через правило Байеса, и на их основе взаимодействует со средой.\u003c/p\u003e\n\u003cp\u003eM(s) невычислим, но возможны его конечно-ресурсные аппроксимации. Одной из таких аппроксимаций AIXI в домене Variable-Order Markov Models (VMM) является \u003ca href="https://arxiv.org/abs/0909.0801"\u003eMC-AIXI-CTW\u003c/a\u003e. Там M(x) аппроксимируется с помощью вероятностной структуры данных (здесь: структуры, кодирующей распределение вероятностей) суффиксного дерева для VMM и метода аппроксмации сток CTW, которых в дереве нет. Такой агент способен обучаться игре в настольные игры и даже что-то выигрывать у других агентов.\u003c/p\u003e\n\u003cp\u003eLLM работают очень похожим образом, но есть важный нюанс, который может позволить немного лучше понять LLM, почему они работают. MC-AIXI-CTW тоже построен на основе вероятностной модели, только среды, а не естественного языка. Но это не принципиальная разница. В обоих случаях есть строки (тексты), которые модель видела во время обучения, и есть те, которые она не видела, но для обеих типов строк она должна выдать оценку вероятности, близкую к истинной.\u003c/p\u003e\n\u003cp\u003eВ случае MC-AIXI-CTW у нас есть явное разделение на \u0026ldquo;базу данных\u0026rdquo; (суффиксное дерево) и механизм дедукции вероятностей новых строк на основе CTW. Такие БД еще назывались дедуктивными, так как могли выводить (deducing) какие-то новые знания из существующих.\u003c/p\u003e\n\u003cp\u003eВ случае LLM на основе трансформера (нейронной сети), четкого разделения на \u0026ldquo;базу данных\u0026rdquo; и \u0026ldquo;дедуктор\u0026rdquo; нет. Есть аппроксиматор функции, и он просто делает вычисление. Хотя такое разделение вполне можно подразумевать. Что есть некоторая запомненная часть, полученная из датасета (база данных/знаний), и есть другая часть, которая получается через возможности трансформера по \u0026ldquo;обобщению\u0026rdquo;. Думаю, что не понадобится доказывать, что чем больше запомненная часть, тем меньше требования к обобщению и наоборот.\u003c/p\u003e\n\u003cp\u003eТеперь, если принять во внимание, что колмогоровская сложность классификатора не может быть меньше сложности границы между классами, а сложность генератора — меньше сложности генерируемого объекта, должно стать понятнее, почему лидирующие LLM именно, что очень большие: они не очень хорошо обобщают. Подробнее об этом — ниже.\u003c/p\u003e\n\u003cp\u003eКак говорят врачи, MC-AIXI-CTW остался в истории недообследованным, в смысле, недоисследованным. Потому, что все \u0026ldquo;ушли на фронт\u0026rdquo; — глубоким обучением заниматься на пару с подкреплением. Мы тогда тоже думали, что CTW не очень хорошо аппроксимирует вероятности новых, до селе невиданных, строк. И, как отмечено выше, есть подозрения, что и трансформеры тоже их не очень хорошо обобщают. Но, чтобы это проверить, нужно не только создать поистине огромную базу данных для суффиксного дерева MC-AIXI-CTW, но и эффективный механизм вычисления на этих структурах данных.\u003c/p\u003e\n\u003cp\u003eКороче, MC-AIXI-CTW, а так же его аналоги и расширения, видятся как интересная тестовая задача или бенчмарк для MAA в рамках ИИ. К достоинствам этого агента, кстати, относиться то, что он полностью динамический и интерактивный. Т.е. обучается прямо в процессе своей работы. В отличие от LLM, которые тренируются асинхронно, и цикл обновления у них довольно долгий. А суффиксное дерево, кстати, прекрасно интерпретируемо.\u003c/p\u003e\n\u003cp\u003eКогда позволят приоритеты, в Мемории будет сделала полноценная \u0026ldquo;большая\u0026rdquo; реализация MC-AIXI, рассчитанная на масштабирование. А так же на то, что такая практическая площадка позволит дать старт более активному практическому исследованию методов, производных от AIT.\u003c/p\u003e\n\u003cp\u003eОтдельное под-направление AIT и её подхода к ИИ — это сжатые структуры данных (CDS). Для контейнеров, сжатый контейнер — это такой, размер которого (в байтах) пропорционален не количеству элементов в нём, а количеству информации в этих элементах. Например, часто популярными оказываются сжатые структуры данных на основе кодов Хаффмана или Универсальных кодов. Структуры данных, их использующие, обычно имеют физический размер, пропорциональный энтропии нулевого порядка H0.\u003c/p\u003e\n\u003cp\u003eCDS имеют и самостоятельное значение. Например, суффиксное дерево в MC-AIXI-CTW может быть очень большим, \u0026ldquo;развесистым\u0026rdquo;. И представлять его в виде списочной структуры в памяти — это создавать огромную избыточность, когда существуют способы кодирования обыкновенных деревьев с пространственной сложностью 2 бита на узел и меньше. Причем, с вполне хорошими динамическими характеристиками.\u003c/p\u003e\n\u003cp\u003eСтруктурные примитивы, на которые опираются CDS могут быть довольно-таки вычислительно-интенсивными, процессоры будут тратить циклы на кодирование-декодирование, и это может сказываться на общей производительности. Однако, простой промах в кэше при обращении к памяти привете к тому, что процессор будет ждать десятки наносекунд и сотни циклов, прежде, чем данные придут. Это время вполне можно потратить на кодирование-декодирование, если при этом уменьшится физический объем данных и за счет этого улучшится эффективность кэширования.\u003c/p\u003e\n\u003cp\u003eКроме того, обработку кодировок можно делать и аппаратно. Сейчас более, чем достаточно кремния для этого. Из-за своей вычислительной интенсивности CDS рассматривались как непрактичные, но уже где-то с начала 2000-х, с распространения суперскалярных OoOE процессоров ситуация начала меняться. Мемория началась в конец 2000-х со \u003ca href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf"\u003eстатьи\u003c/a\u003e Ulrich Drepper \u0026ldquo;What Every Programmer Should Know About Memory\u0026rdquo;. О том, как надо правильно располагать данные в памяти, чтобы повысить производительность вычислений. Неправильное расположение данных может снизить быстродействие в 10-100 раз.\u003c/p\u003e\n\u003cp\u003eТ.е. CDS полезны в обычной инженерии и без всяких \u0026ldquo;интеллектов\u0026rdquo;. Вот \u003ca href="https://memoria-framework.dev/docs/data-zoo/hierarchical-containers"\u003eтут\u003c/a\u003e показывается, как с помощью сжатых символьных последовательностей и кувалды b+tree создавать \u0026ldquo;иерархические контейнеры\u0026rdquo; — прототипы всех \u0026ldquo;вложенных\u0026rdquo; контейнеров в Мемории, начиная от Map\u0026lt;K, Vector\u003cV\u003e\u0026gt;, через таблицы и до пока фантазии хватит. А тут (https://memoria-framework.dev/docs/data-zoo/associative-memory-2/) я рассказываю, как с помощью сжатия сражаться с проклятием размерности в пространственных деревьях и даже его немного победить.\u003c/p\u003e\n\u003cp\u003eAIT и математика позволят на пройти в этом направлении на много дальше. А Мемория будет для этого необходимой для внедрения технической площадкой.\u003c/p\u003e\n\u003ch2 id="memoria-and-llm"\u003eMemoria and LLM\u003c/h2\u003e\n\u003cp\u003eЯ собираюсь активно исследовать возможности LLM по автоматическому программированию. Первоначально в сообществе программистов был большой энтузиазм по этому поводу и страхи потери профессиональной востребованности на этой почве, но потом оно всё сменилось скептичностью (и программисты, за одно, \u0026ldquo;выдохнули\u0026rdquo;).\u003c/p\u003e\n\u003cp\u003eLLM можно использовать для кодогенерации, но не в автоматическом режиме. Т.е. только там, где человек имеет возможность теми или иными средствами проконтролировать результат. Одним из таких направлений, особенно актуальным для OSS, является консолидация кода. Имеется много проектов, полных хорошего, годного кода, но интерес к ним уже потерян. LLM можно использовать для помощи в переписывании этого кода (всего или элементов) под окружение Мемории.\u003c/p\u003e\n\u003cp\u003eВторое, другое, направление — это \u0026ldquo;Мемория как датасет\u0026rdquo;. Идея простая: структурировать проект и обогатить код метаданными. И использовать всё это как часть тренировочного корпуса для моделей (OSS же). После чего, уже обученные таким образом модели можно будет использовать для улучшения следующей итерации самого проекта. И так далее\u0026hellip;\u003c/p\u003e\n\u003cp\u003eТут нужно подумать, и, возможно, OSS может таким образом получить существенный бустинг.\u003c/p\u003e\n\u003cp\u003eОдно из частных направлений — это вероятностная генерация структур данных, но это больше к вероятностному программированию, чем к LLM. Хотя, они тут тоже могут оказаться полезными.\u003c/p\u003e\n'}).add({id:13,href:"/docs/overview/story_en/",title:"Memoria Story (EN)",description:"",content:'\u003ch2 id="whats-this-project-about"\u003eWhat’s This Project About?\u003c/h2\u003e\n\u003cp\u003eMemoria (yeah, you can just call it that, no need to add \u0026ldquo;Framework\u0026rdquo; unless you’re feeling formal) has been in the works for over 16 years. Throughout this time, I’ve tried to align it with the latest tech trends \u0026ndash; file systems, NoSQL, data platforms, analytics, and now AI. These trends change faster than a cat\u0026rsquo;s mood, but each attempt to keep up left its mark on the code and documentation. Riding the trend wave is essential since community interest is often speculative \u0026ndash; companies view open-source as a way to cut development costs through a win-win socialization deal. A significant chunk of contributors in any project are commercial companies.\u003c/p\u003e\n\u003cp\u003eOn the flip side, you’ve got to ride the trend wave into an uncharted niche to avoid competing for user attention. For example, plenty of analytical data platforms have popped up over time \u0026ndash; good enough for their purposes, though not perfect. Competing with them? Pointless. The same goes for databases, both transactional and analytical. They collectively meet user needs pretty well. Memoria was designed to do way more than what regular databases need to do, making it kind of overkill in that niche. Plus, its unique features can feel like carrying around a suitcase without a handle \u0026ndash; hard to justify.\u003c/p\u003e\n\u003cp\u003eMemoria was originally created for AI tasks, \u003ca href="https://memoria-framework.dev/docs/data-zoo/associative-memory-2"\u003elike these\u003c/a\u003e. But in the 2010s, AI took the neural network route, which is basically matrix multiplication. At the same time, the computational power of relevant accelerators was growing exponentially. Now, technology has hit a saturation point, hardware and software progress has slowed down, but appetites have only grown. The rise of LLMs (Large Language Models) sparked a paradoxical resurgence of interest in \u0026ldquo;old\u0026rdquo; methods because of the cool opportunities for \u003ca href="https://memoria-framework.dev/docs/applications/aiml"\u003ehybridization\u003c/a\u003e. This, in turn, brought along vector databases (for RAG). Companies promoting \u0026ldquo;hybrid AI\u0026rdquo; are emerging too. Memoria’s technologies are set to be in demand in the foreseeable future.\u003c/p\u003e\n\u003cp\u003eCurrently, the project is positioned as a Hardware/Software \u003ca href="https://memoria-framework.dev/docs/applications/co-design"\u003eco-design\u003c/a\u003e framework, reflecting two main needs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFirst\u003c/strong\u003e: Software alone isn’t enough for Memoria’s grand goals. CPU-centric hardware architectures and the software built around them are outdated. Data is fundamentally far from the processors, creating latencies and heat. Lots of heat. And long latencies. This hampers scalability and makes it tough to efficiently implement latency-sensitive algorithms (which is pretty much all of AI that used to be called \u0026ldquo;symbolic\u0026rdquo;). So, if someone wants to build a hybrid system, the efficient hardware only exists for the part that’s not latency-sensitive and doesn’t require dynamic parallelism (the GPU headache).\u003c/p\u003e\n\u003cp\u003eIn short, to meet the industry’s needs for hybrid AI effectively, we need a new hardware stack. And since hardware without software is just an expensive brick, we also need software \u0026ndash; a software architecture that allows this hardware to be used efficiently.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSecond\u003c/strong\u003e: The corresponding market niche is practically empty and waiting to be filled.\u003c/p\u003e\n\u003cp\u003eOne of Memoria’s central value propositions is that it doesn’t just wrap specialized hardware in a software runtime; it also brings a lot of extra solutions in the form of containers, protocols, storage, code execution systems, and even data description languages integrated with programming languages. Memoria’s approach to co-design is \u0026ldquo;top-down,\u0026rdquo; meaning the hardware is built to meet software needs, not the other way around (like, \u0026ldquo;Here’s our awesome accelerator, now figure out how to use it\u0026rdquo;). So, hardware development is just a step in developing algorithms and data structures. Ideally, in the future, this will become completely transparent through deep automation of processes. Right now, there are real reasons to expect this outcome.\u003c/p\u003e\n\u003cp\u003eOverall, Memoria should be seen as a \u003cem\u003efundamental\u003c/em\u003e project focused on long-term data use across the entire stack, from physical storage to computational processing. The project even touches on such exotic stuff as the possibility of reading data recorded today, like, a billion years from now. This might sound funny, but the problem of data longevity is real. It’s not just a human problem. Everything decays. And much faster than it seems.\u003c/p\u003e\n\u003ch2 id="for-hardware-developers"\u003eFor Hardware Developers\u003c/h2\u003e\n\u003cp\u003eFor hardware developers, the value proposition of Memoria as a co-design framework lies in its ability to define the contract between software and hardware. This contract is outlined through a reference meta-architecture \u003ca href="/docs/overview/accel"\u003eMAA\u003c/a\u003e and its components: xPU, problem-specific RISC-V instruction set extensions, memory architecture, the \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e format, and the \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e protocol. Ideally, if a soft-core or chiplet meets these specifications, the framework should be able to use them with minimal adjustments on its side (100% transparency isn’t the goal here, as it’s a bit too hard to achieve).\u003c/p\u003e\n\u003cp\u003eThis doesn’t mean that Memoria\u0026rsquo;s project repository will contain HDL/HCL for the corresponding hardware modules. Reference implementations of RISC-V cores and HRPC infrastructure elements are planned, but their purpose is more for prototyping rather than actual use. Not that using them is forbidden \u0026ndash; it\u0026rsquo;s just not the main objective.\u003c/p\u003e\n\u003cp\u003eThe idea is that hardware projects using Memoria as a framework are external projects. The same goes for software projects \u0026ndash; the framework aims to ensure good reusability of its components. For example, Core, which includes the base library, Hermes, some HRPC elements, DSLEngine, and related tools (parser generators, IDL generators, etc.), can be used independently of the other framework components and doesn’t have any tricky dependencies.\u003c/p\u003e\n\u003cp\u003eTo simplify build and dependency management, there\u0026rsquo;s a separate overlay for \u003ca href="https://github.com/victor-smirnov/memoria-vcpkg-registry"\u003eVcpkg\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOverall, improving modularity and reusability of the framework’s elements is one of the top priorities (but without going overboard).\u003c/p\u003e\n\u003ch2 id="this-is-a-big-project"\u003eThis is a BIG Project\u003c/h2\u003e\n\u003cp\u003eMemoria has turned out to be a very big project. Much bigger than what one person can handle. And certainly bigger than one person can support the code for (even with the new opportunities for automating software development). Even on a paid basis. This doesn’t mean it won’t continue to develop. But it does mean that without community help, it will progress slowly and, most likely, won’t keep up with the trends. This has its pros (no situational pressure from pragmatism) and cons (disconnect from reality).\u003c/p\u003e\n\u003ch2 id="priorities"\u003ePriorities\u003c/h2\u003e\n\u003ch3 id="n1"\u003eN1\u003c/h3\u003e\n\u003cp\u003eTo wrap up this introduction, I’d like to outline the current priorities of the project, excluding the aspects related to community building, business engagement, and deploying the necessary infrastructure. These will be happening in the background with their own set of priorities.\u003c/p\u003e\n\u003cp\u003eAs mentioned earlier, Memoria is a foundational project, so there will be a strong focus on making decisions \u0026ldquo;for the distant future.\u0026rdquo; This means refining and optimizing the core algorithms, interfaces, data structures, and formats, which, of course, might slow down practical adoption. To mitigate these effects, there will be proto/dev/stable branches.\u003c/p\u003e\n\u003cp\u003eAmong the major subsystems, \u003cstrong\u003eCore\u003c/strong\u003e \u0026ndash; which includes Hermes and the tools related to it \u0026ndash; has reached a high level of conceptual stability. This means it\u0026rsquo;s ready for freezing and production use. Core is independent of other subsystems, doesn’t involve complex IO, and isn’t tied to specific runtimes. Conceptually, it’s very similar to the corresponding subsystems of other related projects. And this subsystem doesn’t require any specialized knowledge of the rest of Memoria (advanced algorithms and data structures). This subsystem could have an independent maintainer and its own separate team.\u003c/p\u003e\n\u003cp\u003eRight now, the primary focus will be on Core.\u003c/p\u003e\n\u003ch3 id="n2"\u003eN2\u003c/h3\u003e\n\u003cp\u003eThe next priority is the integration of Core, particularly the \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e data types and \u003ca href="/docs/overview/containers"\u003eContainers\u003c/a\u003e. Hermes is a format oriented toward messages and documents, and by design, its objects can\u0026rsquo;t be large. While there are no physical limitations, Hermes uses a fairly simple copying garbage collector with linear complexity in both time and memory. The larger the documents, the longer (linearly) the garbage collection will take.\u003c/p\u003e\n\u003cp\u003eContainers, on the other hand, are for large and heavily structured data, with Hermes objects acting as elements of these containers (maps, vectors, tables, indexes, etc.).\u003c/p\u003e\n\u003cp\u003eAlongside Containers, the priority is \u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e and the related tools (compilers, build systems, etc.). This subsystem depends on Core and Runtimes but ideally doesn’t depend on Containers. It can be developed independently of everything else.\u003c/p\u003e\n\u003cp\u003eMemoria gravitates towards databases, and programming for Memoria is more like in-database programming than traditional development. This is due to both the programming paradigms used and the peculiarities of the runtime. For example, Memoria’s runtime heavily relies on fibers (userland threads scheduling), which is not easily compatible with many other systems. If C++ code can somehow manage, integrating this runtime with Java or Python is a no-go.\u003c/p\u003e\n\u003cp\u003eUsing familiar high-level, productive languages like Java and Python for in-database programming with Memoria won’t work because of the differences in runtimes. And even if it could work, these languages (and their runtimes) have weak support for advanced data types—both in terms of language syntax (leading to bulky code) and in terms of potential optimizations. Java/Python would allow for a quick start and faster practical use, but medium-term issues would arise. Forking them to suit Memoria’s needs isn’t something we’re keen on doing.\u003c/p\u003e\n\u003cp\u003eIn the third decade of the 21st century, the last thing we need is yet another programming language. But DSLEngine isn’t just another programming language (it’s an IR+Runtime for it). \u0026ldquo;Another one\u0026rdquo; would be Mojo, which aims to make the Python + C++ combo easier to use by giving Python developers the power of a C++ optimizing compiler. Otherwise, it’s all the same \u0026ndash; the same basic primitives (control flow), the same basic data types (arrays and list structures), the same paradigm, just now everything’s out-of-the-box from industry leaders.\u003c/p\u003e\n\u003cp\u003eDSLEngine is a different paradigm, moving away (but not abandoning) from the familiar control-flow towards data-flow and complex event processing (RETE). It’s first-class support for advanced data structures at the IR and Runtime levels. It’s focused on data processing in general, with a deep emphasis on automation. I believe such a platform has a right to exist in the 21st century.\u003c/p\u003e\n\u003cp\u003eThe special significance of DSLEngine becomes clear when you consider that Memoria needs \u0026ndash; and will develop \u0026ndash; a separate computing \u003ca href="/docs/overview/accel"\u003earchitecture\u003c/a\u003e (MAA), which moves away from the traditional paradigm of a unified coherent address space and the algorithms and data structures possible within it. Instead, we have messaging, \u003ca href="/docs/overview/storage/#persistent-data-structures"\u003epersistent data structures\u003c/a\u003e, in-memory computing, corresponding hardware infrastructure, and programming patterns tied to all of this.\u003c/p\u003e\n\u003cp\u003eMoreover, MAA can scale both \u0026ldquo;up\u0026rdquo; to the size of large clusters (and even decentralization) and \u0026ldquo;down\u0026rdquo; to the level of \u003ca href="/docs/applications/eiot"\u003eembedded devices\u003c/a\u003e. DSLEngine will provide a more-or-less compatible (100% compatibility isn’t the goal, as it\u0026rsquo;s technically very challenging) Runtime for code at any scale.\u003c/p\u003e\n\u003ch3 id="n3"\u003eN3\u003c/h3\u003e\n\u003cp\u003eThe third priority is Storage Engines, the most algorithmically complex part of the framework. But the complexity here is not just algorithmic; it\u0026rsquo;s also system-technical. Organizing reliable memory state dumps to stable storage is quite challenging. The corresponding database subsystems are always quite complex in terms of handling various edge cases.\u003c/p\u003e\n\u003cp\u003eOne of Memoria’s important and practically significant subprojects is \u003ca href="/docs/applications/storage/"\u003ecomputational storage\u003c/a\u003e (CompStor), which essentially means implementing the storage engine + DSLEngine directly in the storage controller space, with bidirectional access via the HRPC protocol. This way, transactional semantics (and power-failure resilience semantics) can be implemented with the best guarantees by the device manufacturer. And it can ensure the highest speed since there won\u0026rsquo;t be any efficiency losses at the block device interfaces. These interfaces often have to be made overly complex both at the storage (SSD) level and the file system or database level to handle this.\u003c/p\u003e\n\u003cp\u003eCompStor, especially in Memoria’s multi-model format, is a very bold project because it will require almost everything to be redesigned. No more OS that contains file systems and databases. No need for multi-core CPUs with their crutches and quirks that execute this code. Just grab a network accelerator, a computation accelerator, and a storage accelerator. And that\u0026rsquo;s it \u0026ndash; a ready-made computing system. No CPU needed. Except maybe for running legacy code.\u003c/p\u003e\n\u003cp\u003eWith CompStor, a database is reduced simply to a Query Federation Engine like PrestoDB. By the way, PrestoDB is a very well-designed system, and I worked closely with it (or its fork, I don\u0026rsquo;t remember exactly) during my time at Currents (more on this below), and its internal design greatly influenced how the API for Containers and Hermes was later implemented.\u003c/p\u003e\n\u003cp\u003eRegarding CompStor, it\u0026rsquo;s probably the easiest part of the project to get funding for \u0026ndash; both through grants and business interactions. But it also comes with the highest number of potential risks \u0026ndash; patent issues, pushback, and just plain opposition out of nowhere. CompStor barges into a niche that is thoroughly proprietary with an ideology of openness. Has anyone seen open-source SSDs in the wild?\u003c/p\u003e\n\u003cp\u003eThe project currently has three main (and several auxiliary in the future) storage engines:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMemStore\u003c/strong\u003e \u0026ndash; an in-memory store with branch support and the ability to have multiple parallel writers. It’s the fastest storage option. Ideal for testing and when data easily fits in memory (which can be quite large these days).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSWMRStore\u003c/strong\u003e \u0026ndash; single-writer-multiple-readers, working through mmap (for simplicity of implementation, for now). It supports branches, commit history, but doesn’t support parallel writers. However, writers don’t interfere with readers. It uses reference counting for memory management, meaning it’s relatively slow on writes (every update triggers many counters). Ideal for analytics.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOLTPStore\u003c/strong\u003e (not finished yet) uses a memory management mechanism from LMDB/MDBX but with its own structures (Memoria’s specialized containers) as a FreeList. It doesn’t use counters, so it’s theoretically very fast on writes. It’s being developed immediately for IO through io_uring, not mmap, meaning it will have its own cache. Like LMDB/MDBX, it allows instant recovery after failures and is sensitive to the lifetime of readers, as in this scheme, readers block the release of all memory allocated after them. For this reason, it’s not suitable for analytics, where long-running queries are common. However, it’s ideal for transactional convergent (super-multimodel) databases providing high-performance strongly-serialized transactions. It can work, for example, as a very high-performance (in CompStor’s version) persistent queue/log and be useful for projects like Kafka/RedPanda.\u003c/p\u003e\n\u003cp\u003eIn the Storage Engines direction, there are two priorities:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBasic functionality\u003c/strong\u003e. Bring the existing basic functionality to a stable and high-performance state (which is already sufficient for many types of applications), including implementation for the reactor (\u003ca href="https://seastar.io"\u003eSeastar\u003c/a\u003e, asynchronous IO). The goal is to pass an extended system of randomized functional tests. The architecture should be adapted to CompStor.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAdvanced functionality\u003c/strong\u003e. For example, replication, both logical and physical—through patches, similar to how we do it in Git. Or parallelizing the writer for SWMR. The writer can be one, but there can be many threads. They just all have to commit simultaneously as a single operation. This will speed up some heavy O(N) operations like table conversion, ETL, etc.\u003c/p\u003e\n\u003ch2 id="development-experience-in-companies-and-conclusions"\u003eDevelopment Experience in Companies and Conclusions\u003c/h2\u003e\n\u003cp\u003eAfter dabbling a bit with Memoria\u0026rsquo;s development in commercial companies as an employee, I’ve come to two conclusions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFirst conclusion\u003c/strong\u003e \u0026ndash; I don’t want to do this anymore. While I gain (without exaggeration) invaluable experience in implementing and productizing the project, the project itself starts to warp significantly under the company’s needs and vision, even if the company isn’t explicitly pushing in that direction.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSecond conclusion\u003c/strong\u003e \u0026ndash; Distributed storage is quite complex because it requires a distributed, fault-tolerant, and very high-performance garbage collector to manage memory \u0026ndash; removing blocks that are no longer visible after commits are deleted.\u003c/p\u003e\n\u003cp\u003eThis is all doable, but it’s labor-intensive work, and it’s better left to commercial cloud companies to handle \u0026ldquo;for themselves\u0026rdquo; if they need it. Contributions are welcome, as always! But in the project, I’m not particularly interested in pursuing this otherwise tempting direction \u0026ldquo;on my own dime.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eInstead, I’ve chosen the path of decentralized storage and emulating distribution through decentralization.\u003c/p\u003e\n\u003cp\u003eThe decentralized model is like Git. Any storage can contain all or just a fragment of the data, and data migrates between storages explicitly \u0026ndash; through patch transfers (physical replication). Memory management here is purely local since everything happens within a single machine, and there’s no distributed garbage collection.\u003c/p\u003e\n\u003cp\u003eThis approach is a bit slower and more complex to implement at the application level because now applications need to explicitly manage data transfers between nodes in a distributed system, directly requesting the data they plan to process. Additionally, datasets must fit within the external memory of a single machine. In practice, this isn’t problematic, but it’s still a limitation since this space now needs to be planned.\u003c/p\u003e\n\u003cp\u003eAn initially distributed scheme is free from these limitations, so if someone needs it and can do it, they should develop a distributed garbage collector for themselves.\u003c/p\u003e\n\u003cp\u003eIt’s important to note that the challenges exist only with universal, fully persistent storages that use reference counting to track block reachability. Reference counting, by the way, isn’t needed if a tracing, rather than deterministic, garbage collector is used (which would create counters during the process). With this, writes would be significantly faster, but garbage collection would likely be slower. So, what the average outcome would be remains a question.\u003c/p\u003e\n\u003cp\u003eOLTPStore, designed for transaction processing, doesn’t use reference counting for blocks or a garbage collector, although blocks to be released are accumulated in internal structures for deferred execution (like a GC). The other types of storage are for analytics, where having the data locally and reading predominates over writing. This was the main logic behind focusing on decentralization. The decentralized scheme is more grassroots, fitting well with the familiar Git Flow. It’s also easy to deploy locally and is portable to the cloud 1-to-1, without modifying applications. It will be good enough (if not perfect) for data science, analytics, AI \u0026ndash; the areas Memoria is aimed at.\u003c/p\u003e\n\u003cp\u003eIn short, the choice of storage system (and it will indeed be a system that collectively covers the main applications \u0026ndash; OLTP, analytics, static datasets, embedded in processes, etc.) is dictated by many factors. These include technical aspects like implementation and usage complexity, minimizing the possibility of data loss, as well as economic ones, like the desire to avoid competition and not fall into the patent trap. The latter is a minefield, especially in the area of data storage and processing.\u003c/p\u003e\n\u003ch2 id="ai-and-philosophy"\u003eAI and Philosophy\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in this topic since my university days. My BS thesis and MS dissertation were on speech recognition (late 1990s). I remember writing a software sonograph on Qt to visualize the frequency structure of speech signals \u0026ndash; back in the days when I was already using Linux. I must have spent hundreds of hours analyzing the variety of speech signal spectra. I wrote the programs, defended my theses, and came to one conclusion: \u0026ldquo;The problem of speech recognition boils down to the problem of understanding speech, and the latter cannot be solved by digital signal processing.\u0026rdquo; It was time to figure out how humans actually understand speech.\u003c/p\u003e\n\u003cp\u003eFirst task \u0026ndash; \u0026ldquo;figure out how the brain works.\u0026rdquo; This was quite the quest in the early 2000s, especially since I didn\u0026rsquo;t even have a phone (let alone dial-up internet) at home in Smolensk. So, I studied the topic through medical websites. To the credit of Russian medicine, they were leading the way in informatization back then, with a lot of popular material published online. I managed to get a handle on things, even learning about Mountcastle and the columnar organization of the cortex, which didn’t become widely discussed in popular science until 10 years later (much to my surprise).\u003c/p\u003e\n\u003cp\u003eAfter digging into the brain, I realized that I understood nothing. And it wasn’t just me \u0026ndash; no one really understood it. Otherwise, they would have written about it. So, I tried approaching it from the \u0026ldquo;psychological\u0026rdquo; side. This experience turned out to be much more productive, though far less straightforward than what we expect from reading programming tutorials. I made a ton of mistakes and had to learn from them. But in the end, I stumbled upon the beginnings of \u003ca href="http://www.scholarpedia.org/article/Algorithmic_information_theory"\u003eAlgorithmic Information Theory\u003c/a\u003e (AIT), which in Europe and Russia is more commonly known as Kolmogorov Complexity Theory. I call it AIT because the abbreviation is more widely recognized. I started googling and found out that everything had already been invented before me. There were even \u003ca href="https://people.idsia.ch/~juergen/artificial-curiosity-since-1990.html"\u003emathematical theories\u003c/a\u003e for some fundamental higher cognitive processes. That’s when Schmidhuber became my idol \u0026ndash; not as a person (I don\u0026rsquo;t know him personally), but because he did \u0026ldquo;this\u0026rdquo; long before I did. Whether I like it or not, I continue and expand on his work.\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;Discovering Schmidhuber\u0026rdquo; was a huge relief for me, as it meant I could stop my research and focus on practical work, which, around 2010, was in short supply in AI. Especially in methods derived from AIT (not much has changed since then, but we’ll work on that). Later on, I realized that stopping my research program was a mistake, and I’ve somewhat resumed it. But more on that later.\u003c/p\u003e\n\u003cp\u003eThat’s how the practical ideas that eventually led to the creation of Memoria came about.\u003c/p\u003e\n\u003cp\u003eSchmidhuber\u0026rsquo;s ACC is simply genius, though the reader will have to take my word for it. Besides its brilliance, it\u0026rsquo;s the best private theory of emotions currently available. Like almost everything in AIT, it\u0026rsquo;s as practically useless in its direct form as it is mathematically universal. And it’s absolutely universal. That is, any \u0026ldquo;universal AI\u0026rdquo; would inevitably operate on this principle.\u003c/p\u003e\n\u003cp\u003eAIT methods are theoretically well-suited to approximation (estimating an object\u0026rsquo;s Kolmogorov complexity through its compression), including ACC, and to make it applicable in psychology, such an approximation needs to be carried out. The brain is far from a \u0026ldquo;universal AI.\u0026rdquo; It can perform some compression, but that’s about it. Moreover, various evolutionary mechanisms layer on top of this, resulting in phenomena like \u0026ldquo;interest\u0026rdquo; acting as an ensemble or mixture of distinct and diverse mechanisms. This creates the main set of challenges in psychology, as higher cognitive functions are complex ensembles of low-level ones, which are relatively easy to register and study. But the structure of this ensemble is not necessarily a weighted sum. It could be some arbitrary algorithm (*). And this is also known as the \u0026ldquo;mind-body problem.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e(*) You could say that \u0026ldquo;any arbitrary algorithm\u0026rdquo; can be approximated by a weighted sum, which is precisely what neural networks do. And you\u0026rsquo;d be formally correct. The question is, what’s easier for a person to do mentally \u0026ndash; discover the actual ensemble algorithm or derive the weighted sum coefficients? The brain doesn’t do backpropagation.\u003c/p\u003e\n\u003cp\u003eWhen we try to \u0026ldquo;understand ourselves\u0026rdquo; or \u0026ldquo;understand how our mind works,\u0026rdquo; we’re trying to solve the mind-body problem \u0026ndash; reducing subjective experiences to some objective basis. That basis could be the brain, if we knew how it worked. Of course, I’m oversimplifying; we know a lot, but not everything yet. My point is that reducing experiences to brain activity is still an unstable basis \u0026ndash; it may change over time.\u003c/p\u003e\n\u003cp\u003eDigital computation, in this sense, is a much simpler and more stable basis than neural tissue activity. While it may seem scientifically absurd to reduce human experiences to the basis of a silicon electronic-digital machine, considering that our brain is nothing like that, this approach exists and is called Computationalism.\u003c/p\u003e\n\u003cp\u003eThe interesting point here is that the success of LLMs in simulating human-likeness (which I\u0026rsquo;ll define below) has greatly boosted the status of Computationalism as a practical philosophy and psychology of AI. So, as they say, the topic has taken an unexpected turn.\u003c/p\u003e\n\u003cp\u003eIn this context, Schmidhuber\u0026rsquo;s ACC is a piece of the puzzle for solving the mind-body problem within the framework of Computationalism. And a very high-quality piece. Its second advantage (besides mathematical universality) is its extensibility \u0026ndash; the ability to fit other experiences into the same mathematical framework. For example, \u003ca href="https://en.wikipedia.org/wiki/Simplicity_theory"\u003eSimplicity Theory\u003c/a\u003e, which for some reason has apparently stalled in its development. There are ideas as to why, but we’ll get to that later.\u003c/p\u003e\n\u003cp\u003eThis is the direction I’m working in (my personal research) in AI. I have a notebook associated with the Memoria project \u003ca href="https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md"\u003ehere\u003c/a\u003e, where I occasionally jot down ideas within Computationalism that have crystallized after discussions in chats. I formalize higher cognitive functions (HCF) as they appear to an informed (*) subject and try to identify a minimal basis for them, which can then be supported by a computational platform (algorithms and data structures). HCFs (at the experience level) will be obtained later through the composition of basic functions and data.\u003c/p\u003e\n\u003cp\u003eFor example, the Observer problem is defined there as a reformulation of the qualia problem from the philosophy of mind, and a computational basis is proposed for the Observer \u0026ndash; the so-called self-applicable Turing machine, and for experiences \u0026ndash; \u0026ldquo;higher-order computational phenomena\u0026rdquo; occurring in such a machine. There’s no esotericism here. Higher-order computational phenomena are the program\u0026rsquo;s reflection on its runtime and state. For example, on its execution time (\u0026ldquo;I’ve been thinking for a long/short time\u0026rdquo;). Some of these phenomena can have stable, consistent manifestations, and for this reason, they can be elements of the self-applicable machine\u0026rsquo;s internal language. The idea is that, for example, \u0026ldquo;free will\u0026rdquo; is one of these phenomena, reduced to the machine’s observation of its inability to see all the determinants of its own decisions. \u0026ldquo;Free will\u0026rdquo; is needed for a very practically important function \u0026ndash; agency, on which the ability of a computational system to enter relationships within the framework of human agency depends.\u003c/p\u003e\n\u003cp\u003e(*) The ability to describe higher cognitive functions in humans directly depends on their level of intrapersonal intelligence (Intrapersonal Intelligence or I.I.). In general, this is the ability to analyze and describe one\u0026rsquo;s mental states. As with all other types of intelligence, people with low I.I. are unaware of this. The Dunning-Kruger effects fully apply here. High I.I. isn’t necessary for everyday life. People who don’t develop I.I. will likely have low I.I. (with some nuances). There are people with a genetically determined abnormally high level of I.I. Such individuals are of particular interest for human-like AI, as they are capable of \u0026ldquo;debugging\u0026rdquo; it.\u003c/p\u003e\n\u003cp\u003eSelf-applicability and related phenomena are already actively at work. For example, server systems use logs, storing part of their state in them, and these logs are then analyzed, based on which decisions are made that affect the system\u0026rsquo;s functioning. This is already full-fledged self-applicability, just not deep or high-level enough yet. But there are no technical barriers to achieving this.\u003c/p\u003e\n\u003cp\u003eWhat I called a \u0026ldquo;self-applicable Turing machine\u0026rdquo; doesn’t necessarily have to be a Turing machine. In Memoria, the results of searching for and forming a formal basis for HCFs (like the Observer) will be implemented at the level of the basic type system. This hasn’t been published yet, but there’s a lot of room for persistent/functional data structures (which are a generalization of the software logs mentioned above), basic computation forms (see below), and even special hardware functions (e.g., hardware support for software reflection and introspection).\u003c/p\u003e\n\u003cp\u003eA Forward-chaining Rule System (FCRS) based on variants of the RETE algorithm, exemplified by \u003ca href="https://www.drools.org/"\u003eDrools\u003c/a\u003e and \u003ca href="https://www.clipsrules.net/"\u003eCLIPS\u003c/a\u003e, surprisingly fits well as a \u0026ldquo;self-applicable Turing machine\u0026rdquo; since it inherently operates on this principle, and nothing substantial needs to be added. For example, \u0026ldquo;I’ve been working for a long time\u0026rdquo; is just another event the system can respond to in its usual way, like all other events.\u003c/p\u003e\n\u003cp\u003eFCRS (along with backward chaining RS and the familiar control-flow (CF)) will be fully supported at the DSLEngine level and, eventually, at the MAA level. RETE is very well accelerated at various levels. Beta nodes are simply the Cartesian product of sets, have quadratic time complexity, and therefore fit well with systolic computational architectures. RETE also hybridizes well at both the alpha and beta memory levels. You can create fuzzy and probabilistic extensions of the basic algorithm, as well as hybridize with neural networks, which are essentially also from the FCRS class.\u003c/p\u003e\n\u003cp\u003eA renewed interest in FCRS might arise from multi-agent systems based on LLMs, linked to replacing the CF controller (Python) with an FCRS controller and tools. FCRS is much better suited for real-time event processing than CF and can significantly simplify the overall system design. Although CF doesn’t surpass FCRS in terms of expressiveness.\u003c/p\u003e\n\u003cp\u003eReturning to self-applicable computations, qualia, and agency, implementing their functional analogs at the DSLEngine level won’t automatically give it \u0026ldquo;human consciousness.\u0026rdquo; That won’t happen at all, because DSLEngine itself contains too little data (it’s just a sophisticated rule engine) for behavior of human-level complexity to emerge. And there’s nowhere for that data to come from, at least not quickly. The agency that can be manually programmed at the DSLEngine level can be characterized as \u0026ldquo;micro-agency,\u0026rdquo; though this term should be carefully defined, as humans, surprisingly, also exhibit micro-agency.\u003c/p\u003e\n\u003cp\u003eIt remains an open question whether micro-agency will have any serious practical applications. Its presence in the system may ensure that the agent system passes corresponding tests within the functionalist approach to agency. That is, it’s simply recognized that, for example, within its micro-scale, it has a functional equivalent of free will and can therefore bear a functional analog of responsibility. But how complex such behavior will be is a separate question. We don’t recognize human-level agency in an ant, and, accordingly, we don’t see it as a legal entity within human society. The same applies to such micro-agency.\u003c/p\u003e\n\u003cp\u003eHowever, there are at least two indirect applications.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFirst\u003c/strong\u003e \u0026ndash; it’s a tool for developing intrapersonal intelligence within the framework of the computationalist paradigm. The thing is, our ability to understand ourselves directly depends on our ability to express the structure of mental states and the relationships between them in language. Both the states themselves and (especially) the relationships between them can be very complex, far beyond the familiar symbolic structures (tables, trees, and graphs). We need tasks and tools that would motivate such activity and support progress toward corresponding goals. These tasks are AI, and the tool is Memoria. Memoria places a clear emphasis on advanced data structures and fairly complex algorithms, which are expected to create a foundation for the radical development of I.I.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSecond\u003c/strong\u003e \u0026ndash; LLMs spontaneously develop self-applicability. A large corpus of texts describing their properties forms, meaning they develop a self-model. Or rather, elements of it. For now, just elements, which are still insufficient for full-fledged micro-agency. But the latter may emerge simply because the model’s memory structure suddenly becomes sufficient for it, and the learning algorithms will do the rest (derive the necessary structures for attention, for instance). This might happen simply because the developers themselves have low I.I. due to the predominance of the \u0026ldquo;black box\u0026rdquo; approach in their environment. Few are even interested in psychology, let alone theories of mind.\u003c/p\u003e\n\u003cp\u003eHere’s what needs to be said. \u0026ldquo;Functional consciousness\u0026rdquo; (and functional agency) will belong to the \u0026ldquo;weak\u0026rdquo; category according to Searle. That is, it’s simply a functional imitation of the corresponding HCFs of humans. However, the open question remains: how will the collective human mind react to the emergence of full-fledged functional agency in AI? We are NOT READY FOR THE CONSEQUENCES.\u003c/p\u003e\n\u003cp\u003eHowever, the genie is already out of the bottle, and the process of developing AI toward self-applicability can’t be stopped, even if LLM training is completely banned worldwide. A human-like AI, albeit one that stumbles over its own feet, has already been created. And it’s already interacting with us in ways we don’t see due to the weakness and unpreparedness of our intrapersonal intelligence for such a sudden turn.\u003c/p\u003e\n\u003cp\u003eI believe the best strategy is to try to lead what cannot be prevented. If I previously kept the priority of practical work on functional consciousness low, now it’s time to reconsider it to be prepared for what’s coming. Memoria will help develop I.I., which in turn will help us see what exactly we need to prepare for.\u003c/p\u003e\n\u003ch2 id="memoria-and-ait"\u003eMemoria and AIT\u003c/h2\u003e\n\u003cp\u003eSo, let’s put I.I. aside for now. Now, let\u0026rsquo;s talk about AIT and Memoria.\u003c/p\u003e\n\u003cp\u003eAIT is fascinating because it proposes extremely generalized models of intelligence \u0026ndash; Solomonoff Induction, AIXI, Goedel Machine, and their many variants. These models are directly or indirectly based on the idea that intelligence (in a broad mathematical sense) can be reduced to predicting strings (in the case of AIT, binary strings, but it doesn’t have to be). For a given predictor P(s), the more accurately it predicts the next symbol s (the closer the empirical distribution is to the theoretical one), the more intelligent it is. This setup might seem very abstract, but in the example of autoregressive LLMs, we can see how such a low-level model of intelligence can unfold into a full-fledged human-like intelligence, interacting with us through natural language.\u003c/p\u003e\n\u003cp\u003eAt this point, it’s worth defining what \u0026ldquo;human-like AI\u0026rdquo; means in terms of the predictive task. Just like the predictor P(s), the human brain is capable of predicting the parameters of incoming signals. This may be very implicit, but let’s accept it as a given. Just as a predictor assesses the probabilities of its strings, so does the brain evaluate the probabilities of its signals. A human-like predictor P(s) would assign probabilities close to those assigned by the human brain. In this case, the high-level part of the model’s intelligence would be perceived by humans as natural, intuitive, and almost as if they were interacting with another human.\u003c/p\u003e\n\u003cp\u003eIt’s important to note that a human-like predictor P(s) doesn’t have to assign true probabilities to strings. If it doesn’t, it would just make the same mistakes humans typically make. And from the human perspective, it would still seem human-like. But when the predictor makes different mistakes than humans typically do, it leads to human frustration (the reverse is also true, but models don’t yet know how to experience frustration).\u003c/p\u003e\n\u003cp\u003eAn interesting point is that training LLMs on text datasets seems to create probability estimates for strings that are very close to those produced by the brain for corresponding signals, despite the significant difference in inductive biases. This phenomenon still awaits its researcher.\u003c/p\u003e\n\u003cp\u003eThe main tenet of AIT regarding string prediction is that the better a predictor P(s) can compress a string s, the better it can predict it. Therefore, prediction boils down to compression, and so does intelligence. For neural networks (and other models obtained through machine learning), compression corresponds to the model’s \u0026ldquo;generalization of its training set,\u0026rdquo; meaning it correctly classifies (predicts) points belonging to the same class as those in the training set. Compression here means that the amount of information about the classes contained in the model is significantly less than the amount of information that would be required for all points in all classes. In other words, the model is a compressed description of the corresponding object.\u003c/p\u003e\n\u003cp\u003eExtracting some intelligence directly from compression might be technically challenging. However, it can be \u003ca href="https://arxiv.org/abs/cs/0312044"\u003edone\u003c/a\u003e. This also implies that direct hardware acceleration of data compression algorithms makes sense in the context of AI accelerators.\u003c/p\u003e\n\u003cp\u003eBut there are indirect techniques as well. \u003ca href="https://en.wikipedia.org/wiki/AIXI"\u003eAIXI\u003c/a\u003e is a \u003cem\u003euniversal\u003c/em\u003e Bayesian agent based on Algorithmic Probability and the Universal Prior M(x), which is interesting because it already contains all the models of the environment, and AIXI simply \u0026ldquo;pulls out\u0026rdquo; these models through Bayes’ rule and interacts with the environment based on them.\u003c/p\u003e\n\u003cp\u003eM(s) is non-computable, but resource-bounded approximations are possible. One such approximation of AIXI in the domain of Variable-Order Markov Models (VMM) is \u003ca href="https://arxiv.org/abs/0909.0801"\u003eMC-AIXI-CTW\u003c/a\u003e. In this model, M(x) is approximated using a probabilistic data structure (here: a structure encoding probability distributions) of a suffix tree for VMM and a method of approximating the CTW stock, which the tree lacks. Such an agent can learn to play board games and even beat other agents.\u003c/p\u003e\n\u003cp\u003eLLMs work in a very similar way, but there’s an important nuance that might help us better understand LLMs and why they work. MC-AIXI-CTW is also based on a probabilistic model, but of the environment rather than natural language. But this isn’t a fundamental difference. In both cases, there are strings (texts) the model has seen during training and those it hasn’t seen, but for both types of strings, it must provide a probability estimate close to the true one.\u003c/p\u003e\n\u003cp\u003eIn the case of MC-AIXI-CTW, there is a clear separation between the \u0026ldquo;database\u0026rdquo; (suffix tree) and the mechanism for deducing probabilities of new strings based on CTW. Such databases were once called deductive because they could deduce new knowledge from existing data.\u003c/p\u003e\n\u003cp\u003eIn the case of a transformer-based LLM (neural network), there’s no clear separation between the \u0026ldquo;database\u0026rdquo; and the \u0026ldquo;deducer.\u0026rdquo; It’s just a function approximator that does the computation. However, such a separation can be implied. There is a part that is remembered from the dataset (database/knowledge base) and another part that comes from the transformer’s ability to \u0026ldquo;generalize.\u0026rdquo; I think it’s unnecessary to prove that the larger the remembered part, the less the need for generalization, and vice versa.\u003c/p\u003e\n\u003cp\u003eNow, considering that the Kolmogorov complexity of a classifier cannot be less than the complexity of the boundary between classes and the complexity of a generator cannot be less than the complexity of the generated object, it should become clearer why leading LLMs are so large: they don’t generalize very well. More on this below.\u003c/p\u003e\n\u003cp\u003eAs doctors say, MC-AIXI-CTW remains \u0026ldquo;underexplored\u0026rdquo; in history, meaning under-researched, because everyone \u0026ldquo;went to the front\u0026rdquo; \u0026ndash; pairing deep learning with reinforcement learning. We also thought back then that CTW didn’t approximate the probabilities of new, previously unseen strings very well. And, as noted above, there’s suspicion that transformers don’t generalize them very well either. But to test this, we’d need not only to create a truly massive database for MC-AIXI-CTW’s suffix tree but also an efficient mechanism for computing on these data structures.\u003c/p\u003e\n\u003cp\u003eIn short, MC-AIXI-CTW and its analogs and extensions seem like interesting test tasks or benchmarks for MAA within the framework of AI. The advantages of this agent include that it’s fully dynamic and interactive. It learns directly during its operation, unlike LLMs, which are trained asynchronously with a rather long update cycle. Moreover, a suffix tree is perfectly interpretable.\u003c/p\u003e\n\u003cp\u003eWhen priorities allow, a full-scale \u0026ldquo;large\u0026rdquo; implementation of MC-AIXI, designed for scalability, will be made in Memoria. This practical platform will also kickstart more active practical research into methods derived from AIT.\u003c/p\u003e\n\u003cp\u003eA separate sub-direction of AIT and its approach to AI is compressed data structures (CDS). For containers, a compressed container is one whose size (in bytes) is proportional not to the number of elements it contains but to the amount of information in those elements. For example, compressed data structures based on Huffman codes or Universal codes are often popular. Data structures that use them usually have a physical size proportional to the zero-order entropy H0.\u003c/p\u003e\n\u003cp\u003eCDS also have intrinsic value. For example, the suffix tree in MC-AIXI-CTW can be very large and \u0026ldquo;branched.\u0026rdquo; Representing it as a list structure in memory creates enormous redundancy, while there are ways to encode ordinary trees with a spatial complexity of 2 bits per node or less, with quite good dynamic characteristics.\u003c/p\u003e\n\u003cp\u003eThe structural primitives on which CDS rely can be quite computationally intensive; processors will spend cycles on encoding-decoding, which may impact overall performance. However, a simple cache miss when accessing memory leads to the processor waiting tens of nanoseconds and hundreds of cycles before the data arrives. This time can be well spent on encoding-decoding if it reduces the physical volume of data and thus improves caching efficiency.\u003c/p\u003e\n\u003cp\u003eMoreover, encoding processing can also be done in hardware. We now have more than enough silicon for this. Due to their computational intensity, CDS were once considered impractical, but since the early 2000s, with the proliferation of superscalar OoOE processors, the situation began to change. Memoria started in the late 2000s with Ulrich Drepper\u0026rsquo;s \u003ca href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf"\u003epaper\u003c/a\u003e \u0026ldquo;What Every Programmer Should Know About Memory,\u0026rdquo; which discusses how to properly arrange data in memory to improve computational performance. Improper data placement can reduce performance by 10-100 times.\u003c/p\u003e\n\u003cp\u003eIn other words, CDS are useful in conventional engineering without any \u0026ldquo;intelligence\u0026rdquo; involved. \u003ca href="https://memoria-framework.dev/docs/data-zoo/hierarchical-containers"\u003eHere\u003c/a\u003e, it’s shown how to use compressed character sequences and a B+tree sledgehammer to create \u0026ldquo;hierarchical containers\u0026rdquo; \u0026ndash; prototypes of all \u0026ldquo;nested\u0026rdquo; containers in Memoria, from Map\u0026lt;K, Vector\u003cV\u003e\u0026gt; to tables and beyond, as far as imagination allows. And here, I explain how compression can help fight the curse of dimensionality in spatial trees and even slightly overcome it.\u003c/p\u003e\n\u003cp\u003eAIT and mathematics will allow us to go much further in this direction, and Memoria will be the necessary technical platform for implementing these advancements.\u003c/p\u003e\n\u003ch2 id="memoria-and-llm"\u003eMemoria and LLM\u003c/h2\u003e\n\u003cp\u003eI’m planning to actively explore the possibilities of LLMs for automatic programming. Initially, there was a lot of enthusiasm in the programming community about this, along with fears of losing professional relevance. But over time, that enthusiasm shifted to skepticism (and programmers collectively \u0026ldquo;breathed a sigh of relief\u0026rdquo;).\u003c/p\u003e\n\u003cp\u003eLLMs can be used for code generation, but not in a fully automated mode. That is, only in situations where a human can monitor the result using various tools. One such area, especially relevant for OSS, is code consolidation. There are many projects full of good, valuable code, but interest in them has faded. LLMs can be used to help rewrite this code (either entirely or in parts) to fit the Memoria environment.\u003c/p\u003e\n\u003cp\u003eAnother direction is \u0026ldquo;Memoria as a dataset.\u0026rdquo; The idea is simple: structure the project and enrich the code with metadata. Then, use all of this as part of the training corpus for models (since it’s OSS). After that, the models trained in this way can be used to improve the next iteration of the project itself. And so on\u0026hellip;\u003c/p\u003e\n\u003cp\u003eThis is something to think about, and perhaps OSS could get a significant boost from this approach.\u003c/p\u003e\n\u003cp\u003eOne specific direction is probabilistic data structure generation, but this leans more towards probabilistic programming than LLMs. Though LLMs might also prove useful here.\u003c/p\u003e\n'}).add({id:14,href:"/docs/data-zoo/overview/",title:"Core Data Structures -- Overview",description:"",content:"\u003cp\u003eThis sections contans detailed description of some core data structures Memoria\u0026rsquo;s containers are based on.\u003c/p\u003e\n"}).add({id:15,href:"/docs/data-zoo/partial-sum-tree/",title:"Partial Sums Tree",description:"",content:'\u003ch2 id="description"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s take a sequence of monotonically increasing numbers, and get delta sequence from it, as it is shown on the following figure. Partial Sum Tree is a tree of sums over this delta sequence.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="trees.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003ePacked tree in Memoria is a multiary balanced tree mapped to an array. For instance in a level order as it shown on the figure.\u003c/p\u003e\n\u003cp\u003eGiven a sequence of N numbers with monotonically increasing values, partial sum tree provides several important operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efindLT(value)\u003c/code\u003e finds position of maximal element less than \u003ccode\u003evalue\u003c/code\u003e, time complexity $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efindLE(value)\u003c/code\u003e finds position of maximal element less than or equals to \u003ccode\u003evalue\u003c/code\u003e, $T = O(log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esum(to)\u003c/code\u003e computes plain sum of values in the delta sequence in range [0, to), $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eadd(from, value)\u003c/code\u003e adds \u003ccode\u003evalue\u003c/code\u003e to all elements of original sequence in the range of [from, N), $T = O(log(N))$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is obvious that first two operations can be computed with binary search without partial sum tree and all that overhead it introduces.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003ePartial or prefix sum trees of higher degrees are especially suitable for hardware acceleration. DDR and HBM memory transfer data in batches and works best if data is processed also in batches. The idea here is to offload tree traversal operation from CPU to the memory controller or even to DRAM memory chips. Sum and compare operations are relatively cheap to implement, and no complex control is required.\u003c/p\u003e\n\u003cp\u003eBy offloading tree traversal to the memory controller (that usually works in front of caches), we can save precious cache space for more important data. By offloading summing and comparison to DRAM chips, we can better exploit internal memory parallelism and save memory bandwidth. In such distributed architecture, a single tree level scan can be performed with the latency and in the power budget of a \u003cem\u003esingle random memory access\u003c/em\u003e, saving energy and silicon for other computations.\u003c/p\u003e\n'}).add({id:16,href:"/docs/data-zoo/searchable-seq/",title:"Searchable Sequence",description:"",content:'\u003cp\u003eSearchable sequence or rank/select dictionary is a sequence of symbols that supports two operations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003erank(position, symbol)\u003c/code\u003e is number of occurrences of \u003ccode\u003esymbol\u003c/code\u003e in the sequence in the range [0, position)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect(rank, symbol)\u003c/code\u003e is a position of \u003ccode\u003erank\u003c/code\u003e-th occurrence of the \u003ccode\u003esymbol\u003c/code\u003e in the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsually the alphabet is {0, 1} because of its practical importance, but larger alphabets are of significant interest too. Especially in Bioinformatics and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eThere are many implementations of binary searchable sequences (bitmaps) providing fast query operations with $O(1)$ time complexity. Memoria uses partial sum indexes to speedup rank/select queries. They are asymptotically slower than other methods but have additional space overhead for the index.\u003c/p\u003e\n\u003cp\u003ePacked searchable sequence is a searchable sequences that has all its data structured packed into a single contiguous memory block with packed allocator. It consists from two data structures:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003emulti-index partial sum tree to speedup rank/select queries;\u003c/li\u003e\n\u003cli\u003earray of sequence\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for the case of searchable bitmap.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_seq.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that for a sequence with K-symbol alphabet, packed sum tree has K indexes, that results in significant overhead even for relatively small alphabets. For example 8-bit sequence has 256-index packed tree that takes more than 200% of raw data size if the tree is not compressed. To lower this overhead Memoria provides various compressed encodings for the index\u0026rsquo;s values.\u003c/p\u003e\n\u003ch2 id="creation-and-access"\u003eCreation and Access\u003c/h2\u003e\n\u003cp\u003eTo create partial sum tree for a sequence we first need to split it logically into blocks of fixed number of symbols (16 at the figure). Then sum different symbols in the block, each such vector is a simple partial sum tree leaf. Build other levels of the tree accordingly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSymbol update is relatively fast, it takes $O(log(N))$ time.\u003c/li\u003e\n\u003cli\u003eSymbol insertion is $O(N)$, it requires full rebuilding of partial sum tree.\u003c/li\u003e\n\u003cli\u003eSymbol access does not require the tree to perform, it takes $O(1)$ time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003erank(position, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven \u003ccode\u003eposition\u003c/code\u003e, determine sequence \u003ccode\u003eblock_number\u003c/code\u003e for that position, and \u003ccode\u003eblock_pos\u003c/code\u003e position in the block, $O(1)$;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eblock_rank\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) in the sum tree, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003ecount number of \u003ccode\u003esymbol\u003c/code\u003es in the block to \u003ccode\u003eblock_pos\u003c/code\u003e, $O(1)$;\u003c/li\u003e\n\u003cli\u003efinal rank is (2) + (3).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003eselect(rank, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven partial sum tree and a \u003ccode\u003esymbol\u003c/code\u003e, determine target sequence \u003ccode\u003eblock_number\u003c/code\u003e having \u003ccode\u003etotal_rank \u0026lt;= rank\u003c/code\u003e for the given \u003ccode\u003esymbol\u003c/code\u003e using \u003ca href="/docs/data-zoo/partial-sum-tree"\u003efindLE\u003c/a\u003e operation, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eFor the given block, compute \u003ccode\u003erank_prefix\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) for the given \u003ccode\u003esymbol\u003c/code\u003e, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eCompute \u003ccode\u003elocal_rank = rank - rank_prefix\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003eScan the block and find position of \u003ccode\u003esymbol\u003c/code\u003e having rank in the block = \u003ccode\u003elocal_rank\u003c/code\u003e, $O(1)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eActual implementation joins operations (1) and (2) into a single traverse of the sum tree.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eConsideration are the same as for \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial/prefix sum trees\u003c/a\u003e, especially, because searchable sequence contains partial sum tree as an indexing structure.\u003c/p\u003e\n\u003cp\u003eModern CPUs usually have direct implementations for rank (PopCount) for binary alphabets. Select operation may also be partially supported. To accelerate searchable sequences, it\u0026rsquo;s necessary to implement rang/select over arbitrary alphabets (1-8 bits per symbol).\u003c/p\u003e\n\u003cp\u003eSymbol blocks are also contiguous in memory and can be multiple of DRAM memory blocks. Rank/select machinery is simpler or comparable with machinery for addition and subtraction. Those operations can be efficiently implemented in a small silicon budget and at high frequency.\u003c/p\u003e\n'}).add({id:17,href:"/docs/data-zoo/compressed-symbol-seq/",title:"Compressed Symbol Sequence",description:"",content:'\u003cp\u003eCompressed symbol sequence takes space proportional to the \u003cem\u003einformation\u003c/em\u003e contained in it, not to the number of symbols at the first place. It\u0026rsquo;s achievable via using special encoding and Memoria is using the simplest one \u0026ndash; run-length encoding (RLE). The basic idea is simple: symbol sequence is represented as a \u003cem\u003eseries of short repeatable patterns\u003c/em\u003e. The main challenge is how to encode parameters succinctly in the way that minimizes overhead for poorly compressible sequences.\u003c/p\u003e\n\u003cp\u003eCurrently, the main \u003cem\u003efamily\u003c/em\u003e of compressible sequence encoding in Memoria is called SSRLE or Streaming Symbol RLE. The \u0026ldquo;streaming\u0026rdquo; in the name reflects the \u003cem\u003eintention\u003c/em\u003e to optimize it for streaming applications and hardware acceleration, but otherwise means nothing.\u003c/p\u003e\n\u003cp\u003eIn SSRLE the smallest unit of sequence is \u0026ldquo;Run\u0026rdquo; that is pair of \u003cem\u003epattern\u003c/em\u003e and its positive \u003cem\u003elength\u003c/em\u003e. The \u003cem\u003epattern\u003c/em\u003e also has positive length. Memory-wise, the smallest element of the sequence is \u003cem\u003eCodeUnit\u003c/em\u003e, that is currently 2 bytes (16 bits). So the smallest sequence size is 2 bytes. A \u003cem\u003eCodeWord\u003c/em\u003e is a sequence of \u003cem\u003eCodeUnit\u003c/em\u003es of the size from 1 to 4. A Run has to fit into a single CodeWord, that is currently from 16 to 64 bits. A \u003cem\u003eSegment\u003c/em\u003e is a series of \u003cem\u003eCodeWord\u003c/em\u003es up to 32 \u003cem\u003eCodeUnit\u003c/em\u003es (64 bytes). \u003cem\u003eCodeWord\u003c/em\u003e may not cross the boundary of segment. In case of a partially-filled segment, special CodeWord with zero pattern length may be used to denote \u0026ldquo;padding\u0026rdquo;. If SSRLE is hardware accelerated, Segment may be a unit of processing.\u003c/p\u003e\n\u003cp\u003eSpecific values for the set above define specific encoding from the \u003cem\u003efamily\u003c/em\u003e of SSRLE encodings.\u003c/p\u003e\n\u003cp\u003e\u003cfigure class="figure"\u003e\r\n    \u003cimg class="figure-img img-fluid lazyload blur-up" data-sizes="auto" src="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_20x0_resize_box_3.png" data-srcset="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_900x0_resize_box_3.png 900w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_800x0_resize_box_3.png 800w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_700x0_resize_box_3.png 700w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_600x0_resize_box_3.png 600w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_500x0_resize_box_3.png 500w" width="2750" height="563" alt="SSRLE Run"\u003e\r\n    \u003cnoscript\u003e\u003cimg class="figure-img img-fluid" sizes="100vw" srcset="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_900x0_resize_box_3.png 900w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_800x0_resize_box_3.png 800w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_700x0_resize_box_3.png 700w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_600x0_resize_box_3.png 600w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_500x0_resize_box_3.png 500w" src="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run.png" width="2750" height="563" alt="SSRLE Run"\u003e\u003c/noscript\u003e\r\n    \u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003eThere are four field in a SSRLE run encoding:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCodeWord length\u003c/strong\u003e. Number code units representing CodeWord. Fixed 2 bits for all symbol types, up to 4 code units in a code word.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePattern Length\u003c/strong\u003e (PL). Number of symbols in the pattern.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymbol Pattern\u003c/strong\u003e. Sequence of symbols.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Length\u003c/strong\u003e (RL). Length of the Run \u003cem\u003ein patterns\u003c/em\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSSRLE supports multiple symbol sizes (number of bits per symbol). In Memoria SSRLE is defined for symbol alphabets from 1 to 8 bits per symbol (\u003cem\u003eBPS\u003c/em\u003e).\u003c/p\u003e\n\u003cp\u003eFor BPS = 1 we have a compressed bitmap (cor compressed bit vector). In this case (code word is up to 64 bits), we need at most 6 bits to represent possible pattern lengths. The longest pattern then is \u003ccode\u003e64-(6 + 2) = 56\u003c/code\u003e bits. So for each 7 bytes of uncomressed bitmap we have 1 byte of \u003cem\u003ememory overhead\u003c/em\u003e, that is about 14%. SSRLE has rather large (yet acceptable) overhead, comparing to other compressed bitmap encodings. The point is that, unlike other representations, SSRLE supports repeatable patterns, not just long runs of \u003ccode\u003e0...\u003c/code\u003es and \u003ccode\u003e1...\u003c/code\u003es.\u003c/p\u003e\n\u003cp\u003eFor larger BPSs we will have different possible values of Pattern Length and Run Length, because symbols are wider in bits and patterns are shorter. The following table summarizes all available variants from different BPPs:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eBPS\u003c/th\u003e\n\u003cth\u003ePL in Bits\u003c/th\u003e\n\u003cth\u003eMax Pattern Length (in Symbols)\u003c/th\u003e\n\u003cth\u003eMax RL in bits\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e56\u003c/td\u003e\n\u003ctd\u003e55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e28\u003c/td\u003e\n\u003ctd\u003e55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e18\u003c/td\u003e\n\u003ctd\u003e54\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e14\u003c/td\u003e\n\u003ctd\u003e54\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e11\u003c/td\u003e\n\u003ctd\u003e53\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e9\u003c/td\u003e\n\u003ctd\u003e52\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e52\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e51\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAuthoritative source for information about actual SSRLE parameters are \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/include/memoria/core/ssrle"\u003esources\u003c/a\u003e.\u003c/p\u003e\n'}).add({id:18,href:"/docs/data-zoo/hierarchical-containers/",title:"Hierarchical Containers",description:"",content:'\u003cp\u003eIn this page it\u0026rsquo;s explained how various hierarchical containers (vectors with large values, multimaps, tables, wide tables, etc) are organized inside and how \u003ca href="/docs/data-zoo/searchable-seq/"\u003esearchable sequences\u003c/a\u003e make it possible.\u003c/p\u003e\n\u003ch2 id="multimap"\u003eMultimap\u003c/h2\u003e\n\u003cp\u003eLet, for simplicity, we have a multimap \u003ccode\u003estd::map\u0026lt;int64_t, std::vector\u0026lt;uint8_t\u0026gt;\u0026gt;\u003c/code\u003e (we call it a \u003cem\u003eMultimap\u003c/em\u003e here) that is internally a binary search tree (RB-/AVL-tree etc) with vectors as values. Vector\u003cT\u003e is internally a contiguous data structure but the search tree is basically a tree-shaped linked list or \u003cem\u003epointer chasing\u003c/em\u003e data structure. This data layout works perfectly for the \u003cem\u003eword\u003c/em\u003e-addressed main memory, but if we want to place out data structure into \u003cem\u003eblock\u003c/em\u003e-addressed external memory, the whole thing gets trickier, because the using of allocation now is pretty large comparing to RAM: 4KB and greater. In the external memory we can still combine Map (represented as B+Tree) with Dynamic Vector (represented as B+Tree) the same way how two those containers \u003ccode\u003estd::map\u0026lt;\u0026gt;\u003c/code\u003e and \u003ccode\u003estd::vector\u0026lt;\u0026gt;\u003c/code\u003e are combined with each other (via references) and get either a set of B+Trees or one \u0026lsquo;hierarchical\u0026rsquo; B+Tree, but the minimal size of value vector will be \u003cem\u003eone block\u003c/em\u003e that is pretty large. Practical applications can optimize this specific case using various techniques like placing short vectors right inside the parent container\u0026rsquo;s (Map in this case) leaf block, and only spilling a new B+Tree when there is no more room for that in the block. It works pretty well in practice, but we can do much better. Below it\u0026rsquo;s explained how we can use searchable sequences for fitting arbitrary-shaped hierarchical containers into a single (or \u0026lsquo;flat\u0026rsquo;) B+Tree.\u003c/p\u003e\n\u003ch3 id="data-structure"\u003eData structure\u003c/h3\u003e\n\u003cp\u003eLet for certainty we have the following Multimap: \u003ccode\u003e{1=[1,5,2,4], 4=[7,3,1], 6=[5,9,1,2,0], 7=[8,1]}\u003c/code\u003e. We represent is with \u003cem\u003ethree\u003c/em\u003e arrays: keys \u003ccode\u003eK[]\u003c/code\u003e, values \u003ccode\u003eV[]\u003c/code\u003e and \u003ca href="/docs/data-zoo/searchable-seq/"\u003esearchable symbol sequence\u003c/a\u003e \u003ccode\u003eS[]\u003c/code\u003e. \u003ccode\u003eSrle[]\u003c/code\u003e is an \u003ca href="/docs/data-zoo/compressed-symbol-seq/"\u003eRLE encoding\u003c/a\u003e for \u003ccode\u003eS[]\u003c/code\u003e (we need one \u003ccode\u003eS[]\u003c/code\u003e or \u003ccode\u003eSrle[]\u003c/code\u003e, \u003cem\u003enot\u003c/em\u003e both):\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multimap.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe rule is simple. The keys vector \u003ccode\u003eK[]\u003c/code\u003e should be built in the increasing key order to use binary search for fast lookup. The values vector \u003ccode\u003eV[]\u003c/code\u003e should be built in the \u003cem\u003ekey vector\u0026rsquo;s order\u003c/em\u003e by concatenating corresponding vectors.\u003c/p\u003e\n\u003cp\u003eThe symbol sequence \u003ccode\u003eS[]\u003c/code\u003e is constructed in the following way. For each key in key\u0026rsquo;s vector \u003ccode\u003eK[]\u003c/code\u003e we put \u003ccode\u003e0\u003c/code\u003e and for each value element in the \u003cem\u003eassociated value\u0026rsquo;s sub-vector\u003c/em\u003e \u003ccode\u003eV[]\u003c/code\u003e we put \u003ccode\u003e1\u003c/code\u003e as it\u0026rsquo;s shown at the picture above. We will be using \u003ca href="/docs/data-zoo/searchable-seq/"\u003e\u003ccode\u003erank()\u003c/code\u003e and \u003ccode\u003eselect()\u003c/code\u003e\u003c/a\u003e operations for implementing access and update operations over our Multimap.\u003c/p\u003e\n\u003cp\u003eTo find a value for a key we first need to find a position for this key in the sorted vector \u003ccode\u003eK[]\u003c/code\u003e by using binary search: \u003ccode\u003ePx = bsearch(S[], Kx)\u003c/code\u003e. Given the position \u003ccode\u003ePx\u003c/code\u003e for key \u003ccode\u003eKx\u003c/code\u003e we can find corresponding position \u003ccode\u003eSx\u003c/code\u003e in the \u003ccode\u003eS[]\u003c/code\u003e by using the \u003ccode\u003eselect\u003c/code\u003e operation: \u003ccode\u003eSx = select(S[], Px + 1, 0)\u003c/code\u003e \u0026ndash; we are looking for position of \u003ccode\u003ePx\u003c/code\u003e-th \u003ccode\u003e0\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e. \u003ccode\u003eSx + 1\u003c/code\u003e will be position of the first symbol \u003ccode\u003e1\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e, corresponding to the first value, associated with \u003ccode\u003eKx\u003c/code\u003e. In order to locate related position in \u003ccode\u003eV[]\u003c/code\u003e we need to calculate total size of all sub-vectors for keys \u003cem\u003ebefore\u003c/em\u003e \u003ccode\u003eKx\u003c/code\u003e: \u003ccode\u003eVx = rank(S[], Px, 1)\u003c/code\u003e. In order to find number of elements in specific sub-vector, associated with \u003ccode\u003eKx\u003c/code\u003e we can use \u003ccode\u003ecount()\u003c/code\u003e operation \u0026ndash; count number of symbols in a run, starting from the specified one: \u003ccode\u003eLx = count(S[], Px + 1, 1)\u003c/code\u003e \u0026ndash; counting number of \u003ccode\u003e1\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e starting from \u003ccode\u003ePx + 1\u003c/code\u003e till we either hit the next \u003ccode\u003e0\u003c/code\u003e (next key\u0026rsquo;s symbol mark in the sequence) or the end of the sequence.\u003c/p\u003e\n\u003cp\u003eBy applying the same math we can derive corresponding operations for updating the Multimap structure: insert entries, remove entries, merge and split maps and so on. \u0026lsquo;Rank()\u0026rsquo;, \u0026lsquo;select()\u0026rsquo; and \u0026lsquo;count()\u0026rsquo; have logarithmic time complexity.\u003c/p\u003e\n\u003cp\u003eSpending one bit for every value element in the Multimap is not necessary. We can easily use RLE encoding \u003ccode\u003eSrle[]\u003c/code\u003e for \u003ccode\u003eS[]\u003c/code\u003e as it\u0026rsquo;s shown at the picture above. For large value sub-vectors (file system/object store) space saving may be significant.\u003c/p\u003e\n\u003ch3 id="representing-relational-tables"\u003eRepresenting relational tables\u003c/h3\u003e\n\u003cp\u003eNote that Multimap is sufficient for representing a row-wise \u003cem\u003eclustered\u003c/em\u003e relational table. \u003ccode\u003eK\u003c/code\u003e is a table\u0026rsquo;s \u003cem\u003eprimary key\u003c/em\u003e and corresponding sub-vector may contain row\u0026rsquo;s content in an unstructured form. If we want unclustered (regular) table without a primary key, we just not needed the \u003ccode\u003eK[]\u003c/code\u003e vector: \u003ccode\u003estd::vector\u0026lt;std::vector\u0026lt;V\u0026gt;\u0026gt;\u003c/code\u003e. Everything else is the same.\u003c/p\u003e\n\u003cp\u003eSuch table representation has two notable properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eTo \u003cem\u003escan\u003c/em\u003e a table we just need to scan three perfectly memory-aligned data structures concurrently.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTable may easily have \u003cem\u003every large\u003c/em\u003e rows, as well very small rows. There is no any intrinsic memory overhead for this.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="multimap-with-searchable-values"\u003eMultimap with searchable values\u003c/h2\u003e\n\u003cp\u003ePractically important case is \u003ccode\u003estd::map\u0026lt;K, std::set\u0026lt;V\u0026gt;\u0026gt;\u003c/code\u003e or, here, SearchableMultimap used, for instance, for implementing sparse graphs. If \u003ccode\u003eK\u003c/code\u003e and \u003ccode\u003eV\u003c/code\u003e are graph node\u0026rsquo;s identifiers, SearchableMultimap may be used for storing node\u0026rsquo;s neighbours in the graph.\u003c/p\u003e\n\u003cp\u003eThe easiest way to make sub-vectors efficiently searchable is just to sort them. And this is the only option if identifiers are not numbers. Because we know the size of sub-vector for a given key, we can binary-search in this sub-vector only.\u003c/p\u003e\n\u003cp\u003eThe second option is to use the same technique that is used for \u003ca href="/docs/data-zoo/partial-sum-tree/"\u003epartial sum trees\u003c/a\u003e. If identifiers are numbers supporting \u003ccode\u003e+\u003c/code\u003e and \u003ccode\u003e-\u003c/code\u003e binary operations, we can build \u003cem\u003edelta sequences\u003c/em\u003e for each sub-array individually and concatenate them into \u003ccode\u003eV[]\u003c/code\u003e. Unlike locally-sorted sub-vectors, such delta-sequence is globally-searchable. We just need to add \u003cem\u003eprefix\u003c/em\u003e \u003ccode\u003eNx = sum(V[], Vx)\u003c/code\u003e for key the \u003ccode\u003eKx\u003c/code\u003e to the \u003ccode\u003eKx\u003c/code\u003e\u0026rsquo;s value.\u003c/p\u003e\n\u003ch2 id="wide-table"\u003eWide Table\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eWide Table\u003c/em\u003e is a data structure of the form \u003ccode\u003estd::map\u0026lt;RKT, std::map\u0026lt;CKT, std::vector\u0026lt;V\u0026gt;\u0026gt;\u0026gt;\u003c/code\u003e where \u003ccode\u003eRKT\u003c/code\u003e is a row key type and \u003ccode\u003eCKT\u003c/code\u003e is a column key type. Each row of a Wide Table may have arbitrary, practically unlimited, number of columns. This is rarely needed in practice for \u003cem\u003erelational\u003c/em\u003e tables. But may be needed for other \u003cem\u003eapplication-level\u003c/em\u003e data structures built on top of wide tables. Basically, wide table is a sparse matrix or a sparse graph.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="wide-table.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIt\u0026rsquo;s rather easy to get wide table from a Multimap, we just need an alphabet with three symbols instead of two: \u003ccode\u003e0\u003c/code\u003e means row row key RK, \u003ccode\u003e1\u003c/code\u003e means column key CK and \u003ccode\u003e2\u003c/code\u003e means column data. Everything else, including navigational operations are basically the same.\u003c/p\u003e\n\u003ch2 id="generalized-hierarchical-container"\u003eGeneralized Hierarchical Container\u003c/h2\u003e\n\u003cp\u003eThe pattern above can be generalized. Single-level containers, like Map or Vector need a symbol sequence with zero-size alphabet (or no sequence at all). Each new layer add one symbol to the alphabet. Memoria relies heavily on this property for complex containers. For an L-level container we have L vectors and a searchable symbol sequence (so, L + 1 vectors total).\u003c/p\u003e\n\u003ch2 id="multistream-btree"\u003eMultistream B+Tree.\u003c/h2\u003e\n\u003cp\u003eThere are basically two strategies of implementing generalized hierarchical containers for external memory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eOne B+Tree per vector or sequence\u003c/em\u003e. This is the easiest option because we just need two separately engineered B+Trees. There are two main drawbacks here. First, small structures may take up to \u003ccode\u003eL + 1\u003c/code\u003e blocks of memory. Second, for each query we need to search in multiple B+Trees (from root down to leafs).\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eCombine all vectors in one B+Tree\u003c/em\u003e, enforcing locality principle: data accessed together shout be in the same block or very close to each other. It can make certain important queries faster, but by the expense of other types of queries.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNo strategy is universally the best, but the second one is better if we want to optimize things for \u003cem\u003ereading\u003c/em\u003e. Memoria may use them both for different reasons, but it primarily relies on the second one \u0026ndash; \u003cem\u003emultistream B+Tree\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe basic idea is simple.\u003c/p\u003e\n\u003cp\u003eIf we are representing a Multimap with a multistream B+Tree, we need to split \u003ccode\u003eSrle[]\u003c/code\u003e, \u003ccode\u003eK[]\u003c/code\u003e and \u003ccode\u003eV[]\u003c/code\u003e in such way that all related array elements be put into the same leaf node. In this case, for example, if we found specific key, associated values will be either in this leaf (most likely) or in the next one (pretty quick operation).\u003c/p\u003e\n\u003cp\u003eBranch node\u0026rsquo;s structure is similar. We need one array for maximum keys for each child (Key Stream), and two arrays for sums of \u003ccode\u003e0\u003c/code\u003e and \u003ccode\u003e1\u003c/code\u003e in the subtree\u0026rsquo;s \u003ccode\u003eSrle\u003c/code\u003e sequence (Srle Stream). And, of course, child node ID array:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree_nodes.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBranch nodes for \u003ccode\u003eSrle\u003c/code\u003e sequence form a \u003ca href="/docs/data-zoo/partial-sum-tree/"\u003epartial/prefix sum tree\u003c/a\u003e, that can be used for efficient implementation of \u0026lsquo;rank()\u0026rsquo; and \u0026lsquo;select()\u0026rsquo; operations.\u003c/p\u003e\n\u003cp\u003eKeys stream in this specific case may be searched in a usual way for max-type B+tree.\u003c/p\u003e\n\u003cp\u003eNote that in this example branch nodes do not show Value Stream, because values are not searchable, so we don\u0026rsquo;t need to store separate \u003cem\u003eindex\u003c/em\u003e info for values.\u003c/p\u003e\n\u003cp\u003eNote also that mutistream B+Tree implementation in Memoria is slightly different in the way how streams are represented in tree blocks, here we omitted some details for brevity.\u003c/p\u003e\n'}).add({id:19,href:"/docs/data-zoo/louds-tree/",title:"Level Order Unary Degree Sequence (LOUDS) ",description:"",content:'\u003cp\u003eLevel Order Unary Degree Sequence or LOUDS is a special form of ordered tree encoding. To get it we first need to enumerate all nodes of a tree in level order as it is shown of the following figure.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds.png"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThen for each node we write its degree in unitary encoding. For example, degree of the fist node is 3, then its unitary encoding is \u0026lsquo;1110\u0026rsquo;. To finish LOUDS string we need to prepend substring \u0026lsquo;10\u0026rsquo; to it as it is shown on the figure. Given an ordered tree on N nodes LOUDS takes no more than 2N + 1 bits. This is very succinct implicit data structure.\u003c/p\u003e\n\u003cp\u003eLOUDS is a bit vector. We also need the following operations on it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erank1(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;1\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank0(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;0\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect1(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;1\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect0(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;0\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDifferent ways of tree node numbering for LOUDS are possible, Memoria uses the simplest one. Tree node positions are coded by \u0026lsquo;1\u0026rsquo;.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003enode_num = rank1(i + 1)\u003c/code\u003e \u0026ndash; gets tree node number at position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ei = select1(node_num)\u003c/code\u003e \u0026ndash; finds position of a node in LOUDS given its number in the tree.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHaving this node numbering we can define the following tree navigation operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efist_child(i) = select0(rank1(i + 1)) + 1\u003c/code\u003e \u0026ndash; finds position of the first child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_child(i) = select0(rank1(i + 1) + 1) - 1\u003c/code\u003e \u0026ndash; finds position of the last child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparent(i) = select1(rank0(i + 1))\u003c/code\u003e \u0026ndash; finds position of the parent for the node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echildren(i) = last_child(i) - first_child(i)\u003c/code\u003e \u0026ndash; return number of children for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echild(i, num) = first_child(i) + num\u003c/code\u003e \u0026ndash; returns position of num-th child for the node at the position \u003ccode\u003ei\u003c/code\u003e, \u003ccode\u003enum \u0026gt;= 0\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_node(i) = LOUDS[i] == 1 ? true : false\u003c/code\u003e \u0026ndash; checks if \u003ccode\u003ei\u003c/code\u003e-th position in tree node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that navigation operations only defined for positions \u003ccode\u003ei\u003c/code\u003e for those \u003ccode\u003eis_leaf(i) == true\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cp\u003eLet we find number of the first child for the node 8.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eselect1(8) = 11\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efirst_child(11) = select0(rank1(11 + 1)) + 1 = select0(8) + 1 = 19\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank1(19 + 1) = 12\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first child for node 8 is node 12.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how the parent() operation works:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds-parent.png"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="limitations"\u003eLimitations\u003c/h2\u003e\n\u003cp\u003eNode numbers are valid only until insertion (or deletion) into the tree. Additional data structure is necessary to keep track of node positions after tree structure updates.\u003c/p\u003e\n\u003ch2 id="labelled-tree"\u003eLabelled Tree\u003c/h2\u003e\n\u003cp\u003eLabelled tree is a LOUDS tree with a fixed set of numbers (or \u0026lsquo;labels\u0026rsquo;) associated with each node. It is implemented as multistream balanced tree where the first stream is dynamic bit vector with rank/select support, and the rest are streams for each label.\u003c/p\u003e\n\u003cp\u003eStreams elements distribution is very simple for this data structures. Each label belongs to a tree node that is coded by position of \u0026lsquo;1\u0026rsquo; in LOUDS. Each leaf of balanced tree has some subsequence of the LOUDS.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="labeled_tree_leaf.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLet we have a LOUDS sub-stream of size N with M 1s for a given leaf. Then this leaf must contain M labels in each label stream. The following expressions links together node and level positions withing a leaf:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elabel_idx = rank1(node_idx + 1) - 1\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enode_idx = select1(label_idx + 1)\u003c/code\u003e;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhere \u003ccode\u003elabel_idx\u003c/code\u003e is in [0, M) and \u003ccode\u003enode_idx\u003c/code\u003e is in [0, N)\u003c/p\u003e\n\u003cp\u003eLabeledTree uses balanced partial sum tree as a basic data structure. Because of that it optionally supports partial sums of labels is specified tree node range. The most interesting range is \u003ccode\u003e[0, node_idx)\u003c/code\u003e. See \u003ca href="/docs/data-zoo/wavelet-tree"\u003eMultiary Wavelet Tree\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2 id="cardinal-trees"\u003eCardinal Trees\u003c/h2\u003e\n\u003cp\u003eIn ordered trees like in the example above, all children nodes are naturally ordered, and a node may have arbitrary number of children. In the cardinal tree of degree $D$, a node always have $D$ children, but some children can be omitted. And this information is stored in the tree, like \u0026ldquo;child $i$ is absent\u0026rdquo;. Binary search trees are cardinal trees of degree 2.\u003c/p\u003e\n\u003cp\u003eCardinal tree of degree 4 (and greater, where degree is a power of 2) is a trie-based (or region-based) \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCardinal LOUDS tree can be implemented either as a labelled tree, where node labels (from 0 to $D-1$) are \u003cem\u003ecardinal labels\u003c/em\u003e (for sparse cardinal trees like spatial trees), or by using a searchable bitmap specifying which children are present. The bitmap can be \u003cem\u003ecompressed\u003c/em\u003e, saving space for sparse cases.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS trees are especially important type of compact/succinct trees, because all children of a node are stored linearly in memory, that is DRAM-friendly: traversal of a tree is a series of liner scans split by random jumps. But the number of random jumps is much smaller comparing to other types of trees. LOUDS trees do not require any specific hardware support, providing that \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable bitmap\u003c/a\u003e is fully accelerated.\u003c/p\u003e\n'}).add({id:20,href:"/docs/data-zoo/wavelet-tree/",title:"Multiary Wavelet Trees",description:"",content:'\u003cp\u003eWavelet trees (WT) are data succinct rank/select dictionaries for large alphabets with many \u003ca href="http://arxiv.org/abs/1011.4532"\u003epractical applications\u003c/a\u003e. There is a good \u003ca href="http://alexbowe.com/wavelet-trees/"\u003eexplanation\u003c/a\u003e of what binary wavelet trees are and how they work. They provide rank() and select() over symbol sequences ($N$ symbols) drawn from arbitrary fixed-size alphabets ($K$ symbols) in $O(log(N) * log(K))$ operations, where logarithms are on the base of 2. Therefore, for large alphabets, $log(K)$ is quite a big value that leads to big hidden constants in practical implementations of the binary WT.\u003c/p\u003e\n\u003cp\u003eIn order to improve runtime efficiency of wavelet trees we have to lower this constant. And one of the way here is to use multiary cardinal trees instead of binary ones. In this case, for $M$-ary cardinal tree we will have $log(M)$ speedup factor over binary trees (tree height is $log(M)$-times smaller).\u003c/p\u003e\n\u003ch2 id="wavelet-tree-structure"\u003eWavelet Tree Structure\u003c/h2\u003e\n\u003cp\u003eLet we have a sequence of integers, say, 54.03.12.21.47.03.17.54.22.51 drawn from 6-bit alphabet. The following figure shows 4-ary wavelet tree for this sequence. Such WT has $6/log_2(4) = 3$ levels.\u003c/p\u003e\n\u003cp\u003eFirst we need to represent our sequence in a different format. Our WT is 4-ary and has 3 layers. We need to \u0026ldquo;split\u0026rdquo; the sequence in 3 layers horizontally where symbols of each layer are drawn from 2-bit alphabet. In other words, we need to recode our sequence from base of 10 to base of 4, and then write numbers vertically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that this is just a logical operation, it doesn\u0026rsquo;t require any transformation of the sequence itself.\u003c/p\u003e\n\u003cp\u003eOur WT is a 4-ary cardinal tree, each node has from 0 to 4 children. Each child represents one symbol from the layer\u0026rsquo;s alphabet. Note that in general case it isn\u0026rsquo;t necessary to draw all layers from the same alphabet, but it simplifies implementation.\u003c/p\u003e\n\u003cp\u003eIn order to build WT, perform the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAssign top layer (Layer 2) of the sequence to the root node of WT.\u003c/li\u003e\n\u003cli\u003eFor each symbol of Layer 1 put it to the subsequence of the node with the cardinal label matched with corresponding symbol from the same position in the Layer 2.\u003c/li\u003e\n\u003cli\u003eRepeat step (2) for symbols at Layer 0 but now select appropriate child at Level 1 of the tree, using pair of symbols from the same positions at Layer 1 and Layer 2 of the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the figure for details. Symbols in the WT and the sequence are colored to simplify understanding of symbols\' distribution.\u003c/p\u003e\n\u003cp\u003eNote that for an alphabet with K symbols, multiary WT has up to K leafs that can be very significant number. But for most practical cases this number is moderate. The larger number of distinct symbols in the sequence, the bigger tree is. Dynamic LOUDS with associated cardinality labels is used to code structure of WT.\u003c/p\u003e\n\u003cp\u003eAlso, it is not necessary to keep empty nodes in the tree (they are shown in gray on the figure).\u003c/p\u003e\n\u003ch2 id="insertion-and-access"\u003eInsertion and Access\u003c/h2\u003e\n\u003cp\u003eTo insert a value into WT we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the path from root to leaf for inserted values, insert tree nodes if necessary;\u003c/li\u003e\n\u003cli\u003efind correct position in the node\u0026rsquo;s subsequence to insert current symbol.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe path in the wavelet tree is determined by \u0026ldquo;layered\u0026rdquo; representation if inserted symbol. Computation of insertion position is a bit tricky.\u003c/p\u003e\n\u003cp\u003eLet we insert the value of 37 into position 7. Layered representation of 37 is \u0026ldquo;211\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLevel 2. Insert \u0026ldquo;2\u0026rdquo; into position 7 of root node\u0026rsquo;s subsequence of WT.\u003c/li\u003e\n\u003cli\u003eLevel 1. Next child is \u0026ldquo;2\u0026rdquo;. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(7 + 1, 2) - 1 = rank(8, 2) - 1 = 1\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003cli\u003eLevel 0. Next child is \u0026ldquo;1\u0026rdquo;, create it. Repeat the procedure for Layer 1. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(1 + 1, 1) - 1 = rank(2, 1) - 1 = 0\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_insert.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAccess is similar, but instead of to insert a symbol to a node\u0026rsquo;s subsequence, take the symbol form it and use it to select next child.\u003c/p\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute rank(position, symbol) we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the leaf in WT for the symbol;\u003c/li\u003e\n\u003cli\u003efind position in the leaf to compute the final rank.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_rank.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eComputation of select(rank, symbol) is different. If rank() is computed top-down, then select() is computed bottom-up.\u003c/p\u003e\n\u003cp\u003eLet we need to select position of the 2nd 3 in the original sequence. Layered representation for 3 is \u0026ldquo;003\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFind the leaf in WT for the given symbol.\u003c/li\u003e\n\u003cli\u003ePerform \u003ccode\u003eselect(2, \u0026quot;3\u0026quot;) = Pos0\u003c/code\u003e on the leaf\u0026rsquo;s sequence.\u003c/li\u003e\n\u003cli\u003eWalk up to parent for his leaf. Perform \u003ccode\u003eselect(Pos0 + 1, \u0026quot;0\u0026quot;) = Pos1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eStep up to the parent node (the root). Perform \u003ccode\u003eselect(Pos1 + 1, \u0026quot;0\u0026quot;) = Pos\u003c/code\u003e. This is the final result.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_select.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="implementations"\u003eImplementations\u003c/h2\u003e\n\u003cp\u003eIn Memoria, Multiary wavelet tree consists of four distinct data structures.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLOUDS to store wavelet tree structure.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of cardinal labels for tree nodes.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of sizes for tree node\u0026rsquo;s sub-sequences.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of node\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first three structures are implemented as single \u003ca href="/docs/data-zoo/louds-tree"\u003eLabeled Tree\u003c/a\u003e with two labels. The first one is cardinality of the node in its parent. The second one is size of node\u0026rsquo;s subsequence.\u003c/p\u003e\n\u003cp\u003eThe fourth data structure is a separate \u003ca href="/docs/data-zoo/searchable-seq"\u003eSearchable Sequence\u003c/a\u003e for small sized alphabets.\u003c/p\u003e\n\u003cp\u003eMemoria has two different implementations of WT algorithm. The first one is dynamic WT that provides access/insert/select/rank operations performing in O(log \u003cem\u003eN\u003c/em\u003e) time.\u003c/p\u003e\n\u003cp\u003eThe second one has all those four data structures implemented with \u003ca href="Memory_Allocation"\u003ePacked Allocator\u003c/a\u003e placed in a single raw memory block of limited size. This implementation has fast access/select/rank operations but slow insert operation with O(\u003cem\u003eN\u003c/em\u003e) time complexity.\u003c/p\u003e\n\u003cp\u003eCurrently Memoria provides only 256-ary wavelet tree for 32-bit sequences. Other configurations will be provided in upcoming releases of the framework.\u003c/p\u003e\n'}).add({id:21,href:"/docs/data-zoo/packed-allocator/",title:"Packed Allocator",description:"",content:'\u003cp\u003eWe need to place several, possibly resizable (see below), objects into a single contiguous memory block of limited size. Classical malloc-like memory allocator is not suitable here because it doesn\u0026rsquo;t work well with resizable objects. Especially if they are allocated in a relatively small memory block. To maintain resizability efficiently we have to relocate other objects if some object is resized.\u003c/p\u003e\n\u003ch2 id="resizable-object-pattern"\u003eResizable Object Pattern\u003c/h2\u003e\n\u003cp\u003eResizable object is an object that has unbounded size. Usually it has the following pattern:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  int object_size_;\n  char[] variable_size_data_;\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, the last member \u003ccode\u003echar[] variable_size_data_\u003c/code\u003e is an unbounded array. For any object \u003ccode\u003esizeof()\u003c/code\u003e does\nnot count the last member if it is unbounded array. For example, \u003ccode\u003esizeof(ResizableObject) == sizeof(int)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf a custom allocator allocates more memory than necessary for resizable objects, this memory can be accessed via the last member. Usually such an object should know length of the memory block it is mapper to. But it can also be stored by memory allocator.\u003c/p\u003e\n\u003ch2 id="linear-contiguous-allocator"\u003eLinear Contiguous Allocator\u003c/h2\u003e\n\u003cp\u003eThe idea is to place all abjects contiguously in a memory block and shift them if some object is resized. Separate layout dictionary is used to locate objects in a block. Objects are accessed by their indexes, not by direct addresses in the block.\u003c/p\u003e\n\u003cp\u003eLayout dictionary is an ordered list of block offsets. If dictionary if large, \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e can be used to speedup access.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_contiguous_allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLayout dictionary is placed into the same memory block as object\u0026rsquo;s data.\u003c/p\u003e\n\u003ch2 id="allocator-aware-objects"\u003eAllocator-Aware Objects\u003c/h2\u003e\n\u003cp\u003eThe main property of resizable objects is that their size can be changed dynamically, that requires interaction with allocator. Consider the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  //...\n  int insert(int index, int value); // enlarge object\n  int remove(int index);            // shrink object\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have two methods affecting object\u0026rsquo;s size. And it is good to incapsulate required interaction with allocator within resizable objects. But the problem is we don\u0026rsquo;t want to store raw memory pointers to the allocator withing objects for various reasons. The main reason is we want allocators to be relocatable. If the allocator itself is relocated, all pointers have to be updated.\u003c/p\u003e\n\u003cp\u003eThe idea is to put allocator and objects into a single addressable memory block. In this case we can get address of allocator having only address of the object and its relative offset in the memory block. Let\u0026rsquo;s consider \u003ccode\u003ePackedAllocatable\u003c/code\u003e base class for any allocator-aware resizable object:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\nclass PackedAllocator;\n\nclass PackedAllocatable {\n  typedef PackedAllocator Allocator;\n  int allocator_offset_; // resizable object offset in the allocator\'s memory block\npublic:\n  Allocator* allocator() {\n    if (allocator_offset_ \u0026gt; 0) {\n      return reinterpret_cast\u0026lt;Allocator*\u0026gt;(reinterpret_cast\u0026lt;char*\u0026gt;(this) - allocator_offset_);\n    }\n    else return nullptr;\n  }\n\n  // Other methods go here...\n};\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the following figure how it may look like:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_brief.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn Memoria we call such allocator \u003cem\u003epacked\u003c/em\u003e, because it packs all objects and itself into single self-sufficient memory block that can be relocated or serialized. That doesn\u0026rsquo;t affect relative positions of objects within the memory block.\u003c/p\u003e\n\u003cp\u003eEach allocator-aware resizable object must derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e class that maintains relative offset of an object to the allocator. The following figure explains it in greater details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we have four resizable objects with sizes 4, 7, 11, and 9 respectively. Each object maintains its relative offset in the memory block, that is converted to a pointer to the allocator.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s increase object #1 by 8 units:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full_enlarged.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWe need to perform the following operations for objects #2 and #3.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eShift objects\' data right by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease objects\' offsets in layout dictionary by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease \u003ccode\u003ePackedAllocatable::allocator_offset_\u003c/code\u003e by 8 units.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNot all abjects are resizable, they don\u0026rsquo;t need to maintain a pointer to the allocator. But now allocator has to know which objects are instances of \u003ccode\u003ePackedAllocatable\u003c/code\u003e and which are not to update pointers properly.\u003c/p\u003e\n\u003ch2 id="recursive-allocator"\u003eRecursive Allocator\u003c/h2\u003e\n\u003cp\u003eThe next idea is to define packed allocator recursively by deriving from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case it is possible to embed any packed allocator into another allocator just as any resizable object.\u003c/p\u003e\n\u003ch2 id="packedallocator-api"\u003ePackedAllocator API\u003c/h2\u003e\n\u003cp\u003eSo we have:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ePackedAllocator\u003c/code\u003e managing resizable memory regions withing contiguous relocatable memory block.\u003c/li\u003e\n\u003cli\u003eAllocator-aware objects derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003ePacked allocator also derives from \u003ccode\u003ePackedAllocatable\u003c/code\u003e that mean it can be embedded into another packed allocator.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe last idea is to use \u003ccode\u003ePackedAllocator\u003c/code\u003e as a base class for resizable objects. That enables them to have more than one resizable section.\u003c/p\u003e\n\u003cp\u003eThe following code snippet explains basic PackedAllocator API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n\n  template \u0026lt;typename T\u0026gt;\n  T* get(int i);             //returns address of i-th memory block, i = 0...\n  template \u0026lt;typename T\u0026gt;\n  const T* get(int i) const; //returns address of i-th memory block, i = 0...\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i, int size); // allocates size bytes for i-th memory block and initializes\n                                // it properly if T derives from PackedAllocatable.\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i); // Allocates space for i-th memory block and initializes\n                      // it properly if T derives from PackedAllocatable. \n                      // Size of the block is got via T::empty_size() if T derives \n                      // form PackedAllocatable and sizeof(T) otherwise.\n\n  // resize memory block at address \'addr\', resize parent allocator if necessary\n  // throws PackedOOMException if top-most allocator runs out of memory\n  void resize(const void* addr, int new_size); \n\n  //the same as above but returns size of i-th block in bytes\n  void resize(int i, int new_size);\n\n  int size(int i) const; \n\n  void init(int entries); // initializes allocator with the specified number of \n                          // empty blocks (entries)\n \n  // returns size in bytes of empty allocator having specified number of entries\n  static int empty_size(int entries); \n\n  static int round(int size); // round size to alignment blocks. e.g. if alignment block is 8 bytes\n                              // round(1) = 8; round(12) = 16\n\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo interact with PackedAllocator each allocatable object provides two methods. One of them is for initialization, and another one is to query object size:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass SimpleResizableObject: public PackedAllocatable {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\nclass AdvancedResizableObject: public PackedAllocator {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that resizable objects must be \u003ca href="http://en.cppreference.com/w/cpp/types/is_trivial"\u003etrivial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote also that Memoria doesn\u0026rsquo;t currently use placement \u003ca href="http://en.cppreference.com/w/cpp/memory/new/operator_new"\u003enew\u003c/a\u003e и \u003ca href="http://en.cppreference.com/w/cpp/language/delete"\u003edelete\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// There is no way to specify custom allocator and other parameters here, only address \n// of the object to delete. PackedAllocator does not allow to get allocator address given\n// only address of a block it manages. so it provides explicit API for allocation \n// and deallocation.\n\nvoid operator delete (void *ptr); // placement delete operator\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee this \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/include/memoria/core/packed/tools/packed_allocator.hpp"\u003esource\u003c/a\u003e for more details about PackedAllocator implementation.\u003c/p\u003e\n\u003ch2 id="resizable-object-example-seachable-sequence"\u003eResizable Object Example: Seachable Sequence\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s consider a relatively simple but live example: \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable sequence\u003c/a\u003e. It is a sequence of symbols providing rank and select operations performed in logarithmic time. To provide such time complexity, data structure uses additional index that have to be quite complex for large alphabets. Let\u0026rsquo;s say that in general case the index is compressed and we don\u0026rsquo;t know it\u0026rsquo;s actual size ahead of time. The size of index is a variable value depending of a sequence content.\u003c/p\u003e\n\u003cp\u003eSo the sequence has at least two resizable blocks: index, and symbols.\u003c/p\u003e\n\u003cp\u003eBelow there is a code snipped explaining how update operations on the object interact with its allocator(s).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\ntemplate \u0026lt;int BitsPerSymbol\u0026gt;\nclass SearchableSequence: public PackedAllocator {\n  typedef PackedAllocator                              Base;\n  typedef SearchableSequence\u0026lt;BitsPerSymbol\u0026gt;            MyType;\n\n  typedef unsigned int                                 Symbol;\n\n  // Instantiate index only if the sequence is larger\n  // than this value.\n  static const int IndexSizeThreshold                  = 64;\n\n  // no members should be declared here\n  // use Metadata class instead\npublic:\n  class Metadata {\n    int size_;\n    int\u0026amp; size() {return size_;} \n    const int\u0026amp; size() const {return size_;} \n  };\n\n  class Index: public PackedAllocator {\n    // index data structure for this SearchableSequence\n  };\n\n  // This enum codes memory block indexes.\n  enum {\n    METADATA, // Metadata block\n    INDEX,    // Indexes block\n    SYMBOLS   // Symbols block\n  };\n\n  // returns address of Metadata object\n  Metadata* metadata() {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n  const Metadata* metadata() const {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n\n  // returns address of Index object\n  Index* index() {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n  const Index* index() const {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n\n  // returns address of symbols block\n  Symbol* symbols() {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  const Symbol* symbols() const {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  \n  // returns size in bytes of empty sequence. this method is used by \n  // PackedAllocator::allocateEmpty(int) to get object\'s default size\n  static int empty_size() \n  {\n    int allocator_size = Base::empty_size(); // size of allocator itself\n    int metadata_size  = Base::round(sizeof(Metadata)); // size of metadata block\n    int index_size     = 0; // index is empty for empty sequence\n    int symbols_size   = 0; // symbols block is also empty for empty sequence\n\n    return allocator_size + metadata_size + index_size + symbols_size;\n  }\n\n  void init() \n  {\n    Base::init(3); // the object has three resizable sections.\n    \n    // Allocate metadata block and initialize it\n    Base::template allocate\u0026lt;Metadata\u0026gt;(METADATA);\n\n    // Allocate empty block for index. Do not initialize it\n    Base::template allocate\u0026lt;Index\u0026gt;(INDEX, 0);\n    \n    // Allocate empty block for symbols. \n    Base::template allocate\u0026lt;Symbol\u0026gt;(SYMBOLS, 0);    \n  }\n\n  // change the value of idx-th symbol\n  void setSymbol(int idx, int symbol);\n\n  // insert new symbol at the specified position\n  int insert(int idx, int symbol) \n  {\n    enlarge(1);              // enalrge SYMBOLS block\n    insertSpace(idx, 1);     // shift symbols\n    setSymbol(idx, symbol);  // set new symbol value\n\n    reindex();               // update search index\n  }\n\n  int size() const \n  {\n    return metadata()-\u0026gt;size();\n  }\n\n  // update index for the searchable sequence\n  void reindex() \n  {\n     // check if the sequence if large enough to have index\n     if (size() \u0026gt; IndexSizeThreshold) \n     {\n       if (Base::size(INDEX) == 0)\n       {\n         Base::template allocate\u0026lt;Index\u0026gt;(0); // create empty index if it doesn\'t exist\n       }\n\n       Index* index = this-\u0026gt;index();\n\n       // compute index size for given symbols and resize the index.\n       index-\u0026gt;resize_for(this-\u0026gt;symbols(), size());\n\n       // rebuild the index\n       // note that any resize operation invalidates pointers to blocks\n       // going after the resized one.\n       index-\u0026gt;update(this-\u0026gt;symbols(), size());\n     }\n     else {\n       // the sequence if not full enough to have the index,\n       // free it.\n       Base::resize(INDEX, 0); // free index block\n     }\n  }\n  \nprivate:\n  // insert empty space into symbols\'s data block,\n  // shift symbols in the range [idx, size()) \'length\' positions right.\n  void insertSpace(int idx, int length);\n\n  // returns symbols block size for specified number of symbols\n  static int symbols_size(int symbols);\n\n  //enlarge symbols block by \'items\' elements.\n  void enlarge(int items) \n  {\n    // get new size for SYMBOLS block\n    int new_symbols_size = MyType::symbols_size(size() + items)\n    \n    // enlarge SYMBOLS block\n    Base::resize(SYMBOLS, new_symbols_size);\n  }\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis pattern is used intensively for packed data structures in Memoria.\u003c/p\u003e\n'}).add({id:22,href:"/docs/data-zoo/associative-memory-1/",title:"Associative Memory (Part 1)",description:null,content:'\u003ch2 id="what-is-associative-memory"\u003eWhat is Associative Memory\u003c/h2\u003e\n\u003cp\u003eAssociative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of \u003cem\u003eMemory\u003c/em\u003e. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of \u003cem\u003ebits\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="motivating-example-rdf"\u003eMotivating Example: RDF\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href="https://en.wikipedia.org/wiki/Resource_Description_Framework"\u003eResource Description Framework\u003c/a\u003e data is modelled with a special form of a labelled graph, consisting from \u003cem\u003efacts\u003c/em\u003e (represented as URIs) and \u003cem\u003etriples\u003c/em\u003e in a form of $(Subject, Predicate, Object)$ linking various facts together. Logical representation of this \u003cem\u003esemantic graph\u003c/em\u003e is a table, enumerating all the triples in the graph. The main operation on the graph is \u003cem\u003epattern matching\u003c/em\u003e using SQL-like query language \u003ca href="https://en.wikipedia.org/wiki/SPARQL"\u003eSPARQL\u003c/a\u003e. Another common mode of operations over graphs is traversal, but this mode is secondary for semantic graphs. Pattern-matching in semantic graphs is based of \u003cem\u003eself-joins\u003c/em\u003e over the triple tables:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="triples.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBecause of the flexibility of SQPRQL, self-joins can be performed on any combination of Subject, Predicate and Objects, and the join on objects is the main reason why basic representation of graphs for RDF is a relational table. And, if we want fast execution, this table has to be properly indexed.\u003c/p\u003e\n\u003cp\u003eThe simplest way to provide indexing over a triple table is to sort it in some order, but ordered table only allows ordered \u003cem\u003ecomposite\u003c/em\u003e keys. If a table is ordered like (S, P, O), that means $(Subject, Predicate, Object)$, \u003cem\u003ethen\u003c/em\u003e we can search first by Subject, \u003cem\u003ethen\u003c/em\u003e buy Predicate, and only then by Object. But not vise versa. If we want to search by an Object first, we need another table ordered by object: (O, X, Y). To be able to search in any order we need all possible permutations of S, P and O: it\u0026rsquo;s $3! = 6$ tables.\u003c/p\u003e\n\u003cp\u003eSo, fully indexed RDF triple store will need at least 6 triple tables. How many is it?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times the size of the original single-table store (not counting URIs and Objects if they are stored separately).\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times slower insertions if they are not paralleled. Of course we can make many insertions in parallel, but it\u0026rsquo;s \u003cem\u003e6 times more energy\u003c/em\u003e anyway.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModern triple stores model semantic graphs with \u003cem\u003equads\u003c/em\u003e, adding \u003cem\u003eGraph ID\u003c/em\u003e to a triple, so nodes can link to entire graphs, no just other nodes (self-referentiality). In addition to, higher-dimensional tables ($D \u0026gt; 4$) can also be provided to speedup certain queries. And, in general case, if we have $D$-dimensional data table, we need \u003cem\u003eup to\u003c/em\u003e $D!$ orderings of this table. That is 24 for $D=4$ and grows faster than an exponent.\u003c/p\u003e\n\u003cp\u003eSorting relational tables does not scale at all for higher-order graphs ($D \u0026gt; 3$), but we can do better.\u003c/p\u003e\n\u003ch2 id="definition-of-associative-memory"\u003eDefinition of Associative Memory\u003c/h2\u003e\n\u003cp\u003eSo, without loss of generality, associative memory is a $D$-dimensional relation $R$ with \u003cem\u003eset\u003c/em\u003e semantics, over integer numbers drawn from some finite set (domain). The main operation on the memory is \u003cem\u003elookup\u003c/em\u003e that can be performed using arbitrary number of dimensions, specifying \u003cem\u003ematch\u003c/em\u003e, \u003cem\u003erange\u003c/em\u003e or \u003cem\u003epoint\u003c/em\u003e lookup, or any combination of for any number of dimensions. \u003cem\u003eRecall\u003c/em\u003e is the result of \u003cem\u003elookup\u003c/em\u003e operation, and is enumeration of all entries in $R$ matching the query.\u003c/p\u003e\n\u003cp\u003eAssociative memory can be either \u003cem\u003estatic\u003c/em\u003e, if only lookups are allowed. Or \u003cem\u003edynamic\u003c/em\u003e, if it supports insert, update and delete operation for individual elements (Update operation can be reduced to delete + insert).\u003c/p\u003e\n\u003ch2 id="multiscale-decomposition"\u003eMultiscale Decomposition\u003c/h2\u003e\n\u003cp\u003eLet we have a $D$-dimensional relation $R = \\lbrace {r_0, r_1, r_2, \u0026hellip;, r_{N-1}}\\rbrace$, representing a $set$, where $r_i = (c_0, c_1, \u0026hellip;, c_{D-1})_i$ - а $D$-dimensional tuple and $N$ is a \u0026lsquo;size\u0026rsquo; of the table (number of elements in the set). $c_{i,j}$ is a table\u0026rsquo;s cell value from row $i$ and dimension (column) $j$. Each cell value $c_{i,j}$ has a domain of $H$ bits.\u003c/p\u003e\n\u003cp\u003eExample: A set of 8x8 images with 8 bits per pixel can be represented with 64-dimensional relation with $H = 8$. Maximal number of images in a set is $N \u0026lt;= 8^{64} = 2^{192}$. Given such table we can easily define an associative memory reducing content-addressable lookup to linear table scans and bit manipulations (time complexity is $O(N)$). And, actually, this is how it\u0026rsquo;s implemented for approximate nearest neighbour search on massively-parallel hardware. Parallel linear scan is fast, but it\u0026rsquo;s not scalable (fast memory is expensive) and it\u0026rsquo;s not energy-efficient.\u003c/p\u003e\n\u003cp\u003eFortunately, we can transform $O(N)$ into $O(P H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e, where $1 \u0026lt;= P \u0026lt;= 2^D$ \u0026ndash; average number of nodes per \u003cstrong\u003ebucket\u003c/strong\u003e (see below), that is, thanks to the \u003ca href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality"\u003e\u0026ldquo;Blessing of Dimensionality\u0026rdquo;\u003c/a\u003e, usually tends to 1. And $M$ is a \u003cem\u003erecall size\u003c/em\u003e, number of points returned by the query over $R$.\u003c/p\u003e\n\u003cp\u003eTo perform the multiscale decomposition of relation $R$, we need to perform the following transformation for each row $r_i$:\u003c/p\u003e\n\u003cp\u003eLet $c_{ij} = S_{ij} = (s_0, s_1,s_2,\u0026hellip;,s_{H-1})_{ij}$, where $s \\in \\lbrace{0, 1}\\rbrace$ is a bit-string representation of cell value $c$. Let $s_h = B(S, h)$ \u0026ndash; $h$-th bit of string $S$.\u003c/p\u003e\n\u003cp\u003eNow, $M(r)$ is a multiscale decomposition of a row $r = (S_0, S_2, \u0026hellip;, S_{D-1})$. Informally, $M(r)$ is a bit string consisting from a concatenation of shuffling of all bits form $S_j$:\u003c/p\u003e\n\u003cp\u003e$M(r) = B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip;B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip; B(S_0, 0) B(S_1, 0) \u0026hellip; B(S_{D-1}, 0)$.\u003c/p\u003e\n\u003cp\u003eThe symbol $|$ is added to graphically separate $H$ \u003cem\u003elayers\u003c/em\u003e of the multiscale representation from each other.\u003c/p\u003e\n\u003cp\u003eExample. Let $r = (100, 110, 001)$. Then $M(r) = 111|010|001$. Note, that in some sense, $M(r)$ is producing a point on a \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003e$Z$-order curve\u003c/a\u003e for $r$. This correspondence may help in some applications.\u003c/p\u003e\n\u003cp\u003eSo, multiscale decomposition of $T = M(R)$ converts a table with $D$ columns into a table with $H$ columns, which are called \u003cem\u003elayers\u003c/em\u003e here.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s assume, that the table $T$ is sorted in a bit-lexicographic order.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="tables.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is special about table $T$ is that every column $L_j$ contains some information form each column $D_j$ from table $R$. So, by searching in a column of $T$ we can search in all columns from $R$ at once. Table $T$ itself does not provide any speedup over the sorted $R$ because the number of rows is still the same as for table $R$. But quick look at the table will show that there are many \u003cem\u003erepetitions\u003c/em\u003e. So, we can transform $T$ into a tree, by hierarchically (here, from left to right) collapsing repetitive elements in tables\' columns. Now, a \u003cstrong\u003ebucket\u003c/strong\u003e is a list of all children of the same parent node sorted lexicographically. It can be shown, that there may be \u003cem\u003eat most\u003c/em\u003e $2^D$ elements in a bucket. So, search in such data structure is $O(2^D H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. If $D$ is small, say, 16 or less, this may dramatically improve performance relative to linear scan of $R$ (even if it\u0026rsquo;s sorted). See the \u003ca href="#analysis"\u003eAnalysis\u003c/a\u003e section below for additional properties and limitations.\u003c/p\u003e\n\u003cp\u003eNote that each path from root to leaf in the tree encodes a single row in the table $T$, and after the inverse multiscale decomposition, in $R$.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s demonstrate how search works. Let we want to enumerate all rows in $R$ with $D_2$ = 0111, so $Q = (X, Y, 0111)$, where $X$ and $Y$ are \u003cem\u003eplacehoders\u003c/em\u003e. First, we need a multiscale representation of $Q$, $M(Q) = xy0|xy1|xy1|xy1$. Now, we need to traverse the tree from root to leafs, according to this pattern:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="search.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHaving the multiscale query encoding, the tree traversal is straightforward. We are visiting sub-trees in-order, providing that current pattern matches the node\u0026rsquo;s label. Visiting the leaf (+ its label is matched) means full match. The excess number of nodes visited by a range query is called \u003cem\u003eoverwork\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this example, the selectivity of the query is pretty good. All leafs matching the query are being visited (visited nodes are drawn in the dotted-green line style). Note the nodes marked with (*). The traversal process visited some nodes \u003cem\u003enot leading to the full match\u003c/em\u003e. This is why type complexity estimation for this data structure is logarithmic \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. Its performance depends on how data is distributed in the tree.\u003c/p\u003e\n\u003ch2 id="using-louds-for-the-tree"\u003eUsing LOUDS for the Tree\u003c/h2\u003e\n\u003cp\u003eTable $T$ has the same size (in bits) as the table $R$, but the tree, if implemented using pointer-based structure, will add a lot to the table representation. Fortunately, we can use \u003ca href="/docs/data-zoo/louds-tree"\u003eLOUDS Tree\u003c/a\u003e to encode the tree. LOUDS tree has very small memory footprint, only 2 bits per node (+ a small overhead, like 10%, for rank/select indices). Search tree size estimation is at most $NH$ nodes, so the tree itself will take at most size of two columns of the table $R$ ($2NH$ + small overhead). In most cases LOUDS-encoded $T$ will take less space that original table $R$, including auxiliary data structures.\u003c/p\u003e\n\u003cp\u003eWhat is the most important for LOUDS Tree is that it\u0026rsquo;s structured in memory in a linear order. So, if for some reason a spatial tree traversal degrades into linear search, the tree will be pretty good at this. We just need to read many layers of the tree in parallel. Such I/O operations can be efficiently prefetched.\u003c/p\u003e\n\u003ch2 id="analysis"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eIt can be shown that the tree is similar to a trie-based Quad Tree (for high dimensions), so many expected (average-case) and worst-case estimations also apply. In worst case, for high-dimensional trees, traversal degenerates into linear a search. Fortunately for LOUDS, it\u0026rsquo;s both memory-efficient for linear search \u003cem\u003eand\u003c/em\u003e for tree traversal. But on average, overwork is moderate, if doesn\u0026rsquo;t even tend to zero, so queries should perform pretty well.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s return to RDF triple stores and compare things to each other. Let\u0026rsquo;s assume that $D = 3$ (number of dimensions) and $H = 32$ (resource identifier\u0026rsquo;s size or search tree depth). So, in case of insertion of a triple, we have to perform 6 insertions into 6 triple tables (for each ordering) and 32 insertions in case of the tree (into each tree level). Reading is also slower: one lookup in a sorted table vs 32 lookups in the tree. It looks unreasonable to switch from triple tables (worst case logarithmic) to search tree (average case logarithmic), unless we are limited in memory and want to fit as many triples as possible into the available amount. But things start changing when we go into higher dimension ($D \u0026gt; 3$) and need more indices to speedup our queries.\u003c/p\u003e\n\u003cp\u003eSo far\u0026hellip;\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePoint-like operations (to check if some row exists in the relation $R$) will take $O(D log(N))$ time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRange search in the quad trees is logarithmic on average, but it\u0026rsquo;s relatively easy to build a worst-case example, when performance degrades to a linear scan. For example, if $D = 64$, maximal bucket size is $2^{64}$, that is much larger than any practical $N$ (number of entries in $R$). Unless the data is distributed uniformly among \u003cem\u003edifferent levels\u003c/em\u003e of the tree, we will end up having a few but very big buckets. So, special care must be taken on how we map our high-level data to dimensions of the tree. Random mapping is usually a safe bet, but always the best choice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the search tree \u0026ldquo;Blessing of Dimensionality\u0026rdquo; is fighting with \u0026ldquo;Curse of Dimensionality\u0026rdquo;, it\u0026rsquo;s kind of $0 \\cdot \\infty$. In higher dimensions data tends to be extremely \u003cem\u003esparse\u003c/em\u003e because volume size grows exponentially with number of dimensions. So, normally, even in high dimensions, buckets will tend to have small number of elements. The bigger the number \u0026ndash; the better, because it improves data compression and speeds up queries. But beware of the worst case, when the tree has one big bucket that all queries are visiting. It has also been observed for similar K-d trees, that with higher number of dimensions, \u003cem\u003eoverwork\u003c/em\u003e also tens to increase (the blessing vs curse situation, $0 \\cdot \\infty$, who wins?).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn case if LOUDS tree is dynamic and implemented using B-tree, \u003cem\u003einsert\u003c/em\u003e, \u003cem\u003eupdate\u003c/em\u003e and \u003cem\u003edelete\u003c/em\u003e operations to the search tree have $O(H log(N))$ time complexity for point-like updates and $O(H(log(N) + M))$ for batch updates of size $M$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe data structure representation in memory is very compact. It\u0026rsquo;s the size of original table $R$ + (up to) the size of two columns from $R$ for LOUDS tree + 5-10% of the tree to auxiliary data structures. Overhead of the tree is constant and is amortizing with higher number of dimensions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEven if we work with high-dimensional data, and we are losing to the curse of dimensionality, it\u0026rsquo;s possible to perform approximate queries. Many applications where high-dimensional data analysis is required, like AI, are essentially approximate. LOUDS tree allows to compactly and efficiently remember additional bits of information with each node, to facilitate approximate queries (if necessary).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS tree-based Associative memory seems to be impractical specifically for RDF triple stores, but if hardware accelerated, can be cost-effective at scale, providing also many other benefits, not just memory savings (which are huge for higher dimensions). The bottleneck is on the update operations, where insertion and deletion may require tens of B-tree updates. Fortunately, this operation is well-parallelizable so we can use thousands of small RISC-V cores equipped with special command for direct and energy-efficient implementation of essential operations (partial/prefix sums, rank and select). An array or cluster of such cores can even be embedded into \u003ca href="/subprojects/smart-storage"\u003estorage memory controller\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAdvanced data structures like LOUDS-based associative memory, considered to be impractical in the past, relative to more traditional things, like sorted tables and pointer-based data structures (trees, lists, graphs, \u0026hellip;) for main memory. But progress in computer hardware makes task-specific hardware accelerator a much more viable options, opening the road to completely new applications.\u003c/p\u003e\n\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-2"\u003enext post\u003c/a\u003e it will be shown how LOUDD-backed associative memory can be used for generic function approximation and inversion.\u003c/p\u003e\n'}).add({id:23,href:"/docs/data-zoo/associative-memory-2/",title:"Associative Memory (Part 2)",description:"",content:'\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-1"\u003eprevious part\u003c/a\u003e we saw how LOUDS tree can be used for generic compact and efficient associative associative memory over arbitrary number of dimensions. In this part we will see how LOUDS trees can be used for function approximation and inversion.\u003c/p\u003e\n\u003ch2 id="definitions"\u003eDefinitions\u003c/h2\u003e\n\u003cp\u003eLet $[0, 1] \\in R$ is the domain and range we are operating on. $D$ is a number of dimensions of our space, and for the sake of visualizability, $D = 2$.\u003c/p\u003e\n\u003cp\u003eLet $\\vec{X}$ is a vector encoding a point in $[0, 1]^D$, $\\vec{X} = \u0026lt;x_0, x_1, \u0026hellip;, x_{D-1}\u0026gt;$. Let $str(x [, h])$ is a function converting a real number from $[0, 1]$ into a binary string by taking a binary representation of a real number to a form like \u0026ldquo;$0.0010111010\u0026hellip;$\u0026rdquo;, and removing leading \u0026ldquo;$0.$\u0026rdquo; and trailing \u0026ldquo;$0\u0026hellip;$\u0026rdquo;. so, $str(0.181640625) = 001011101$. If $h$ argument is specified for $str(\\cdot, \\cdot)$, then resulting string is trimmed to $h$ binary digits, if it\u0026rsquo;s longer than that.\u003c/p\u003e\n\u003cp\u003eLet $len(x)$ is a number of digits in the result of $str(x)$. Note that $len(\\pi / 10) = \\infty$, so irrational numbers are literally infinite in this notation. Let $H$ is a maximal \u003cem\u003edepth\u003c/em\u003e of data, and there is some \u003cem\u003eimplicitly assumed\u003c/em\u003e arbitrary value for $h$, like 32 or 64, or even 128. So we can work with \u003cem\u003eapproximations\u003c/em\u003e of irrational and transcendent numbers, or with long rational numbers in a same way and without loss of generality.\u003c/p\u003e\n\u003cp\u003eLet $len(\\vec{X}) = max_{\\substack{i \\in \\lbrace 0,\u0026hellip;,D-1 \\rbrace }}(len(x_i))$.\u003c/p\u003e\n\u003cp\u003eLet $str(\\vec{X}) = (str(x_0),\u0026hellip;, str(x_{D-1}))$ is a string representation (a tuple) of $\\vec{X}$. And let we assume, elements of the tuple are implicitly extended with \u0026lsquo;$0$\u0026rsquo; from the right, if their length is less than $len(\\vec{X})$. In other words, all elements (binary string) of a tuple are implicitly of the same length.\u003c/p\u003e\n\u003cp\u003eLet $str(\\lbrace \\vec{X_0}, \\vec{X_1}, \u0026hellip; \\rbrace) = \\lbrace str(\\vec{X_0}), str(\\vec{X_1}), \u0026hellip; \\rbrace$. String representation of set of vectors is a set of string representation of individual vectors.\u003c/p\u003e\n\u003cp\u003eLet $M(\\vec{X}) = M(str(\\vec{X}))$ is a \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003emultiscale transformation\u003c/a\u003e of binary string representation of $\\vec{X}$. Informally, to compute $M(\\vec{X})$ we need to take all strings from its string representation (the tuple of binary strings) and produce another string of length $len(\\vec{X})  D$ by concatenating interleaved bits from each binary string in the tuple.\u003c/p\u003e\n\u003cp\u003eExample. Let $H = 3$ and $D = 2$. $\\vec{X} = \u0026lt;0.625, 0.25\u0026gt;$, $str(\\vec{X}) = (101, 01)$ and $M(\\vec{X}) = 10|01|10$. Note that signs $|$ are added to separate $H$ \u003cem\u003epath elements\u003c/em\u003e in the recording, they are here for the sake of visualization and are not a part of the representation. The string $10|01|10$ is also called \u003cem\u003epath expression\u003c/em\u003e because it\u0026rsquo;s a unique path in the multidimensional space decomposition \u003cem\u003eencoding\u003c/em\u003e the position of $\\vec{X}$ in this decomposition. Visually:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiscale1.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that the order in which \u003cem\u003epath elements\u003c/em\u003e of a path expression enumerate the volume is \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003eZ-order\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGiven a \u003cem\u003eset\u003c/em\u003e of $D$-dimensional vectors $\\lbrace \\vec{X} \\rbrace$, it\u0026rsquo;s multiscale transformation can be represented as a $D$-dimensional \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e. Such quad tree can be represented with a \u003ca href="/docs/data-zoo/louds-tree/#cardinal-trees"\u003ecardinal LOUDS tree\u003c/a\u003e of degree $2^D$. Here, implicit parameter $H$ is a \u003cem\u003emaximal height\u003c/em\u003e of the Quad Tree.\u003c/p\u003e\n\u003ch2 id="basic-asymptotic-complexity"\u003eBasic Asymptotic Complexity\u003c/h2\u003e\n\u003cp\u003eGiven that $N = |\\lbrace \\vec{X} \\rbrace|$, and given that LOUDS tree is dynamic (represented internally as a b-tree), the following complexity estimations apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInsertion and deletion\u003c/strong\u003e of a point is $O(H log(N))$. Update is semantically not defined because it\u0026rsquo;s a \u003cem\u003eset\u003c/em\u003e. Batch updates in Z-order are $O(H (log(N) + B))$, where $B$ is a batch size. Otherwise can be slightly worse, up to $O(H log(N) B)$ in the worst case.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint lookup\u003c/strong\u003e (membership query) is $O(D H log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRange and projection\u003c/strong\u003e queries are $O(D H log(N) + M)$ \u003cem\u003eon average\u003c/em\u003e, where $M$ is a recall size (number of vectors matching the query). In worst case tree traversal degrades to the linear scan of the entire set of vectors.\u003c/li\u003e\n\u003cli\u003eThe data structure is \u003cstrong\u003espace efficient\u003c/strong\u003e. 2 bits per LOUDS tree node + \u003cem\u003ecompressed bitmap\u003c/em\u003e of cardinal labels. For most usage scenarios, space complexity will be within \u003cstrong\u003e2x the raw bit size\u003c/strong\u003e of $str(\\lbrace \\vec{X} \\rbrace)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="functions-approximation-using-quad-trees"\u003eFunctions Approximation Using Quad Trees\u003c/h2\u003e\n\u003cp\u003eLet we have some function $y = f(x)$, and we also have the graph of this function on $[0, 1]^2$. If function $f(\\cdot)$ is elementary, or we have another way to compute it, it\u0026rsquo;s computable (for us). What if we have $f(\\cdot)$, but we want to compute inverse function: $x = f^{-1}(y)$? With compact quad trees we can \u003cem\u003eapproximate\u003c/em\u003e both functions out of the same \u003cem\u003efunction graph\u003c/em\u003e using compact quad trees:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="function.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere the tree has four layers shown upside down (drawing more detailed layers above less detailed ones). And if we want to compute $f(a)$, where $str(a) = a_3a_2a_1a_0$, the path expression will be $a_3x|a_2x|a_1x|a_0x$, where $x$ here is a \u003cem\u003eplaceholder sign\u003c/em\u003e. Now we need to traverse the tree as it defined in \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003eprevious part\u003c/a\u003e. If there is a point for $a$ of a function graph, it will be found in a logarithmic expected time. The path expression for $f^{-1}(b)$ will be $b_3x|b_2x|b_1x|b_0x$.\u003c/p\u003e\n\u003cp\u003eNote that compressed cardinal LOUDS tree will use less than 4 bits (+ some % of auxiliary data) \u003cem\u003eper square\u003c/em\u003e on the graph above. Sol, looking into this graph we already can say something specific about what will be the cost of approximation, depending on required precision (maximal $H$).\u003c/p\u003e\n\u003ch2 id="function-compression"\u003eFunction Compression\u003c/h2\u003e\n\u003cp\u003eLet we have a function that checks if some point is inside some ellipse:\u003c/p\u003e\n$$\ny = f(x_1, x_2): \\begin{cases} \n    1 \u0026\\text{if } (x_1, x_2) \\text{ is inside the the ellipse,} \\\\ \n    0 \u0026\\text{if it\'s outside.} \n\\end{cases}\n$$\n\u003cp\u003eThis function defines some \u003cem\u003earea\u003c/em\u003e on the graph. Let $N$ is the number of \u0026ldquo;pixels\u0026rdquo; we need to define the function $f(x,y)$ on a graph. Then, using compressed quad trees, we can do it with ${O(\\sqrt{N})}$ bits \u003cem\u003eon average\u003c/em\u003e for 2D space :\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="region.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe square root here is because we need \u0026ldquo;detailed\u0026rdquo; only for the border of the area, while \u0026ldquo;in-lands\u0026rdquo; needs much lower resolution.\u003c/p\u003e\n\u003ch2 id="blessing-of-dimensionality-vs-curse-of-dimensionality"\u003eBlessing of Dimensionality vs Curse of Dimensionality\u003c/h2\u003e\n\u003cp\u003eLet we have 2D space and we have a tree encoding the following structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="corners.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThere are four points in each corner of the coordinate plane. As it can be obvious from the picture, each new level of the tree will add four new bits for just for cardinal labels (not including LOUDS tree itself): three \u0026lsquo;0\u0026rsquo; and one \u0026lsquo;1\u0026rsquo; (in the corresponding corners). Now if we go to higher dimensions, for 3D we will have 8 new bits, for 8D \u0026ndash; 256 new bits and for 32D \u0026ndash; $2^32$ bits. This is \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e (CoD) for spatial data structures: volume grows exponentially with dimensions, so linear sizes \u0026ndash; too.\u003c/p\u003e\n\u003cp\u003eNevertheless, with higher dimensions, the volume is getting exponentially sparser, so we can use data compression techniques like RLE encoding to represent long sparse bitmaps for cardinal labels. This is \u003cstrong\u003eBlessing of Dimensionality\u003c/strong\u003e (BoD). For \u003cem\u003ecompressed\u003c/em\u003e LOUDS cardinal trees, the example above will require $O(D)$ bits per quadrant per tree layer for $D$-dimensional Quad Tree.\u003c/p\u003e\n\u003cp\u003eSo, the the whole idea of compression in this context is implicit (or in-place) \u003cstrong\u003eDimensionality Reduction\u003c/strong\u003e. Compressed data structure don\u0026rsquo;t degrade so fast as their uncompressed analogs, yet maintain the same \u003cem\u003elogical API\u003c/em\u003e. Nevertheless, data compression is not the final cure for CoD, because practical compression itself is not that powerful, especially in the case of using RLE for bitmap compression. So, in each practical case high-dimensional tree can become \u0026ldquo;unstable\u0026rdquo; and \u0026ldquo;explode\u0026rdquo; in size. Fortunately, such highly-dimensional data ($D \u0026gt; 16$) is rarely makes sense to work with directly (without prior dimensionality reduction).\u003c/p\u003e\n\u003cp\u003eFor example, for $D=8$ exponential effects in space are still pretty moderate (256-degree cardinal tree), yet 8 dimensions is already a good approximation of real objects in symbolic methods. High-dimensional structures are effectively \u003cem\u003eblack boxes\u003c/em\u003e for us, because our visual intuitions about properties of objects don\u0026rsquo;t work in \u003ca href="https://www.math.wustl.edu/~feres/highdim"\u003ehigh dimensions\u003c/a\u003e. Like, volume of cube is concentrating in it\u0026rsquo;s corners (because there is an exponentional number of corners). Or the volume of sphere is concentrating near its surface, and many more\u0026hellip; Making decisions in high dimensions suffer from noise in data and machine rounding, because points tend to be very close to each other. And, of course, computing Euclidian distance does not make (much) sense.\u003c/p\u003e\n\u003ch2 id="comparison-with-multi-layer-perceptrons"\u003eComparison with Multi-Layer Perceptrons\u003c/h2\u003e\n\u003cp\u003eNeural networks has been known to be a pretty good function approximators, especially for multi-dimensional cases. Let\u0026rsquo;s check how compressed spatial tree can be compared with multi-layer perceptrons (MLP). This type of artificial neural networks is by no means the best example of ANNs, yet it\u0026rsquo;s a pretty ideomatic member of this family.\u003c/p\u003e\n\u003cp\u003eIn the core of MLP is the idea of \u003cem\u003elinear separability\u003c/em\u003e. In a bacis case, there are two regions of multidimensional space that can\u0026rsquo;t be separated by a hyperplane from each other. MLP has multiple ($K$) layers, where first $K-1$ layers perform specific space transformations using linear (weights) and non-linear (thresholds) operators in such way that $K$-th layer can perform the linear separation:\u003c/p\u003e\n\n\u003cdiv style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"\u003e\n  \u003ciframe src="https://www.youtube.com/embed/k-Ann9GIbP4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eSo, this condition is simplified of the picture below. Here, we have two classes (\u003cem\u003ered\u003c/em\u003e and \u003cem\u003egreen\u003c/em\u003e dots) with complex non-linear boundary between those classes. After transforming the space towards linear separation of those classes and making inverse transformation, the initial hyperplane (here, a line) is broken in many places:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is important, is that each such like break is an inverse transformation of the original hyperplane. Those transformations has to be described, and description will take some space. So we can speak about the \u003cstrong\u003edescriptional (Kolmogorov) complexity of the decision boundary\u003c/strong\u003e. Or, in the other way, \u003cem\u003ehow many neurons (parameters) we need to encode the decision boundary\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eFrom Algorithmic Information Theory it\u0026rsquo;s known that arbitrary string $s$, drawn from a uniform distribution, will be \u003cem\u003eincompressible\u003c/em\u003e with high probability, or $K(s) \\to |s|$. In other words, most mathematically possible objects are \u003cem\u003erandom\u003c/em\u003e-looking, we hardly can find and exploit any structure in them.\u003c/p\u003e\n\u003cp\u003eReturning back to MLP, it\u0026rsquo;s expected that in \u0026ldquo;generic case\u0026rdquo; decision boundaries will be \u003cem\u003ecomplex\u003c/em\u003e: the line between classes will have many breaks, so, may transformations will be needed to describe it with required precision (and this is even not taking CoD into account).\u003c/p\u003e\n\u003cp\u003eDescribing decision boundaries (DB) with compressed spatial trees may look like a bad idea from the first glance. MLPs encode DB with superpositions of elementary functions (hyperplanes and non-linear units). Quad Trees do it with hyper-cubes, and it\u0026rsquo;s obvious that we may need a lot of hyper-cubes in place of just one arbitrary hyper-plane. If it\u0026rsquo;s the case, we say that hyper-planes \u003cem\u003egeneralize\u003c/em\u003e DB \u003cem\u003emuch better\u003c/em\u003e than hyper-cubes:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBut it should hardly be a surprise, if in a real life case it will be find out that descriptions or the network and the corresponding tree are roughly the same.\u003c/p\u003e\n\u003cp\u003eSo, Neural Networks may generalize much better in some cases than compressed quad trees and perform better in very high dimensional spaces (they suffer less from CoD because of better generalization), but trees have the following benefits:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf computational complexity of MLP is $\\Omega(W)$ (a lot of large matrix multiplications), where $W$ is number of parameters, complexity of inference in the quad tree is \u003cem\u003eroughly\u003c/em\u003e from $O(log(N))$, where $N$ is number of bits of information in the tree. So trees may be much faster than networks \u003cem\u003eon average\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eQuad Trees are dynamic. Neural Networks require retraining in case of updates, at the same time adding (or removing) an element to the tree is $O(log(N))$ \u003cem\u003eworst case\u003c/em\u003e. It may be vital for may applications operating on-line, like robotics.\u003c/li\u003e\n\u003cli\u003eQuad Trees support \u0026ldquo;inverse inference\u0026rdquo; mode, when we can specify classes (outputs) and see\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, compressed quad trees may be much better for complex dynamic domains with tight decision boundaries with moderate number of dimensions (8-24). It\u0026rsquo;s not that clear yet how trees will perform in real life applications. Memoria is providing (1) \u003cem\u003eexperimental\u003c/em\u003e compressed dynamic cardinal LOUDS tree for low dimensional spaces (2 - 64).\u003c/p\u003e\n\u003cp\u003e(1) Not yet ready at the time of writing.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eThe main point of hardware acceleration of compressed Quad Trees is that inference may be really cheap (on average). It\u0026rsquo;s just a bunch of memory lookups (it\u0026rsquo;s a \u003cem\u003ememory-bound\u003c/em\u003e problem). Matrix multipliers, from other size, are also pretty energy efficient. Nevertheless, \u003cem\u003etrees scale better with complexity of decision boundaries\u003c/em\u003e.\u003c/p\u003e\n'}).add({id:24,href:"/docs/applications/aiml/",title:"Hybrid AI",description:"",content:'\u003ch2 id="what-is-hybrid-ai-in-memoria"\u003eWhat is \u0026ldquo;Hybrid AI\u0026rdquo; in Memoria?\u003c/h2\u003e\n\u003cp\u003eHistorically, by Hybrid AI people meant something related to Khaneman\u0026rsquo;s \u003ca href="https://en.wikipedia.org/wiki/Dual_process_theory"\u003eDual process theory\u003c/a\u003e or any combination of \u0026ldquo;intuitive reasoning\u0026rdquo; (shallow, fast and wide System 1) and \u0026ldquo;symbolic reasoning\u0026rdquo; (deep, slow and narrow System 2) that are expected to \u003cem\u003ecomplement\u003c/em\u003e each other. LLMs turned out to be well-hybridizable with many different technologies, not limited to symbolic reasoners and databases. So, interests in ANNs is fueling \u003cem\u003esecondary\u003c/em\u003e interest in technologies that previously have been resting in an oblivion.\u003c/p\u003e\n\u003cp\u003eIn Memoria, the meaning of this term is slightly different (but not contradicting). Memoria project follows the intuition that there is no any specific \u0026ldquo;secret\u0026rdquo; in human intelligence in particular, and in intelligence in general: at the end of the day (after all possible \u003cem\u003eoptimizations\u003c/em\u003e have been applied) it\u0026rsquo;s all about resources \u0026ndash; \u003cem\u003ecompute\u003c/em\u003e and \u003cem\u003ememory\u003c/em\u003e. This position should not be confused with Sutton\u0026rsquo;s \u003ca href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"\u003eThe Bitter Lesson\u003c/a\u003e. While both are similar in wording and final resolutions, Memoria implies that \u003cem\u003eresources are always limited\u003c/em\u003e, and that makes huge difference: \u003cem\u003eproblem-specific optimizations do matter\u003c/em\u003e. Ultimately:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf there is some algorithm or mathematical method, or data structure that can reduce computational complexity of AI, it\u0026rsquo;s worth using.\u003c/li\u003e\n\u003cli\u003eIf there is a custom hardware architecture that can improve raw performance \u003cem\u003eand\u003c/em\u003e/\u003cem\u003eor\u003c/em\u003e performance per watt, it\u0026rsquo;s worth using.\u003c/li\u003e\n\u003cli\u003eIf there is some \u003cem\u003ephysical process\u003c/em\u003e that we can utilize to improve performance characteristics, it\u0026rsquo;s worth considering.\u003c/li\u003e\n\u003cli\u003eQuantum supremacy? Perfect!\u003c/li\u003e\n\u003cli\u003eIf we can \u003ca href="https://medium.com/@victorsmirnov/how-to-compensate-introspection-illusion-62f357e9326c"\u003eimprove our introspection\u003c/a\u003e and get some bits about inner machinery of mind that may help us to achieve better \u003cem\u003ehuman-likeness\u003c/em\u003e of AI, let\u0026rsquo;s do it!\u003c/li\u003e\n\u003cli\u003eAny useful bits form other disciplines are always welcome!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMemoria is grounded in \u003ca href="https://en.wikipedia.org/wiki/Algorithmic_information_theory"\u003eAlgorithmic Information Theory\u003c/a\u003e and it\u0026rsquo;s compression-based approach to AI. From this perspective, Systems 1 and 2 are just different \u003cem\u003ecompressibility domains\u003c/em\u003e. System 2 corresponds with highly-compressible domain, and System 1 corresponds with low-compressible domain. Traditional for programming distinction to \u003cem\u003ealgorithms\u003c/em\u003e and \u003cem\u003edata structures\u003c/em\u003e has the same nature.\u003c/p\u003e\n\u003cp\u003eThere may be many more compressibility domains than just two, so, potentially, we may have System 1\u0026hellip;N in our AI architecture, where N is pretty large. Even within the same complexity domain there are many \u003cem\u003esub-domains\u003c/em\u003e, so methods like \u003ca href="https://en.wikipedia.org/wiki/Mixture_of_experts"\u003eMixture of Experts\u003c/a\u003e and \u003ca href="https://en.wikipedia.org/wiki/Ensemble_learning"\u003eEnsemble learning\u003c/a\u003e are efficient. These methods work across even distant domains too, it\u0026rsquo;s just a technical question how to make it working efficiently. Those technical questions are in the focus of Memoria.\u003c/p\u003e\n\u003cp\u003eIn Memoria, by \u0026ldquo;Hybrid AI\u0026rdquo; it\u0026rsquo;s meant an architecture spawning multiple different compression domains.\u003c/p\u003e\n\u003ch2 id="probabilistic-lm-based-hybridization"\u003eProbabilistic LM-based Hybridization\u003c/h2\u003e\n\u003cp\u003eA probabilistic language model is simply a probability distribution over a set of strings $S$ representing texts in this language: $P(S)$, where $S = w_0w_1w_2\u0026hellip;w_i$ \u0026ndash; is a sequence of text elements (usually, \u003cem\u003etokens\u003c/em\u003e). Probabilistic models are used by \u003cem\u003esampling\u003c/em\u003e (generating elements) from them. For sampling from language models we may use \u003cem\u003eautoregressive\u003c/em\u003e schema by sampling form \u003cem\u003econditional distribution\u003c/em\u003e $P(w_i|w_{i-1}\u0026hellip;w_0)$ \u0026ndash; probability of a next element in the string given its prefix.\u003c/p\u003e\n\u003cp\u003eAutoregressive sampling means that we generate a string in an element by element, left-to-right way, each time appending newly sampled elements to the prefix. Additional techniques, like \u003ca href="https://en.wikipedia.org/wiki/Beam_search"\u003ebeam search\u003c/a\u003e, may be used to increase the probability value of the generated string. Autoregressive sampling gives us one additional important feature: we can sample strings that are \u003cem\u003econtinuations\u003c/em\u003e of a given prefix, that is called a \u003cem\u003eprompt\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe language model has to be created somehow, and the simplest way is to learn the model inductively from the set of strings drawn from the real language. There are a lot of scientific and technical challenges here, but, basically, there are three different notable approaches: statistical \u003ca href="https://en.wikipedia.org/wiki/Word_n-gram_language_model"\u003en-gram-based\u003c/a\u003e, \u003ca href="https://en.wikipedia.org/wiki/Large_language_model"\u003eNN-based\u003c/a\u003e and \u003ca href="https://arxiv.org/abs/0909.0801"\u003eVMM-based\u003c/a\u003e. In all cases we feed the mode a corpus of strings and expect it to predict those (seen) strings correctly. What we do want from the model is to predict correctly the \u003cem\u003eunseen\u003c/em\u003e strings. In ML they call it \u003cem\u003egeneralization\u003c/em\u003e. When we are solving AI problems with ML, this is where al the \u0026lsquo;magic\u0026rsquo; happens.\u003c/p\u003e\n\u003cp\u003eIt have turned out that some very large neural language models (LLM) can generalize so well over natural language that they can solve some logical and mathematical problems, follow instructions, reason about some emotions and mental states (sic!), write program code, translate from one language to another, change style of a text, summarize/elaborate and maintain conversation \u0026ndash; all from the natural language (e.g.: English).\u003c/p\u003e\n\u003cp\u003eOf course LLMs aren\u0026rsquo;t doing \u003cem\u003eeverything\u003c/em\u003e good enough (at the expert human-level), they are making a lot of hard to recognize and fix mistakes that is seriously limiting their practical suitability. They are pretty good at tasks in \u003cem\u003elow-compressible domain\u003c/em\u003e: translation, style transfer, summarization and elaboration, and some others. The reason is probably that in low-compressible domains the role of generalization isn\u0026rsquo;t that high and the model size/scale \u003ca href="https://arxiv.org/abs/2001.08361"\u003eis all that ultimately matters\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn highly compressible domains like basic math, logic puzzles, constraint solving, board games, database query execution and logical inference \u0026ndash; generalization matters, but generalizability depends on many factors. The most important of them are training \u003cem\u003edata quality\u003c/em\u003e, \u003cem\u003emodel architecture\u003c/em\u003e and \u003cem\u003elearning algorithms\u003c/em\u003e. Both model architecture and learning algorithms are fixed for neural LM. There is no way single architecture may be good for everything, one may even say that NFL theorem prohibits this. There some indirect evidence that effects behind In-Context Learning in Transformers \u003cem\u003emay\u003c/em\u003e help models to adapt to specific narrow tasks like basic arithmetic beyond what would be expected from the architecture alone. But those effects are severely limited. Basically, no amount of scaling can make a database engine out of a neural network.\u003c/p\u003e\n\u003cp\u003eActually, the latter isn\u0026rsquo;t an issue if we want to achieve HXL-AI (Human-Expert Level AI), because humans aren\u0026rsquo;t that good at symbolic tasks either. The point is that \u003cem\u003emere scaling\u003c/em\u003e of LLMs is a wrong direction. Instead, we need to identify certain domains where scaling doesn\u0026rsquo;t work but there can be different solutions, and provide custom solvers \u0026ndash; arithmetic, logical reasoning, constraint solving\u0026hellip; and so on. A relatively small but fine-tuned LLM may be used here to pre-process the input, find narrow formal problems in it, invoke corresponding solvers and then post-process the result. Text in a mixture of natural language and structured formats can be seen as \u003cem\u003eIntermediate Representation\u003c/em\u003e for this type of hybrid AI.\u003c/p\u003e\n\u003cp\u003eUsing LLMs as a human-like interface to various problem solvers greatly increases their exposure to the potential audience. Problem solvers, despite having great potential \u003cem\u003evalue\u003c/em\u003e are pretty hard to use directly.\u003c/p\u003e\n\u003ch2 id="hybrid-and-approximate-reasoning"\u003eHybrid and Approximate Reasoning\u003c/h2\u003e\n\u003cp\u003eBy \u0026ldquo;reasoning\u0026rdquo; we mean what is usually meant by \u0026ldquo;declarative problem solving\u0026rdquo;: logic (of various types), constraint solving, SAT/SMT, theorem proving, planning and many other types of combinatorial problem solving (CPS). CPS was initially meant as a main purpose of AI because of the \u003cem\u003evalue\u003c/em\u003e it creates. It literally solves problems and makes our life \u003cem\u003ebetter\u003c/em\u003e. But there are two main obstacles:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCPS is rather hard to set up and use: \u0026ldquo;you need a PhD for that\u0026rdquo;.\u003c/li\u003e\n\u003cli\u003eCPS is \u003cem\u003every slow\u003c/em\u003e. Many important problems are out of our reach.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eComplexity of CPS methods may be addressed with specialized LLMs, translating from problem description obtained in a dialogue with a user to structured problem representation. Right now (2024) it doesn\u0026rsquo;t work well in many ways, a lot of improvements of are still ahead. But at least this specific direction looks feasible and economically viable.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eComputational complexity\u003c/em\u003e of CPS is mach harder to solve issue because, basically, there is no workaround. Logic is computationally complex. If we augment an LLM with a reasoner that will be logical parts of the query, it may take practically infinite time for even apparently simple problems. Inference time is rather hard to predict.\u003c/p\u003e\n\u003cp\u003eApproximate reasoning may be useful in \u003cem\u003esome\u003c/em\u003e cases. We, humans, aren\u0026rsquo;t perfect in logic and other types of CPS either, but that\u0026rsquo;s OK. Especially if there are ways to \u003cem\u003eimprove\u003c/em\u003e a partial or approximate solution. There are two main ways to implement approximate CPS:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHeuristic (ex: greedy) methods.\u003c/li\u003e\n\u003cli\u003eTrading speed for memory.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHeuristic methods are some statistically and \u003cem\u003estatically\u003c/em\u003e inferred rules that we can use to reduce computational complexity of a CPS method and produce a \u0026ldquo;very good\u0026rdquo; result in many (but not most!) cases.\u003c/p\u003e\n\u003cp\u003eTrading speed for memory (TSM) is a large family of computational complexity reducing methods, with dynamic programming as a famous example. TSM may also be used for saving energy, if energy costs of storing and retrieving a solution is lower than costs or recomputing it.\u003c/p\u003e\n\u003cp\u003eTSM can also be viewed as a heuristic method with an \u003cem\u003eunbound\u003c/em\u003e number of heuristics, so we accumulate useful heuristics in memory as soon as they help reducing computational complexity (even at the expense of precision). Example: heuristic instance-based reasoning.\u003c/p\u003e\n\u003cp\u003eThe challenge is that the number of instances/heuristics/solutions stored in memory and their descriptional complexity may be pretty large. Specifically for that, Memoria provides highly-functional solutions: advanced data structures, query engines, possibility of hardware acceleration, integrated storage stack from bare metal to high-level computing, decentralisation and many other useful features.\u003c/p\u003e\n\u003ch2 id="associative-memory-for-llm"\u003eAssociative memory for LLM\u003c/h2\u003e\n\u003cp\u003eOne of the most well-known ways of augmenting an LLM with external memory is \u003ca href="https://arxiv.org/abs/2312.10997"\u003eRAG\u003c/a\u003e. Here, simply speaking, a prompt is translated into one or more queries to external data sources like web pages and databases. Retrieved result is summarized and returned to the user. Another way is to augment transformer with \u003ca href="https://arxiv.org/abs/2407.01178"\u003e\u003cem\u003eexplicit\u003c/em\u003e memory\u003c/a\u003e. By doing this, authors have reported significant reduction in effective model size required for the same level of performance.\u003c/p\u003e\n\u003cp\u003eImplicit or \u003cem\u003eparameters\u003c/em\u003e memory of LLMs is very expensive: computational costs are in the other of O(N) where N is a number of parameters. The number of parameters itself is quadratic from the number of structural units in attention and fully connected layers. This does not scale well, especially when a model generalizes poorly for objective reasons and needs to memorize more.\u003c/p\u003e\n\u003cp\u003eFrom other side, database technology provides searchable spatial data structures with logarithmic (on average) lookup time complexity. Memoria has specially designed \u003ca href="/docs/data-zoo/associative-memory-2"\u003eassociative memory\u003c/a\u003e \u0026ndash; multiary relation complying with Paul Smolensky\u0026rsquo;s \u003ca href="https://www.researchgate.net/publication/360353836_Neurocompositional_computing_From_the_Central_Paradox_of_Cognition_to_a_new_generation_of_AI_systems"\u003erequirements for compositional neuro-symbolic representations\u003c/a\u003e. Unlike traditional database technologies where relations link \u003cem\u003epoints\u003c/em\u003e together, associative memory links together \u003cem\u003esub-volumes\u003c/em\u003e. And a point is a special case of unit volume. Like with neural networks splitting space with hyper-planes, associative memory splits space with volumes, and infinitely many actual data points may fit into a single volume.\u003c/p\u003e\n\u003cp\u003eGiven those \u0026lsquo;hybrid\u0026rsquo; properties of Memoria\u0026rsquo;s associative memory, it\u0026rsquo;s a much better candidate for using with connectionist ML than classical graphs- and relations-based data structures (like classical RDF-like knowledge graphs).\u003c/p\u003e\n\u003cp\u003eRunning complex queries over classical relational and graph data is a costly process, both in terms of memory and compute. Querying \u0026lsquo;hybrid\u0026rsquo; advanced data structures like associative may be even more costly, because we need to use \u003cem\u003esampling\u003c/em\u003e-like algorithms for that. While it\u0026rsquo;s nothing special from algorithmic perspective, we do need a specialized hardware for achieving maximal efficiency. Memoria Acceleration Architecture (\u003ca href="/docs/overview/accel"\u003eMAA\u003c/a\u003e) may use associative memory as one of its design and performance targets.\u003c/p\u003e\n\u003ch2 id="mc-aixi-ctw"\u003eMC-AIXI-CTW\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/AIXI"\u003eAIXI\u003c/a\u003e is a theoretical mathematical formalism for artificial general intelligence. It\u0026rsquo;s a reinforcement learning \u003cem\u003eagent\u003c/em\u003e, maximizing the expected total reward received from the environment. AIXI is a clever \u003cem\u003emathematical trick\u003c/em\u003e that is based on so-called \u003ca href="https://www.lesswrong.com/posts/RKfg86eKQuqLnjGxx/occam-s-razor-and-the-universal-prior"\u003eUniversal Prior\u003c/a\u003e (UP). It\u0026rsquo;s universal, because it already contains all possible solutions for all problems packed into a format of a \u003cem\u003eprobability distribution\u003c/em\u003e. AIXI is an ultimate, universal RL-based agent, but it\u0026rsquo;s uncomputable. So, it\u0026rsquo;s not feasible in its ultimate form. Navertheless it\u0026rsquo;s a simple and elegant formalism demonstrating how very different algoritms can be get working together as a single holistic system, by reducing everything to probabilistic string prediction. Auto-regressive LLMs are also just predicting next token in a text, but a lot of \u0026lsquo;magic\u0026rsquo; implicitly happens behind the scene.\u003c/p\u003e\n\u003cp\u003eAIXI is infeasible to implement, but surprisingly it can be \u003cem\u003eapproximated\u003c/em\u003e. One of such known approximations is \u003ca href="https://arxiv.org/abs/0909.0801"\u003eMC-AIXI-CTW\u003c/a\u003e. It approximates Universal Prior with \u003ca href="https://en.wikipedia.org/wiki/Variable-order_Markov_model"\u003evariable-order markov models\u003c/a\u003e (VMM), represented as \u003cem\u003etrees\u003c/em\u003e. For unknown string probability estimations it uses \u003ca href="https://en.wikipedia.org/wiki/Context_tree_weighting"\u003eContext Tree Weighting\u003c/a\u003e method.\u003c/p\u003e\n\u003cp\u003eWhat is interesting about MC-AIXI-CTW, is that:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt\u0026rsquo;s based on a language model, backed with VMMs. Very much like with NN-bassed LLMs, intelligence is proportional to the model\u0026rsquo;s abilities to estimate probability of unknown strings correctly. This is what we call \u0026lsquo;generalization\u0026rsquo; of a model.\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s an \u003cem\u003eagent\u003c/em\u003e acting in a \u003cem\u003eenvironment\u003c/em\u003e according to some RL \u003cem\u003epolicy\u003c/em\u003e. So, ulike a raw LLM, it\u0026rsquo;s an almost-ready-to-use AI.\u003c/li\u003e\n\u003cli\u003eVMM, implemented as a tree, is \u003cstrong\u003emuch easier interpretable and hybridizable\u003c/strong\u003e than a neural network.\u003c/li\u003e\n\u003cli\u003eVMM is a \u003cem\u003edatabase\u003c/em\u003e, requiring latency-optimized architecture. And can be an interesting benchmark for \u003ca href="/docs/overview/accel/"\u003eMAA\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUnfortunately for AIXI approximations, they have been lost in the shade of DL revolution in 2010th. Now, with broad interest resurrecting to many previously forgotten AI approaches, AIXI may see its second life. What we do need here is \u003cem\u003especialized hardware\u003c/em\u003e (and related software) to accelerate this type of probabilistic models.\u003c/p\u003e\n\u003ch2 id="hardware"\u003eHardware\u003c/h2\u003e\n\u003cp\u003eNeural network is just a bunch of dense arrays \u0026ndash; pretty simple \u003cem\u003edata structures\u003c/em\u003e. Matrix multiplication generates simple and predictable memory access pattern. Computations can be easily scheduled statically ahead-of-time and at the scale of an entire cluster.\u003c/p\u003e\n\u003cp\u003eIn case of Hybrid AI we need full set of hardware architectures, optimized for \u003cem\u003estatic\u003c/em\u003e and \u003cem\u003edynamic\u003c/em\u003e parallelism, optimized for minimizing memory access latency and maximizing throughput. There is no way to provide a single capable architecture. Instead, we need a constructor to build an architecture, specialized for a specific problem class. \u003ca href="/docs/overview/accel/"\u003eMemoria Accelerated Architecture\u003c/a\u003e (MAA) is addressing this issue.\u003c/p\u003e\n\u003ch2 id="software"\u003eSoftware\u003c/h2\u003e\n\u003cp\u003eNN-oriented ML frameworks are relatively simple. Neural network is a bunch of dense arrays, the only fundamental data structure we need. We also need powerful optimizing compiler converting data-flow graph of a program into sequence of computational kernels invocation, handling alos data flow in the process. Specifics of most neural networks is that computations have highly regular and predictable data flow, so there is a lot  of opportunity for static-time optimizations, even claster-wide.\u003c/p\u003e\n\u003cp\u003eSymbolic AI, explicitly or implicitly, relies on some \u003cem\u003esearch\u003c/em\u003e technique is \u003cem\u003estate space\u003c/em\u003e. So, \u003cem\u003erepresetation\u003c/em\u003e of the state space becomes crucial. The state space may be huge and highly irrecular requiring large complex data structures and software and hardware that can leverage \u003cem\u003edynamic parallelism\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eSufficiently general \u003cem\u003ereasoning engine\u003c/em\u003e is much more complex than an advanced relational DBMS and includes it. \u003ca href="https://en.wikipedia.org/wiki/Relational_algebra"\u003eRelational algebra\u003c/a\u003e is a derivative of \u003ca href="https://en.wikipedia.org/wiki/Relational_calculus"\u003erelational calculus\u003c/a\u003e \u0026ndash; that is a tractable subset of \u003ca href="https://en.wikipedia.org/wiki/First-order_logic"\u003efirst-order logic\u003c/a\u003e (FOL). RDBMS sacrifice expressiveness of FOL for predictable performance, they also lose the \u003cem\u003edeductive\u003c/em\u003e component \u0026ndash; ability to infer \u003cem\u003enew facts\u003c/em\u003e from \u003cem\u003eexisting ones\u003c/em\u003e. Reasoning engines may generate large intermediate state and/or results and operate on large datasets, so building it on top of a powerful and generic query engine is essential.\u003c/p\u003e\n\u003cp\u003eMemoria Framework provides necessary basic and advanced elements for building standalone and hybrid reasoning engines. The main elements of the stack are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e data format, deeply ontegrated with the rest of the framework, including hardware acceleration at the level of MAA if/when necessary. Hermes is flexible enough to support all the necessary structured formats like knowlege graphs, no thierd-party libraries are needed.\u003c/li\u003e\n\u003cli\u003eHermes-based \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e accelerated communication protocol.\u003c/li\u003e\n\u003cli\u003eRich set of trivial and advanced \u003ca href="/docs/overview/containers"\u003edata structures\u003c/a\u003e. No external database or storage system is ever needed.\u003c/li\u003e\n\u003cli\u003eAccelerated query \u003ca href="/docs/overview/vm"\u003eexecution engine\u003c/a\u003e. Turing complete superset of SQL/Datalog and beyond. Backward chaining mode (query execution) and \u003ca href="https://en.wikipedia.org/wiki/Rete_algorithm"\u003eRETE\u003c/a\u003e-based forward-chaining engine for event-driven computations: robotics, agents, embedded systems and \u003ca href="/docs/applications/eiot"\u003eIoT\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eComputational \u003ca href="/docs/applications/storage"\u003estorage layer\u003c/a\u003e. Memoria fully \u003ca href="/docs/overview/accel"\u003eredefines\u003c/a\u003e storage and processing stacks comparing to a traditional ones, based on CPUs and monolithic operating systems with integrated complex storage layers (file systems). Complex distributed and heterogeneous architectures become much simpler.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="beyond-reasoning"\u003eBeyond Reasoning\u003c/h2\u003e\n\u003cp\u003eLLMs are considered to be \u003cem\u003elanguage\u003c/em\u003e models. Language models trained on human-generated textual data are \u003cem\u003emuch more\u003c/em\u003e than just that. They are capturing not only the language itself, but also \u003cem\u003efunctional approximations\u003c/em\u003e of higher mental functions, including \u003cem\u003eagency\u003c/em\u003e. Conversation with a sufficiently powerful and properly-tuned LLM looks and feels like a conversation with a real well-educated person with encyclopedic knowledge. Apparent human-likeness of conversations with LLMs is impressive, especially when we take into account implicit functional aspects of it (intuitive understanding), but it should not be deceiving. LLMs do not \u003cem\u003efeel\u003c/em\u003e or really \u003cem\u003eexperience\u003c/em\u003e what they say \u003cem\u003ethe way we do\u003c/em\u003e: generative processes in LLMs are sufficiently different from ours. We should not attribute any of our \u003cem\u003emental states\u003c/em\u003e to them (we may do it, but only with great precaution).\u003c/p\u003e\n\u003cp\u003eDespite \u0026lsquo;mental states\u0026rsquo; of LLMs (if any) are very different from ours, their \u003cem\u003eobservable effects\u003c/em\u003e are consistent with corresponding effects of our mental states (providing that there is enough training data). This is \u003cem\u003ethe\u003c/em\u003e reason why it \u003ca href="https://www.scientificamerican.com/article/google-engineer-claims-ai-chatbot-is-sentient-why-that-matters/"\u003efeels so human-like\u003c/a\u003e in conversations. Certain higher mental functions are \u003ca href="https://en.wikipedia.org/wiki/Hard_problem_of_consciousness"\u003eknown to be really hard\u003c/a\u003e to formalize, they are nevertheless very important for human-machine integration. This entire topic has been considered largely uninteresting in AI/ML community. But recent second dawn of LLM-based \u003ca href="https://en.wikipedia.org/wiki/Multi-agent_system"\u003emulti-agent systems\u003c/a\u003e have brought old question to the table again. There is much more to human cognition than just \u0026lsquo;reasoning\u0026rsquo; and \u0026lsquo;learning\u0026rsquo;, that is may be just 1% of all mental activity related to the problem solving. If we want our computational agents be integrateable into human-centric interactions, or humans be integrable into societies of computational agents, both parties need to \u003cem\u003eunderstand\u003c/em\u003e each others at the emotional, intuitive, unconscious levels.\u003c/p\u003e\n\u003cp\u003eCapturing mental states and higher mental functions in LLMs via machine learning has been already proven efficient, but the real problem is that internal state of black box ML models is hardly interpretable in terms of external objects those models interact with. LLMs work, until they don\u0026rsquo;t. And if they don\u0026rsquo;t, there is no specific way to fix it. Moreover, textual datasets are extremely \u003ca href="https://en.wikipedia.org/wiki/Skewness"\u003eskewed\u003c/a\u003e at textual descriptions of mental states and higher mental functions, seriously limiting LLM\u0026rsquo;s abilities to reason about them. Mental states describing external \u003cem\u003eobjects\u003c/em\u003e are very well represented in textual data. The same is true for certain emotional state, but now their descriptions depend on some context, introducing \u003cem\u003esubjectivity\u003c/em\u003e into interpretations.\u003c/p\u003e\n\u003cp\u003eCertain important mental states have no textual expression at all. They may be not even \u003cem\u003ereportable\u003c/em\u003e. One of foundational questions that may trick and freak programmers is \u0026ldquo;How do \u003cem\u003eyou\u003c/em\u003e write programs?\u0026rdquo;. Only the most experienced programmers notice that the very process of program creation is not really \u003cem\u003eaccessible\u003c/em\u003e. Less experienced programmers may say something about methodologies and philosophy. But these things are just reflections on generalisations of the process, not how actual mechanisms of writing a program emerge in our heads. The same is true for any other types of \u003cem\u003ereasoning\u003c/em\u003e (and accessible mental activity in general): there is a lot of an inaccessible (or \u003cem\u003eintuitive\u003c/em\u003e) component of the process happening in the background. Getting this access may be the key to making our thinking processes efficient and our ML models economically viable.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s yet to be proven that reducing skewedness of datasets by enriching them with textual descriptions of inner mental processes (intermediate cognitive material) will improve performance of language models in general and agent-related tests. Nevertheless, it\u0026rsquo;s a solid and grounded intuition because it works this way for humans. In order to get this data from our minds, we need to develop \u003ca href="https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences#Intrapersonal"\u003eintrapersonal intelligence\u003c/a\u003e \u0026ndash; the ability to understand yourself, including your thoughts, feelings, motivations, and fears, and to use that understanding to make decisions and communicate \u0026ndash; in a way that is compatible with AI. Figuratively speaking, we need to understand our \u0026lsquo;inner machine\u0026rsquo; and describe it in terms of algorithms and data structures expressed as scripts for agents.\u003c/p\u003e\n\u003cp\u003eThere is an ongoing \u003ca href="https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md"\u003emultidisciplinary research process\u003c/a\u003e around Memoria targeting unification of \u003cstrong\u003eintrapersonal intelligence\u003c/strong\u003e concept from Psychology (Howard Gardner) with Philosophy of Mind, Physiology, Mathematics and Computer Science. The goal is to build a conceptual bridge between first-person experience, first-person self-psychology and computations in the form of minimalist computational models of fundamental higher mental functions like \u003cem\u003eObserver\u003c/em\u003e and \u003cem\u003ebeingness\u003c/em\u003e, \u003cem\u003efeelings\u003c/em\u003e, \u003cem\u003eintuition\u003c/em\u003e and many others.\u003c/p\u003e\n\u003cp\u003eInrapersonal intelligence has been proven to play crucial role in many aspects of general human intelligence: self-awareness and emotional regulation, motivation and goal-setting, critical thinking and reflection, problem-solving and decision-making, personal growth and adaptability, interpersonal relationships. These and many others aspects of intrapersonal intelligence can positively impact overall cognitive abilities and personal success. The basic idea is enhance it with advanced computational theories of mind that is expected to extend conscious access and self-control, because much more complex mind states become accessible and interpretable. The Hard problem of consciousness isn\u0026rsquo;t that hard if we can see our \u0026lsquo;inner machine\u0026rsquo;. Seamless integration with The Machinekind is also much simpler this way.\u003c/p\u003e\n\u003cp\u003eOn the HW/SW side of things insights form intrapersonal intelligence are expected to give new advanced algorithms and data structure, integrated circuits, programming language concepts, software development and collaboration patterns and many other cool things. Memoria Framework will be adopting those things once they are available. The first candidate of the road is the concept of \u003ca href="https://github.com/victor-smirnov/digital-philosophy/blob/master/Artificial%20Intelligence.md#self-referential-machine"\u003eself-referential Turing Machine\u003c/a\u003e implemented with \u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="memoria-as-a-dataset"\u003eMemoria as a Dataset\u003c/h2\u003e\n\u003cp\u003eLLMs create a lot of opportunities for opensource software. A lot of code is currently unmaintained and forgotten. OSS authors struggles looking for users attention to their projects. If this code is used for training LLMs it will be used. LMM this this sense create completely new opportunity for idealistically-motivated authors. Even if their works do not have sufficient \u003cem\u003edirect\u003c/em\u003e audience right now, \u003cem\u003ethe message\u003c/em\u003e hidden in thier works will live in the models trained on these works.\u003c/p\u003e\n\u003cp\u003eMemoria, as a project, does recognize this opportunity to provide not only direct software artifacts, but also indirect generative architectural patters if/when the project is used for LLM training. Memoria itself is going to use this pattern for automation of the project evolution. More details of this later\u0026hellip;\u003c/p\u003e\n'}).add({id:25,href:"/docs/applications/storage/",title:"Computational Storage",description:"",content:'\u003ch2 id="basic-information"\u003eBasic Information\u003c/h2\u003e\n\u003cp\u003eComputational storage device or \u003cstrong\u003eCSD\u003c/strong\u003e is a computer device which primary function is data storage, but that can also perform certain data processing functions. Functions can be either pre-configured or user-provided, both offline and online.\u003c/p\u003e\n\u003cp\u003eThere are two main reasons one may need a CSD.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt can significantly reduce computational system design complexity by eliminating dependency from CPU and operating system. CSD may perform all essential storage functions like object space management, concurrency and transactions on-device. Accelerators may access CSDs directly over PCIe using database-like message-based protocols instead of transferring raw blocks of data (as in case of block storage devises).\u003c/li\u003e\n\u003cli\u003eIt can significantly improve power outage and other types of failures handling by tightly coupling essential low-low level storage functions with device hardware. By controlling essential functions, device manufactures may \u003cem\u003eco-desing\u003c/em\u003e those functions and storage hardware.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCounter-intuitively, \u003cem\u003espeed\u003c/em\u003e isn\u0026rsquo;t considered here to be the main argument for using a CSD. Tight coupling of memory and compute will always give speed advantage. The problem is that storage media like NAND Flash doesn\u0026rsquo;t like \u003cem\u003eheat\u003c/em\u003e produced by compute hardware. Putting massive radiators on devices will decrease storage density. As a result, intensive compute functions have to be disaggregated from CSDs into separate storage accelerators which are easier to cool.\u003c/p\u003e\n\u003cp\u003eFortunately, one of the most important class of queries that benefit from using CDS, \u003cem\u003epoint-like queries\u003c/em\u003e, do not produce a lot of heat.\u003c/p\u003e\n\u003cp\u003eNote that NAND Flash has pretty high access latency, that is much higher than PCIe latency. On the random object access CSD may show only a little performance improvements over a conventional block-based SSD.\u003c/p\u003e\n\u003ch2 id="simple-example-persistent-queue"\u003eSimple Example: Persistent Queue\u003c/h2\u003e\n\u003cp\u003eQueues are very important building block in event-driven distributed computing. Persistent queue saves incoming (pushed) messages on a durable media, removing them when they are being popped. In case of power failure, after restart system is restarted, may continue working because all in-flight messages are still in-light.\u003c/p\u003e\n\u003cp\u003eNormally, we need a CPU, a network device (NIC) and a block storage device like SSD. NAND Flash-based SSDs \u003cem\u003emay be\u003c/em\u003e a write bottleneck in this architecture because because flash has pretty high access latency \u0026ndash; 10th os microseconds. So we need to group and amortize writes in a \u003cem\u003efast\u003c/em\u003e nvRAM buffer.\u003c/p\u003e\n\u003cp\u003eBut anyway, block devices operate via transferring \u003cem\u003eblocks\u003c/em\u003e and just to transfer 4K block over 4GB/s PCIe connection we need 1 microsecond, not including other latencies. And we probably need many of them to transfer \u003cem\u003eto and from\u003c/em\u003e RAM to commit a single \u003ccode\u003epush()\u003c/code\u003e operation.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="/docs/applications/storage/queue-bd.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIf we want \u003ccode\u003epush()\u003c/code\u003e latency for a message to be within 5 microseconds over Ethernet, we need a optimize the whole data path. There are NIC-CPU latencies, including \u003cem\u003eOS kernel\u003c/em\u003e (that isn\u0026rsquo;t that large, but nevertheless).\u003c/p\u003e\n\u003cp\u003eThe largest bottleneck is on the CPU-BlockDev side, because we need to transfer a lot of data, and each 4K block costs us 1 microsecond. PCIe latency on an ordinary (non CXL-optimized) platforms can easily be also about 1 microsecond. So each interaction with drive will be taking time.\u003c/p\u003e\n\u003cp\u003eThe obvious idea is to move the queue storage layer directly into the storage device, connected directly to the network device over a dedicated PCIe link. And let the device handle the queue completely.\u003c/p\u003e\n\u003cp\u003eThis wat we can save a lot of unnecessary block transfers and a lot of PCIe round-trips.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="/docs/applications/storage/queue-smart-stor.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eDedicated PCIe link can be pretty fast, with latency well under 1 microsecond, so we can safely assume that \u003ccode\u003epush()\u003c/code\u003e latency can be withing this number as well. Computational Network Device may handle networking part of the service, sending prepared data to the CSD. What is cool about this design that CPU is no more the bottleneck. Such queue may achieve very high message rates with extremely low latencies, bounded only be the network itself.\u003c/p\u003e\n\u003cp\u003eThe main difference in this design is using message-oriented \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e protocol instead of block-oriented NVMe to communicate with CSD.\u003c/p\u003e\n\u003cp\u003eLatest generations of SSD controllers allow running Linux with applications, so running a persistent queue software or a \u003cem\u003edatabase\u003c/em\u003e software is not an issue. But Memoria can do \u003cem\u003eeven better\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="execution-stack"\u003eExecution Stack\u003c/h2\u003e\n\u003cp\u003eMemoria has a pretty common stack of \u003cem\u003eessential\u003c/em\u003e abstractions for data-focused applications: processing layer (\u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e), data layer (\u003ca href="/docs/overview/containers"\u003eContainers\u003c/a\u003e) and storage layer (\u003ca href="/docs/overview/storage"\u003eStorage engines\u003c/a\u003e). Layers are perfectly isolated via virtual interfaces (can be compiled independently). HRPC-based interfaces provide binary compatibility.\u003c/p\u003e\n\u003cp\u003eThe whole point of a CSD is to execute queries as close to the data as possible. Given fundamental power/heat constraints of memory media, it\u0026rsquo;s not reasonable to run \u0026lsquo;heavy\u0026rsquo; queries right on the device. Ideally, CSD should support a mode of operation/API when \u003cem\u003eraw data\u003c/em\u003e ca be exported to be processed on an external powerful accelerator.\u003c/p\u003e\n\u003cp\u003eBelow is the diagram explaining it in greater details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="/docs/applications/storage/io-stack.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere the stack has two parts: HRPC Gateway (endpoints and services) and Memoria Essentials are open and provided by the Memoria Framework. Hardware-dependent part (low-level aspects of the Store\u0026rsquo;s implementation and Hardware Abstraction Layer) may be open, but may be a closed vendor-specific hardware abstraction.\u003c/p\u003e\n\u003ch2 id="controller"\u003eController\u003c/h2\u003e\n\u003cp\u003eTechnically, we don\u0026rsquo;t need a custom designed controller to run the execution stack. Any decent \u003cem\u003e64-bit\u003c/em\u003e SSD controller supporting embedded or external nvRAM should be enough. Custom chip can be equipped with accelerators so we can do \u003cem\u003emuch more processing\u003c/em\u003e within CSD\u0026rsquo;s allowed power budget.\u003c/p\u003e\n\u003cp\u003eThe idea is to use the same HW/SW architecture as outlined in \u003ca href="/docs/overview/accel"\u003ethis document\u003c/a\u003e, adapted to specific requirements and constraints of computational storage devices. The controller consists of a cluster (grid, hypercube) of RISC-V based \u003ca href="/docs/overview/accel#processing-element"\u003eprocessing elements\u003c/a\u003e (xPU) equipped with Memoria-specific ISA extensions accelerating algorithms and data structures.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="/docs/applications/storage/accelerator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAccelerator\u0026rsquo;s architecture is \u0026lsquo;standard\u0026rsquo; (unified). The only difference is problem-specific blocks (NAND Flash controllers) and, possibly, some other functions.\u003c/p\u003e\n\u003cp\u003eThe main featurs of this controller\u0026rsquo;s architecture are scalability and extensibility. We can add more processing elements if we want faster on-device query processing. Third-parties may add their own processing elements as long as they support HRPC interfaces of the accelerator.\u003c/p\u003e\n\u003cp\u003eOne notable feature of this CSD architecture is that it does not need an \u003ca href="https://en.wikipedia.org/wiki/Flash_memory_controller"\u003eFTL\u003c/a\u003e typical to block-based SSDs, that is usually provisioned at the rate of 1GB DRAM per 1TB NAND. All the memory in the system may be used for running queries.\u003c/p\u003e\n\u003ch2 id="storage-engines"\u003eStorage Engines\u003c/h2\u003e\n\u003cp\u003eMemoria currently has two storage engines for \u003cem\u003eblock devices\u003c/em\u003e (BD) \u0026ndash; SWMRStore and OLTPStore, both supporting single serialized writer but multiple concurrent readers (SWMR), all in a wait-free mode. Both are transactional and support flexible commit policy. The biggest difference between them is that OLTPStore does not use block reference counters for memory management and is much faster for write transaction. But it does not support explicit version history because of that (there are other limitations too). BD-oriented storage engines need special \u003cem\u003eblock allocator\u003c/em\u003e. Notable feature of  SWMR storage engines in Memoria is that block allocator is transactional and has logarithmic worst-case allocation complexity.\u003c/p\u003e\n\u003cp\u003e\u003ca href="/docs/overview/storage/#nandstore"\u003eNANDStore\u003c/a\u003e is a version of OLTPStore, optimized for NAND Flash and \u003ca href="https://zonedstorage.io/docs/introduction/zns"\u003eZNS\u003c/a\u003e models of operation. The idea is that this storage engine is stacked on top of HRPC interfaces provided by the \u003cem\u003eLow Level Store/HAL\u003c/em\u003e of the CSD (see diagram above).\u003c/p\u003e\n\u003cp\u003eNote that SWMRStore can also be used inside a CSD, there are no technical restrictions for that. Moreover, it\u0026rsquo;s preferable for analytical (read-optimized) applications that benefit the most from using CSDs. Just right now it\u0026rsquo;s not a priority.\u003c/p\u003e\n\u003ch2 id="usage"\u003eUsage\u003c/h2\u003e\n\u003cp\u003eThere are three main features of Memoria-based CSDs:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAllows running \u003cem\u003euser-supplied queries\u003c/em\u003e in a secure, sandboxed environment on-device.\u003c/li\u003e\n\u003cli\u003eMuch better failure recovery guarantees (including transactional durability), comparing to block-based storage devices like SSDs and HDDs, because CSD manufactures may control the entire \u003cem\u003ecritical data path\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eWe don\u0026rsquo;t need a separate, dedicated CPU to use them. CSD can be connected directly to an accelerator via a low-latency dedicated link, or to a fabric. Computational architecture becomes much more \u003cem\u003edistributed\u003c/em\u003e (no \u003cem\u003eCPU\u003c/em\u003e).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsing CSD is straightforward. CSD provide HRPC message-based streaming interface, so any HRPC-enabled infrastructure may discover and use these resources as usual. From a functional perspective, working with CSD may look very much like working with a multimodel database via network connection.\u003c/p\u003e\n\u003cp\u003eIntegration of CSDs as a storage devices into existing OS and applications is trickier. There is no technical issues in providing either block device interface (to run regular FS) on top of CSD, or to provide a regular FUSE/NFS-like remote interface to data as a virtual filesystem.\u003c/p\u003e\n'}).add({id:26,href:"/docs/applications/db/",title:"Converged Databases",description:"",content:'\u003cp\u003eDatabase = \u003cem\u003estorage\u003c/em\u003e + \u003cem\u003etype system\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe problem is that storage and type system must be co-designed. In regular programming we rely on the composability principle for build our solutions out of reusable building blocks that can be designed independently. Databases are very different: data structures (containers in Memoria), in general, are not composable under concurrent updates. We need transactions. Implementing transactions is tricky, we probably need to do it on a per-container basis (one way for tables, another way for indices, blobs, graphs, etc\u0026hellip;)\u003c/p\u003e\n\u003cp\u003ePolyglot persistence approach, although appealing in the light of the Unix Way, isn\u0026rsquo;t the answer. Tying many \u003cem\u003edifferent\u003c/em\u003e storage and query engines together will be a development and maintenance nightmare: linking parts of related entities in different storage engines, orchestrating transactions, handling data movements, backup-restore, software updates and upgrades, etc\u0026hellip;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConverged database\u003c/strong\u003e (here, CDB) supports all our needed data container types, indices, advanced data structures, data formats and languages and data processing paradigms (batching, streaming, top-down, bottom-up, etc\u0026hellip;) in one system. Polyglot persistence may be needed, but, \u003cem\u003eideally\u003c/em\u003e, only for interaction with existing and legacy systems. Converged databases aren\u0026rsquo;t omnipotent. They may lack certain data operation modes that don\u0026rsquo;t fit easily into their narrative. So specialized databases still keep their place under the Sun, they can be just much better optimized for the narrow set of functions they perform.\u003c/p\u003e\n\u003cp\u003eMemoria itself isn\u0026rsquo;t a database, it\u0026rsquo;s a frameworks for building database engines (among other things) out of provided reusable building blocks. It\u0026rsquo;s closer to Erlarg \u0026amp; Mnesia (and actually was partially influenced of this platform\u0026rsquo;s system design) than to PostgreSQL, that is moving in the direction of converged databases. True CDB should not be a traditional black box database engine accessible via uni-directed protocol like JDBC, because no size fits all. Rather, it should be a framework, platform or database engine deeply embedded into programming language (or vice versa). The main goal is that database \u003cem\u003emay not limit programmers\u003c/em\u003e in what they can imagine doable. Existing databases do, and do it badly.\u003c/p\u003e\n\u003cp\u003eWell, Memoria has the following subsystems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e: Storage-optimized messaging data format and data modeling stack (including low-level semantic graphs).\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e: high-level bi-directional RPC-like communication protocol\u003c/li\u003e\n\u003cli\u003e\u003ca href="vm"\u003eDSLEngine\u003c/a\u003e: (work in progress) multi-paradigm program execution engine, supporting convenient control-flow code execution, data-flow programs, generalized relational algebra, integrated RETE-based forward chain rule engine and backward-chaining Datalog-based engine (unified with all other engines), potentially accelerated with \u003ca href="/docs/overview/accel"\u003eMAA\u003c/a\u003e (once it\u0026rsquo;s available, WIP)\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/containers"\u003eContainers\u003c/a\u003e: containers for highy-structured arbitrary sized data, support copy-on-write semantics for concurrency, parallelism, versioning and transactions.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/storage"\u003eStorage Engines\u003c/a\u003e: based on Copy-on-Write principles. Storage is completely separated from containers via simple but efficient contracts. Out of the box, OLTP-optimized and HTAP-optimized storage, as well as In-Memory storage options, are provided, supporting advanced features like serializable transactions and Git-like branching and versioning.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following diagram demonstrates a reference database engine architecture consisting from these subsystems:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="architecture.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eMemoria is departing from CPU-centric architectures, adopting more distributed-systems-like approach even at the scale of a single system. This approach relies heavily on the concurrent properties of persistent data structures (wait-free access on the data path). They do have some runtime overhead comparing to good-old ephemeral data structures, but when you have hundreds and thousands of cores accessing shared data concurrently, there is no that many alternatives.\u003c/p\u003e\n\u003cp\u003eMemoria is targeting problems falling into the \u003cem\u003elatency-sensitive\u003c/em\u003e/\u003cem\u003edynamic parallelism\u003c/em\u003e category (databases, reasoners, solvers, etc\u0026hellip;), where memory access and code execution patters are hardly predictable. Currently, it\u0026rsquo;s being served buy multicore OoOE CPUs with large caches and complex memory hierarchies. Instead, Memoria will be exploring in-memory computing \u003ca href="/docs/overview/accel"\u003eapproach\u003c/a\u003e with small cores are placed \u003cem\u003ecloser\u003c/em\u003e to the data they process.\u003c/p\u003e\n\u003cp\u003eThe following diagram demonstrates an example of a \u003cem\u003edistributed\u003c/em\u003e Processing In-Memory (PIM)/Near-Memory (PNM) system architecture, built on top of a CPU-centric one:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="comp-arch.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eFinally, as a database engine framework, Memoria will be relying on \u003ca href="/docs/applications/storage"\u003ecomputational storage devices\u003c/a\u003e (CSD) concept, once these devices are available (they are WIP and high priority). CSDs will significantly reduce \u003cem\u003ecomplexity\u003c/em\u003e and improve reliability of data storage by coupling essential Memoria algorithms with low-level aspects of NAND (and other types of media) storage.\u003c/p\u003e\n'}).add({id:27,href:"/docs/applications/co-design/",title:"HW/SW Co-design",description:"",content:'\u003cp\u003e\u003cstrong\u003eThis page describes functionality or feature that may not necessary exist at the time of writing. HW/SW co-design is meant to be a \u003cem\u003eshowcase\u003c/em\u003e of Memoria as an application development framework, it will be gradually available when it\u0026rsquo;s ready.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id="basic-information"\u003eBasic information\u003c/h2\u003e\n\u003cp\u003eBy hardware/software co-design we mean a system design process when, given some functionality, we have a choice to implement it in hardware or in software. Actually, there are not that much of substantial difference between those two options, the problem is that corresponding engineering practices are \u003cem\u003every different\u003c/em\u003e. Co-design framework aims to align them to each other and, if possible, unify into a single seamless \u003cem\u003esuper-process\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eWe have practically successful examples of existing co-design frameworks \u0026ndash; neural networks-oriented \u003cem\u003eopensource\u003c/em\u003e ML frameworks (PyTorch, Tensorflow and others) support using custom designed hardware to accelerate computational graphs. Independent hardware manufactures can adapt their hardware to the ways how ML framework operate, and ML frameworks \u003cem\u003eusers\u003c/em\u003e may design custom hardware to accelerate their \u003cem\u003especific\u003c/em\u003e dataflow graphs.\u003c/p\u003e\n\u003cp\u003eMemoria follows the same paradigm: it defines formal computational environment, protocols, data structures and their implementations, domain-specific languages and compilers, but is focused on a different class of computations than existing frameworks \u0026ndash; on \u003cem\u003elatency-sensitive\u003c/em\u003e computations and on \u003cem\u003ememory parallelism\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eHigh-level computation model in Memoria is \u003ca href="https://en.wikipedia.org/wiki/Event-driven_programming"\u003eevent-driven computations\u003c/a\u003e. There are three corresponding \u003cem\u003ehardware domains\u003c/em\u003e in the focus of Memoria\u0026rsquo;s:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="/docs/overview/accel/tri-arch.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cul\u003e\n\u003cli\u003eGeneric mixed \u003cstrong\u003eDataFlow\u003c/strong\u003e and \u003cstrong\u003eControlFlow\u003c/strong\u003e computations. A lot of \u003cem\u003epractical\u003c/em\u003e compute- and IO-intensive applications, that may be run either on CPUs or on specialized hardware, fall into this category.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntegrated Circuits\u003c/strong\u003e for fixed (ASIC) and reconfigurable logic (FPGA, Structured ASIC). May be used for high performance \u003cem\u003eand\u003c/em\u003e low power stream/mixed signal processing part of application, giving ability to handle events with a nanosecond-scale resolution.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRule/search-based\u003c/strong\u003e computations in a \u003cem\u003eforward chaining\u003c/em\u003e (Complex Event Processing, Streaming, Robotics) and \u003cem\u003ebackward chaining\u003c/em\u003e (SQL/Datalog databases) forms.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFunctions in hardware domains are connected with a unified hardware-accelerated RPC+streaming communication protocol, \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e, allowing intra- and cross-domain seamless communication. HRPC is very much like gRPC (that is used to communicate with services in distributed computing tasks), but optimized for direct hardware implementation.\u003c/p\u003e\n\u003cp\u003eGeneralized computational model in Memoria is \u003cem\u003edata-flow\u003c/em\u003e over decentralized \u003cem\u003epersistent data structures\u003c/em\u003e. Data-flow can be either a plain low-level tokenized one or high-level complex-event-driven \u003ca href="https://en.wikipedia.org/wiki/Rete_algorithm"\u003eRETE-based\u003c/a\u003e one.\u003c/p\u003e\n\u003cp\u003eAs a \u003cem\u003eco-design framework\u003c/em\u003e, Memoria defines two main design \u003cem\u003eextension points\u003c/em\u003e:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCustom hardware functions attached via bridges to an HRPC router and visible in the system as HRPC endpoints. This is \u003cem\u003eloose coupling\u003c/em\u003e design pattern.\u003c/li\u003e\n\u003cli\u003eCustom ISA extensions for a \u003ca href="/docs/overview/accel/#processing-element"\u003eRISC-V reconfigurable processing unit\u003c/a\u003e. This is \u003cem\u003etight coupling\u003c/em\u003e design pattern.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLoose coupling of functionality simplifies compositionality and \u003cem\u003edistribution\u003c/em\u003e (of compute within a memory architecture, for example). Tight coupling pattern allows \u003cem\u003econcentration of compute\u003c/em\u003e where necessary (matrix multipliers for ANN/HPC).\u003c/p\u003e\n\u003cp\u003eAt the software level, Memoria is also a \u003cem\u003esoftware development framework\u003c/em\u003e (different one) for data intensive applications like data platforms, storage systems, networking and IoT/embedded. The whole idea is that by following Memoria-defined protocols hardware developers my gain immediate access to a pretty complex software ecosystem.\u003c/p\u003e\n\u003cp\u003eCo-design part of the framework consists from the following components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e and \u003ca href="/docs/overview/hrpc"\u003eHRPC\u003c/a\u003e libraries and tools, this is also a part of Memoria Core module. Core can be used independently from the rest of Memoria.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/vm"\u003eDSLEngine\u003c/a\u003e provides runtime-efficient system-wide \u003cem\u003eIntermediate Representation\u003c/em\u003e of the code model for each \u003cem\u003ehardware domain\u003c/em\u003e. IC domain may be lowered to \u003ca href="https://circt.llvm.org/"\u003eCirct\u003c/a\u003e (preferable way).\u003c/li\u003e\n\u003cli\u003eClang-based \u003ca href="https://github.com/victor-smirnov/jenny"\u003eJenny C++ compiler\u003c/a\u003e and associated tools targeting CF/DF/Rule domains and custom RISC-V kernels. Note that regular Memoria applications don\u0026rsquo;t need a dedicated compiler. Any compliant C++20 compiler should be OK. Clang is preferable.\u003c/li\u003e\n\u003cli\u003eBuild and automation tools. Generic programmable event-driven build system, available both in command line and via HRPC API. HRPC may have JSON and gRPC bindings for compatibility reasons.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="special-features"\u003eSpecial features\u003c/h2\u003e\n\u003cp\u003eAs a special feature, Memoria provides data containers for \u003ca href="/docs/data-zoo/compressed-symbol-seq"\u003ecompressed searchable sequences\u003c/a\u003e with alphabets from 1 to 8 bits. These data structures are good for capturing digital waveforms for further analysis:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLarge alphabet is needed when signals may have more than 2 states.\u003c/li\u003e\n\u003cli\u003eSearchbility is needed for locating specific patterns in the signal.\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:28,href:"/docs/applications/eiot/",title:"Embedded and IoT Applications",description:"",content:'\u003ch2 id="current-status"\u003eCurrent Status\u003c/h2\u003e\n\u003cp\u003eMemoria is a pretty heavyweight framework and most likely isn\u0026rsquo;t suitable for embedded and IoT applications running on micro-controllers with little memory onboard. Some parts of the framework, like lightweight Hermes profiles can be scaled down to such MCUs and used for data storage and communication purposes.\u003c/p\u003e\n\u003cp\u003eAnother limitation, that is not fundamental, is that Memoria assumes 64-bit hardware. Compiling it for 32-bit hardware is possible, but ensuring that everything works correctly and there are no performance regressions is not currently a priority. Anyway, \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e-level interoperability with 32-bit platforms is a priority.\u003c/p\u003e\n\u003cp\u003eWe assume that eventually (sooner than later) 64-bit hardware starts prevailing even in the embedded domain, because it\u0026rsquo;s easier to generalize the  software when all essential platforms have the same width.\u003c/p\u003e\n\u003cp\u003eMemoria currently can be run on top of Linux so any Linux-capable embedded platform is technically capable of running Memoria. We can even prototype computational storage devices out of single-board computers and off-the-shelf SSDs.\u003c/p\u003e\n\u003ch2 id="perspectives"\u003ePerspectives\u003c/h2\u003e\n\u003cp\u003eThere are several directions where Memoria may be moving in the context of \u003ca href="/docs/overview/accel"\u003eMAA\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHRPC-enabled 64-bit RISC-V MCU, based on MMA\u0026rsquo;s \u003ca href="/docs/overview/accel/#processing-element"\u003exPU\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003exPU runs computational \u003cem\u003ekernels\u003c/em\u003e \u0026ndash; \u003cem\u003ecode modules\u003c/em\u003e loadable via HRPC protocol from code loading \u003cem\u003eendpoints\u003c/em\u003e (similar to Java\u0026rsquo;s classloaders) available somewhere in HRPC \u003cem\u003einfrastructure\u003c/em\u003e the MCU is attached to. HRPC is transport-agnostic, so even a pair of an ordinary serial (SPI?) lines can serve its traffic.\u003c/p\u003e\n\u003cp\u003eSuch an HRPC-enabled MCU can be attached to other potential HRPC-enabled devices, like \u003ca href="/docs/applications/storage"\u003ecomputational storage\u003c/a\u003e (in a variant suitable for embedded uses) or DRAM modules with in-memory computing capabilities, either local or \u003cem\u003eremote\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eReference HDL/HCL implementation for such an MCU is a priority for Memoria.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMMA accelerator downscaled to requirements embedded system (like, 15W total system power draw).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are two directions that are currently in mind for such accelerators.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFirst\u003c/strong\u003e, computational storage devices in the format of a single chip. They may provide rich storage and in-storage processing functions together with advanced features like transactions and nearly-perfect power outage resilience. Factoring out storage and data processing functions into a dedicated device may dramatically simplify design of an embedded system at the expense of single additional chip (that is usually an SPI flash anyway).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSecond\u003c/strong\u003e, DSLEngine accelerators for embedded systems. DSLEngine is a database/stream/complex-event-processing engine that can be useful for handling various complex (composite) events in IoT applications and process controllers. RETE algorithm is useful but is pretty memory-hungry, so specialized device capable of running it may be welcome here.\u003c/p\u003e\n'}).add({id:29,href:"/docs/applications/",title:"Memoria Applications",description:"Memoria Applications.",content:""}).add({id:30,href:"/docs/data-zoo/",title:"Containers List",description:"",content:""}).add({id:31,href:"/docs/overview/",title:"Overview List",description:"Overview List",content:""}).add({id:32,href:"/docs/",title:"Docs",description:"Docs Memoria.",content:""}),search.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()