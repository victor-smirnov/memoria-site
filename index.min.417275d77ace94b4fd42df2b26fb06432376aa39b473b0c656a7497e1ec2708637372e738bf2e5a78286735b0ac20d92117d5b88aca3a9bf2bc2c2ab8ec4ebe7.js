var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement),f=suggestions.classList.contains('d-none');let c=0;b.keyCode===38&&!f?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&!f&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/overview/introduction/",title:"Introduction to Memoria",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eData dominates. If you\u0026rsquo;ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u0026ndash; Rob Pike in \u003ca href="http://www.lysator.liu.se/c/pikestyle.html"\u003e“Notes on Programming in C”\u003c/a\u003e, 1989.\u003c/p\u003e\n\u003ch2 id="what-memoria-is"\u003eWhat Memoria is\u003c/h2\u003e\n\u003cp\u003eMemoria is a full-stack data engineering framework aiming at exploiting inherent structure in data at all scales, starting form bare fabric or reality and ending at high-level visualizations. See \u003ca href="/docs/overview/definitions"\u003edefinitions\u003c/a\u003e for the quick overview of philosophy and math behind Memoria.\u003c/p\u003e\n\u003ch2 id="target-applications"\u003eTarget Applications\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHardware-accelerated Transactional Databases and Storage Engines.\u003c/li\u003e\n\u003cli\u003eFilesystems.\u003c/li\u003e\n\u003cli\u003eAnalytics.\u003c/li\u003e\n\u003cli\u003eArtificial Intelligence and Machine Learning.\u003c/li\u003e\n\u003cli\u003eSoftware and hardware development tools.\u003c/li\u003e\n\u003cli\u003eAnd any combination of the above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="project-structure"\u003eProject structure\u003c/h2\u003e\n\u003cp\u003eMain implementation language is modern C++ (14/17/20). Python bindings are also provided, mainly for ad-hoc manipulation with a data.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eStructured data is exposed to applications via \u003cstrong\u003eContainers\u003c/strong\u003e API.\u003c/li\u003e\n\u003cli\u003eContainers are managed by a \u003cstrong\u003eStore\u003c/strong\u003e. It\u0026rsquo;s an API that may have multiple implementations, like in-memory or on-disk.\u003c/li\u003e\n\u003cli\u003eComputations over data are described via low-level \u003cstrong\u003eDataflow graph\u003c/strong\u003e-like language, that may have multiple compilation targets (CPU, Accelerators, FPGAs/ASICs).\u003c/li\u003e\n\u003cli\u003eThe project\u0026rsquo;s structure is deeply automated with dedicated \u003cstrong\u003eMemoria Build Tool (mbt)\u003c/strong\u003e, on top of Clang libraries and Python scripts.\u003c/li\u003e\n\u003cli\u003eAdditional \u003cstrong\u003etools\u003c/strong\u003e, like \u003ca href="/docs/datascope/overview"\u003eDatascope\u003c/a\u003e to accommodate development process.\u003c/li\u003e\n\u003cli\u003eRich set of highly customizable generic data containers and dataflow templates for various narrow fields (SQL Analytics, AI and ML, probabilistic programming, etc).\u003c/li\u003e\n\u003cli\u003eHigh-performance cross-platform (Windows, Linux, MacOSX) runtime environment with Asynchronous IO and Fibers.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="what-memoria-is-not"\u003eWhat Memoria is NOT\u003c/h2\u003e\n\u003cp\u003eMemoria itself is not a DBMS, because \u003cem\u003eno size fits all\u003c/em\u003e. Instead, it can be used to build a memory-focused computational infrastructure (both at the hardware and at the software levels), fitting the needs of specific applications.\u003c/p\u003e\n\u003cp\u003eNevertheless, \u003ca href="/subprojects/swimmer-db/"\u003eSwimmerDB\u003c/a\u003e is a reference implementation of a Memoria-enabled scaleup-oriented (embedded, single-host or small-cluster) database engine, mostly for prototyping data structures, but also for real applications if the database fits their requirements.\u003c/p\u003e\n'}).add({id:1,href:"/docs/overview/definitions/",title:"Basic Definitions",description:"Basic Definitions",content:'\u003cp\u003eBelow there are few definitions that ware used intensively throughout the docs.\u003c/p\u003e\n\u003ch2 id="data-structure"\u003eData Structure\u003c/h2\u003e\n\u003ch2 id="container"\u003eContainer\u003c/h2\u003e\n\u003ch2 id="store"\u003eStore\u003c/h2\u003e\n\u003ch2 id="dataflow"\u003eDataflow\u003c/h2\u003e\n\u003ch2 id="single-writter-mutiple-readers-swmr"\u003eSingle Writter Mutiple Readers (SWMR)\u003c/h2\u003e\n\u003ch2 id="multiple-writers-multiple-readers-mwmr"\u003eMultiple Writers Multiple Readers (MWMR)\u003c/h2\u003e\n'}).add({id:2,href:"/docs/overview/whyc++/",title:"Why C++",description:"Why Memoria is using C++",content:'\u003cp\u003eMemoria is using C++ as its main development language and it\u0026rsquo;s a deliberate decision. This document explains what C++ gives to data platforms and how optimal language and run-time environment might look like.\u003c/p\u003e\n\u003cp\u003eFrom set-theoretic perspective there are much more data structures than algorithms, this is the reason why generic programming (GP) support is important for any modern programming language. The same linear search algorithm can be applied to the great variety of physical data layouts and types. Generic programming hides this variety from the programmer.\u003c/p\u003e\n\u003ch2 id="two-types-of-generic-programming"\u003eTwo Types of Generic Programming\u003c/h2\u003e\n\u003cp\u003eThere are two main ways how generic programming can be implemented. \u003cstrong\u003ePolymorphic\u003c/strong\u003e GP is based on the fact that all objects allocated on the heap have common layout and access pattern. They are long-lived and referenced via their addresses in the heap. Primitive data types may also be wrapped into an object (boxed). In this case it\u0026rsquo;s possible to compile only one instance of generic procedure and then use virtual dispatch to select possibly different execution paths for different types. Polymorphic GP-supporting compiler just checks generic type substitution rules at compile-type, then erase all type-specific information on generic variables because it\u0026rsquo;s not actually needed at run-time. This type of GP is the most scalable, providing that the programmer is happy with heap allocation and primitive data types boxing.\u003c/p\u003e\n\u003cp\u003eAs a specific optimization, some polymorphic GP compilers can do whole or partial program monomorphisation, when they generate type-specific instances of procedures. Note that from the programmer\u0026rsquo;s perspective, this optimization is a black box, there is no flexible way to mange it. Yet it\u0026rsquo;s a cheap way to increase performance of generic code in some cases.\u003c/p\u003e\n\u003cp\u003eCompilers supporting \u003cstrong\u003eMonomorphic\u003c/strong\u003e GP produce specialized instance of generic procedure for each set of types it is used with in the program. This type of GP produces highly data-specialized and potentially the most performant code but it\u0026rsquo;s not scalable. Monomorphic GP code is not modularizable, there is no separate compilation of generic libraries possible. In some cases it\u0026rsquo;s possible to hide generic code behind \u003ca href="https://bitbucket.org/vsmirnov/memoria/wiki/TemplatePimpl"\u003equasi-generic interfaces\u003c/a\u003e, compiled separately from application code using it. Nevertheless, if definition of a generic procedure or datatype is changed, all code using it directly must be recompiled.\u003c/p\u003e\n\u003cp\u003eMonomorphic GP is not limited to a specific heap data layout scheme, it doesn\u0026rsquo;t depend on objects and virtual dispatch, it\u0026rsquo;s truly generic. From other side, polymorphic GP is limited to heap objects but allows separate compilation of generic code because only one type of generic procedure is instantiated. Unfortunately, there is no cheap and elegant way to have both types of GP in a single language. So, practical way is to stick with only one type of GP: Java, C# and other languages with managed heaps support polymorphic GP. C++, D and Rust support mononorphic GP.\u003c/p\u003e\n\u003cp\u003eNote also that while some languages like C# (and Java in a distant future) support elements of monomorphic GP like generics on primitive data types, this should not be considered fully featured monomorphic GP, because such schemes are not Turing-complete and hence can\u0026rsquo;t be considered a programming. More on this below.\u003c/p\u003e\n\u003cp\u003eBy and large, the following is true for both types of GP with minor exceptions. Monomorphic GP is focused on Turing-complete type construction and specialization. The result is a great variety of different physical data layouts in the memory and layout-specific operations on that data, that may lead to unnecessary increase in machine code size.\u003c/p\u003e\n\u003cp\u003ePolymorphic GP is built around specific object layouts in the heap allowing to compile only one instance of a generic procedure for all types. This type of GP does not usually affect run-time code in any way. Instead it allows a programmer to define a set of type substitution rules, which are enforced at compile time. Polymorphic GP just guarantees that actual type substitution in the program is correct.\u003c/p\u003e\n\u003cp\u003eThough monomorphic GP languages may also provide type substitution checking, rules are much more complex in this case, and there is ongoing debate around how to implement this feature efficiently. Rust has some form of substitution checking, but C++ doesn\u0026rsquo;t (yet).\u003c/p\u003e\n\u003ch2 id="generic-programming-for-data-structures"\u003eGeneric Programming for Data Structures\u003c/h2\u003e\n\u003cp\u003eSo, polymorphic GP is scalable in terms of generic code side but limited to object heaps with predefined data layouts. This is fine until our data structures are well-reducible to object graphs (linked lists) without introducing much overhead. Many practically important lightweight in-memory data structures like arrays, maps, trees, generic graphs and so on are of this kind. When specialization of such simple data structure on a primitive data types is desirable, restricted form of monomorphisation like in C# may be provided.\u003c/p\u003e\n\u003cp\u003eThough linked list is pretty generic data structure by itself and suitable to be a foundation for many practical data structures, object heaps are not always optimal when we need more precise (bit-grade) control on physical data layout in main memory. For example, linked list-based implementations of suffix trees, used in bioinformatics, have around 40x space overhead over raw unstructured genome data. But custom schemes have much more improved space properties.\u003c/p\u003e\n\u003cp\u003eIn order to be efficient for generic data structures implementation, any programming language must provide generic configurable mapping from logical data space (ex.: objects) to physical data layout in main and external memory. C++ by itself together with its GP capabilities is well suitable for this task.\u003c/p\u003e\n\u003ch2 id="c-for-data-structures"\u003eC++ for Data Structures\u003c/h2\u003e\n\u003cp\u003eFirst, C++ is a multi-paradigm language with very cheap abstractions which do not impose much overhead at run-time. In the simplest case C++ uses zero-cost object mapping to a raw memory location. That means,  information other than object properties is put into an object by the compiler. Important details of this mapping like alignment can be controlled via compiler attributes and pragma-directives.\u003c/p\u003e\n\u003cp\u003eSecond, C++ has generic value types when a value is physically put inside specified context withing existing memory mapping:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003etemplate \u0026lt;typename T\u0026gt;\nstruct IntContextMapping {\n    int prefix_; // Some data fields before.\n    T value_     // value_ mapping will be in-lined between prefix_ \n                 // and suffix_ using default alignment.\n    int suffix_; // Some data fields after.\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThird, C++ allows building almost arbitrary types using template metaprogramming, and hence, arbitrary memory mappings. The following example demonstrates how different classes can be mapped linearly to a memory region with specified order via linear class inheritance hierarchy generator:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// Forward declaration for linear hierarchy generator helper class. The\n// helper takes list of mapping members which are classes with one template\n// parameter, that is a chained base class.\ntemplate \u0026lt;template \u0026lt;typename Base\u0026gt; class C...\u0026gt; struct LinearHierarchy; \n\n// Main case. Split the list into head and tail. Instantiate Head as a superclass\n// with LinearHierarchy\u0026lt;Tail...\u0026gt; as a parameter.\ntemplate \u0026lt;\n    template \u0026lt;typename\u0026gt; class Head,\n    template \u0026lt;typename\u0026gt; class Tail...\n\u0026gt; \nstruct LinearHierarchy: Head\u0026lt;LinearHierarchy\u0026lt;Tail...\u0026gt;\u0026gt; {};\n\n// There are no more members in the list. Stop the generator. \ntemplate \u0026lt;\u0026gt; struct LinearHierarchy {\n    void visit() {} // do nothing here\n};\n\n\n// First example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member1: Base {\n    int value1_ = 1; // simple data member \n\n    // Function member that may call functions in other layout\n    // members upward the hierarchy providing their names are known.\n    void visit() \n    {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value1_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\n// Second example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member2: Base {\n    int value2_ = 2;\n    void visit() {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value2_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\n// Fird example class for our linear mapping. \ntemplate \u0026lt;typename Base\u0026gt;\nstruct Member3: Base {\n    int value3_ = 3;\n    void visit() {\n        Base::visit();\n        std::cout \u0026lt;\u0026lt; value3_ \u0026lt;\u0026lt; \u0026quot; \u0026quot;;\n    }\n};\n\nvoid do_something() \n{\n    LinearHierarchy\u0026lt;Member1, Member2, Member3\u0026gt; m;\n    m.visit(); // prints 3 2 1\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn other words we can build arbitrary type-to-memory mappings using basic C++ rules for POD types and template metaprogramming. In Memoria complex high-level data structures like relational tables are split into reusable fragments, which are combined by the metaprogramming framework into compound structures and mapped to raw memory blocks. Building a complex data structure from simple blocks, compiler can solve combinatorial optimization problems by selecting fragments most suitable for the specific context.\u003c/p\u003e\n'}).add({id:3,href:"/docs/overview/hardware-accel/",title:"Hardware Acceleration in Memoria",description:"Hardware Acceleration in Memoria",content:'\u003ch2 id="introduction"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eMemoria was started back in 2007 after the motivation from the book \u003ca href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf"\u003eWhat Every Programmer Should Know About Memory\u003c/a\u003e by Ulrich Drepper. The idea is simple. DRAM is very slow on the random access relative to CPU speed, and while waiting for the data from memory, a CPU can perform a lot of operations. By optimizing the data layout in memory, we can reduce waiting time, improving performance. But the price is \u0026ndash; greater number of instruction, performed on such memory-optimized data layout. Modern out-of-order (OoO) CPUs can perform many operations each cycle, so this strategy works very well for them. The problem is that OoO CPUs scale poorly with their silicon area. Roughly speaking, from 22nm to 7nm, number of cores per consumer CPU increased 2 times. So, cores are getting bigger and bigger each iteration, but their performance is not improving that fast. By the 2020 we had 8-16 hot and large OoO cores in consumer CPUs and up to 64 cores in the server ones. But it\u0026rsquo;s obvious that all this silicon budgets can be spent in a better way: to application-specific hardware accelerators. Going from 7nm to 3nm will make this situation even more obvious. Given the importance of structured data processing, it\u0026rsquo;s reasonable to suppose that there is a room for Momoria-specific silicon in this budget.\u003c/p\u003e\n\u003ch2 id="candidates-for-acceleration"\u003eCandidates for Acceleration\u003c/h2\u003e\n\u003cp\u003eMemoria is essentially a storage technology, so it\u0026rsquo;s mostly an IO-bound task. So, it\u0026rsquo;s not that much to accelerate there. Nevertheless, some aspects may certainly benefit form direct hardware implementation.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSymbol sequences. \u003ccode\u003eSelect()\u003c/code\u003e, \u003ccode\u003eRank()\u003c/code\u003e and other operations for alphabet sizes larger than 2. Some CPUs have \u003ca href="https://en.wikipedia.org/wiki/SSE4#POPCNT_and_LZCNT"\u003e\u003ccode\u003epopcnt\u003c/code\u003e\u003c/a\u003e operation support for binary alphabets, but no equivalents for larger alphabets. Software emulation is too slow.\u003c/li\u003e\n\u003cli\u003eHardware support for compressed symbol sequences (alphabet size \u0026gt;= 2).\u003c/li\u003e\n\u003cli\u003eMemory compression and hardware tiered memory.\u003c/li\u003e\n\u003cli\u003eHardware memory protection (tagged memory). Programs contain bugs, it\u0026rsquo;s inevitable, which can corrupt data permanently. \u003ca href="https://www.rust-lang.org"\u003eLinear types\u003c/a\u003e is not the ultimate solution because of unsafe code that can spread errors deep to the safe code. Again, compilers may also have errors.\u003c/li\u003e\n\u003cli\u003eFast memory checksuming: detecting corruption as soon as possible in case of hardware failures.\u003c/li\u003e\n\u003cli\u003eFast memory encryption both for data storage in external memory and in the main one.\u003c/li\u003e\n\u003cli\u003eNative \u003ccode\u003eDataType\u003c/code\u003es support, like variable length numbers, strings, unicode, etc.\u003c/li\u003e\n\u003cli\u003eNative support for some \u003ca href=""\u003eLinkedData\u003c/a\u003e constructions and workflows.\u003c/li\u003e\n\u003cli\u003eHardware data shuffle/scatter/gather engine.\u003c/li\u003e\n\u003cli\u003eAdopting \u003ca href="https://en.wikipedia.org/wiki/Scratchpad_memory"\u003escratchpad memory\u003c/a\u003e architecture instead of tall cache hierarchies or in addition to them.\u003c/li\u003e\n\u003cli\u003eMemoria-specific concurrency and parallelism modes support (hardware inter-core queues?).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHardware data types support (sequences, integers, strings etc), hashing, compression and encryption, hardware memory protection may improve single-threaded performance, that is especially important for low-power devices. It\u0026rsquo;s not expected that entire Memoria will be run on an \u003ca href="https://en.wikipedia.org/wiki/Microcontroller"\u003eMCU\u003c/a\u003e-class device. Certain \u003ca href="https://bitbucket.org/vsmirnov/memoria/wiki/Memoization4AI"\u003edata structures\u003c/a\u003e nevertheless may be accelerated via dedicated Memoria IP and used for inference in EdgeAI applications.\u003c/p\u003e\n\u003cp\u003eBeing a storage technology, Memoria does not provide any data processing, leaving this area to applications. Nevertheless, to achieve high efficiency, storage layer and processing layer must be tightly coupled. To fulfill this need, Memoria project is providing an \u0026lsquo;accelerator generator\u0026rsquo; \u0026ndash; design space exploration tool for data-intensive applications. In its core idea, Memoria itself is also a design space exploration tool, but for data structures and storage engines. Extending this idea into accelerator\u0026rsquo;s area looks like a natural move. In this respect Memoria is pretty similar to the \u003ca href="https://bar.eecs.berkeley.edu/projects/rocket_chip.html"\u003eRocket Chip SoC generator\u003c/a\u003e for RISC-V-based systems.\u003c/p\u003e\n\u003ch2 id="memoria-acceleration-architecture"\u003eMemoria Acceleration Architecture\u003c/h2\u003e\n\u003cp\u003eAcceleration Architecture is based on the \u003ca href="https://riscv.org"\u003eRISC-V\u003c/a\u003e accelerators design pattern that is essentially an application-specific ISA extensions for a RISC core, backed with specific hardware. \u003ca href="https://github.com/victor-smirnov/jenny"\u003eJenny Metaprogramming Platform\u003c/a\u003e will be supporting those extensions at the C/C++ level.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRISC-V toolchain: compiler, binutils, simulator(s)\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria-accel"\u003eRocket Chip-based accelerator sandbox\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eArty A7-100T board\u003c/li\u003e\n\u003cli\u003eAlveo U50 application accelerator\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eISA-level simulator (not cycle-accurate) of a RISC-V-based \u003ca href="https://en.wikipedia.org/wiki/Massively_parallel_processor_array"\u003eMPPA\u003c/a\u003e accelerator for compute-intensive applications on top of Memoria-provided data (SQL, ETL, Datalog, probabilistic programming, etc).\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://fires.im/"\u003eFireSim\u003c/a\u003e extensions for cycle-accurate simulation.\u003c/li\u003e\n\u003cli\u003eVerification tools.\u003c/li\u003e\n\u003cli\u003eJenny-integrated metaprogramming tools.\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:4,href:"/docs/overview/qt_creator_instructions/",title:"QT Creator Instructions",description:"",content:'\u003ch2 id="build-and-install-vcpkg-for-memoria"\u003eBuild and install VCPkg for Memoria\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# Assuming current folder is /home/guest/cxx\n$ git clone https://github.com/microsoft/vcpkg.git\n$ cd vcpkg\n$ ./bootstrap-vcpkg.sh\n$ ./vcpkg install boost icu abseil yaml-cpp fmt\n$ ./vcpkg install llvm[clang,libcxx,libcxxabi,compiler-rt,tools,enable-rtti,enable-threads,enable-eh]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id="configuring-vcpkgs-provided-cmake-tool"\u003eConfiguring VCPkg\u0026rsquo;s provided cmake tool\u003c/h2\u003e\n\u003cp\u003eIn Options/Kits/Cmake tab add another cmake configuration by specifying full path VCPkg\u0026rsquo;s own cmake distribution.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-required-clang-compiler"\u003eConfigure Required clang compiler\u003c/h2\u003e\n\u003cp\u003eMemoria currently is built with clang compiler version 6.0 or newer. If you system already provides it, like most Linux distributions do, then this step is unnecessary. Otherwise, build clang yourself and configure it on the Options/Kits/Compiler tab:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-clang.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="add-new-kit-for-clang"\u003eAdd new Kit for Clang\u003c/h2\u003e\n\u003cp\u003eAdding new Kit is necessary if QtCreator did not recognize clang compiler automatically. Just create new kit by cloning and existing one and specify clang 6.0 as C and C++ compilers:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-newkit.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="vcpkgs-cmake-selection"\u003eVCPkg\u0026rsquo;s Cmake Selection\u003c/h2\u003e\n\u003cp\u003eNow specify that VCPkg\u0026rsquo;s provided cmake tool will be used for new Kit, and specify the path to VCPkg\u0026rsquo;s libraries definitions:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-kit-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eProvide your full path to vcpkg.cmake:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-vcpkg-toolchain.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-memorias-build-parameters"\u003eConfigure Memoria\u0026rsquo;s build parameters\u003c/h2\u003e\n\u003cp\u003eToggle BUILD_* options as specified on the screenshot. This will build Tests, as well as threads- and fibers-based Memoria allocators, with libbacktrace support in case of exceptions. Uncheck BUILD_WITH_BACKTRACE option on MacOSX.\u003c/p\u003e\n\u003cp\u003eMore details on build options can be found in top-level CMakeLists.txt\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-project-cfg.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThat\u0026rsquo;s it! Optionally specify for \u003cstrong\u003eBuild Steps\u003c/strong\u003e -j12 to build in 12 threads.\u003c/p\u003e\n\u003cp\u003ePress Ctrl+B to start build process.\u003c/p\u003e\n'}).add({id:5,href:"/docs/overview/faq/",title:"FAQ",description:"Answers to frequently asked questions.",content:""}).add({id:6,href:"/docs/data-zoo/overview/",title:"Core Data Structures -- Overview",description:"",content:"\u003cp\u003eThis sections contans detailed description of some core data structures Memoria\u0026rsquo;s containers are based on.\u003c/p\u003e\n"}).add({id:7,href:"/docs/data-zoo/partial-sum-tree/",title:"Partial Sums Tree",description:"",content:'\u003ch2 id="description"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s take a sequence of monotonically increasing numbers, and get delta sequence from it, as it is shown on the following figure. Partial Sum Tree is a tree of sums over this delta sequence.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="trees.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003ePacked tree in Memoria is a multiary balanced tree mapped to an array. For instance in a level order as it shown on the figure.\u003c/p\u003e\n\u003cp\u003eGiven a sequence of N numbers with monotonically increasing values, partial sum tree provides several important operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efindLT(value)\u003c/code\u003e finds position of maximal element less than \u003ccode\u003evalue\u003c/code\u003e, time complexity $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efindLE(value)\u003c/code\u003e finds position of maximal element less than or equals to \u003ccode\u003evalue\u003c/code\u003e, $T = O(log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esum(to)\u003c/code\u003e computes plain sum of values in the delta sequence in range [0, to), $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eadd(from, value)\u003c/code\u003e adds \u003ccode\u003evalue\u003c/code\u003e to all elements of original sequence in the range of [from, N), $T = O(log(N))$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is obvious that first two operations can be computed with binary search without partial sum tree and all that overhead it introduces.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003ePartial or prefix sum trees of higher degrees are especially suitable for hardware acceleration. DDR and HBM memory transfer data in batches and works best if data is processed also in batches. The idea here is to offload tree traversal operation from CPU to the memory controller or even to DRAM memory chips. Sum and compare operations are relatively cheap to implement, and no complex control is required.\u003c/p\u003e\n\u003cp\u003eBy offloading tree traversal to the memory controller (that usually works in front of caches), we can save precious cache space for more important data. By offloading summing and comparison to DRAM chips, we can better exploit internal memory parallelism and save memory bandwidth. In such distributed architecture, a single tree level scan can be performed with the latency and in the power budget of a \u003cem\u003esingle random memory access\u003c/em\u003e, saving energy and silicon for other computations.\u003c/p\u003e\n'}).add({id:8,href:"/docs/data-zoo/searchable-seq/",title:"Searchable Sequence",description:"",content:'\u003cp\u003eSearchable sequence or rank/select dictionary is a sequence of symbols that supports two operations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003erank(position, symbol)\u003c/code\u003e is number of occurrences of \u003ccode\u003esymbol\u003c/code\u003e in the sequence in the range [0, position)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect(rank, symbol)\u003c/code\u003e is a position of \u003ccode\u003erank\u003c/code\u003e-th occurrence of the \u003ccode\u003esymbol\u003c/code\u003e in the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsually the alphabet is {0, 1} because of its practical importance, but larger alphabets are of significant interest too. Especially in Bioinformatics and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eThere are many implementations of binary searchable sequences (bitmaps) providing fast query operations with $O(1)$ time complexity. Memoria uses partial sum indexes to speedup rank/select queries. They are asymptotically slower than other methods but have additional space overhead for the index.\u003c/p\u003e\n\u003cp\u003ePacked searchable sequence is a searchable sequences that has all its data structured packed into a single contiguous memory block with packed allocator. It consists from two data structures:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003emulti-index partial sum tree to speedup rank/select queries;\u003c/li\u003e\n\u003cli\u003earray of sequence\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for the case of searchable bitmap.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_seq.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that for a sequence with K-symbol alphabet, packed sum tree has K indexes, that results in significant overhead even for relatively small alphabets. For example 8-bit sequence has 256-index packed tree that takes more than 200% of raw data size if the tree is not compressed. To lower this overhead Memoria provides various compressed encodings for the index\u0026rsquo;s values.\u003c/p\u003e\n\u003ch2 id="creation-and-access"\u003eCreation and Access\u003c/h2\u003e\n\u003cp\u003eTo create partial sum tree for a sequence we first need to split it logically into blocks of fixed number of symbols (16 at the figure). Then sum different symbols in the block, each such vector is a simple partial sum tree leaf. Build other levels of the tree accordingly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSymbol update is relatively fast, it takes $O(log(N))$ time.\u003c/li\u003e\n\u003cli\u003eSymbol insertion is $O(N)$, it requires full rebuilding of partial sum tree.\u003c/li\u003e\n\u003cli\u003eSymbol access does not require the tree to perform, it takes $O(1)$ time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003erank(position, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven \u003ccode\u003eposition\u003c/code\u003e, determine sequence \u003ccode\u003eblock_number\u003c/code\u003e for that position, and \u003ccode\u003eblock_pos\u003c/code\u003e position in the block, $O(1)$;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eblock_rank\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) in the sum tree, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003ecount number of \u003ccode\u003esymbol\u003c/code\u003es in the block to \u003ccode\u003eblock_pos\u003c/code\u003e, $O(1)$;\u003c/li\u003e\n\u003cli\u003efinal rank is (2) + (3).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003eselect(rank, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven partial sum tree and a \u003ccode\u003esymbol\u003c/code\u003e, determine target sequence \u003ccode\u003eblock_number\u003c/code\u003e having \u003ccode\u003etotal_rank \u0026lt;= rank\u003c/code\u003e for the given \u003ccode\u003esymbol\u003c/code\u003e using \u003ca href="/docs/data-zoo/partial-sum-tree"\u003efindLE\u003c/a\u003e operation, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eFor the given block, compute \u003ccode\u003erank_prefix\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) for the given \u003ccode\u003esymbol\u003c/code\u003e, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eCompute \u003ccode\u003elocal_rank = rank - rank_prefix\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003eScan the block and find position of \u003ccode\u003esymbol\u003c/code\u003e having rank in the block = \u003ccode\u003elocal_rank\u003c/code\u003e, $O(1)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eActual implementation joins operations (1) and (2) into a single traverse of the sum tree.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eConsideration are the same as for \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial/prefix sum trees\u003c/a\u003e, especially, because searchable sequence contains partial sum tree as an indexing structure.\u003c/p\u003e\n\u003cp\u003eModern CPUs usually have direct implementations for rank (PopCount) for binary alphabets. Select operation may also be partially supported. To accelerate searchable sequences, it\u0026rsquo;s necessary to implement rang/select over arbitrary alphabets (1-8 bits per symbol).\u003c/p\u003e\n\u003cp\u003eSymbol blocks are also contiguous in memory and can be multiple of DRAM memory blocks. Rank/select machinery is simpler or comparable with machinery for addition and subtraction. Those operations can be efficiently implemented in a small silicon budget and at high frequency.\u003c/p\u003e\n'}).add({id:9,href:"/docs/data-zoo/compressed-symbol-seq/",title:"Compressed Symbol Sequence",description:"",content:"\u003cp\u003eTBC\u003c/p\u003e\n"}).add({id:10,href:"/docs/data-zoo/hierarchical-table/",title:"Hierarchical Table",description:"",content:"\u003cp\u003eTBC\u003c/p\u003e\n"}).add({id:11,href:"/docs/data-zoo/louds-tree/",title:"Level Order Unary Degree Sequence (LOUDS) ",description:"",content:'\u003cp\u003eLevel Order Unary Degree Sequence or LOUDS is a special form of ordered tree encoding. To get it we first need to enumerate all nodes of a tree in level order as it is shown of the following figure.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThen for each node we write its degree in unitary encoding. For example, degree of the fist node is 3, then its unitary encoding is \u0026lsquo;1110\u0026rsquo;. To finish LOUDS string we need to prepend substring \u0026lsquo;10\u0026rsquo; to it as it is shown on the figure. Given an ordered tree on N nodes LOUDS takes no more than 2N + 1 bits. This is very succinct implicit data structure.\u003c/p\u003e\n\u003cp\u003eLOUDS is a bit vector. We also need the following operations on it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erank1(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;1\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank0(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;0\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect1(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;1\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect0(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;0\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDifferent ways of tree node numbering for LOUDS are possible, Memoria uses the simplest one. Tree node positions are coded by \u0026lsquo;1\u0026rsquo;.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003enode_num = rank1(i + 1)\u003c/code\u003e \u0026ndash; gets tree node number at position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ei = select1(node_num)\u003c/code\u003e \u0026ndash; finds position of a node in LOUDS given its number in the tree.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHaving this node numbering we can define the following tree navigation operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efist_child(i) = select0(rank1(i + 1)) + 1\u003c/code\u003e \u0026ndash; finds position of the first child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_child(i) = select0(rank1(i + 1) + 1) - 1\u003c/code\u003e \u0026ndash; finds position of the last child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparent(i) = select1(rank0(i + 1))\u003c/code\u003e \u0026ndash; finds position of the parent for the node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echildren(i) = last_child(i) - first_child(i)\u003c/code\u003e \u0026ndash; return number of children for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echild(i, num) = first_child(i) + num\u003c/code\u003e \u0026ndash; returns position of num-th child for the node at the position \u003ccode\u003ei\u003c/code\u003e, \u003ccode\u003enum \u0026gt;= 0\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_node(i) = LOUDS[i] == 1 ? true : false\u003c/code\u003e \u0026ndash; checks if \u003ccode\u003ei\u003c/code\u003e-th position in tree node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that navigation operations only defined for positions \u003ccode\u003ei\u003c/code\u003e for those \u003ccode\u003eis_leaf(i) == true\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cp\u003eLet we find number of the first child for the node 8.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eselect1(8) = 11\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efirst_child(11) = select0(rank1(11 + 1)) + 1 = select0(8) + 1 = 19\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank1(19 + 1) = 12\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first child for node 8 is node 12.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how the parent() operation works:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds-parent.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="limitations"\u003eLimitations\u003c/h2\u003e\n\u003cp\u003eNode numbers are valid only until insertion (or deletion) into the tree. Additional data structure is necessary to keep track of node positions after tree structure updates.\u003c/p\u003e\n\u003ch2 id="labelled-tree"\u003eLabelled Tree\u003c/h2\u003e\n\u003cp\u003eLabelled tree is a LOUDS tree with a fixed set of numbers (or \u0026lsquo;labels\u0026rsquo;) associated with each node. It is implemented as multistream balanced tree where the first stream is dynamic bit vector with rank/select support, and the rest are streams for each label.\u003c/p\u003e\n\u003cp\u003eStreams elements distribution is very simple for this data structures. Each label belongs to a tree node that is coded by position of \u0026lsquo;1\u0026rsquo; in LOUDS. Each leaf of balanced tree has some subsequence of the LOUDS.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="labeled_tree_leaf.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLet we have a LOUDS sub-stream of size N with M 1s for a given leaf. Then this leaf must contain M labels in each label stream. The following expressions links together node and level positions withing a leaf:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elabel_idx = rank1(node_idx + 1) - 1\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enode_idx = select1(label_idx + 1)\u003c/code\u003e;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhere \u003ccode\u003elabel_idx\u003c/code\u003e is in [0, M) and \u003ccode\u003enode_idx\u003c/code\u003e is in [0, N)\u003c/p\u003e\n\u003cp\u003eLabeledTree uses balanced partial sum tree as a basic data structure. Because of that it optionally supports partial sums of labels is specified tree node range. The most interesting range is \u003ccode\u003e[0, node_idx)\u003c/code\u003e. See \u003ca href="/docs/data-zoo/wavelet-tree"\u003eMultiary Wavelet Tree\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2 id="cardinal-trees"\u003eCardinal Trees\u003c/h2\u003e\n\u003cp\u003eIn ordered trees like in the example above, all children nodes are naturally ordered, and a node may have arbitrary number of children. In the cardinal tree of degree $D$, a node always have $D$ children, but some children can be omitted. And this information is stored in the tree, like \u0026ldquo;child $i$ is absent\u0026rdquo;. Binary search trees are cardinal trees of degree 2.\u003c/p\u003e\n\u003cp\u003eCardinal tree of degree 4 (and greater, where degree is a power of 2) is a trie-based (or region-based) \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCardinal LOUDS tree can be implemented either as a labelled tree, where node labels (from 0 to $D-1$) are \u003cem\u003ecardinal labels\u003c/em\u003e (for sparse cardinal trees like spatial trees), or by using a searchable bitmap specifying which children are present. The bitmap can be \u003cem\u003ecompressed\u003c/em\u003e, saving space for sparse cases.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS trees are especially important type of compact/succinct trees, because all children of a node are stored linearly in memory, that is DRAM-friendly: traversal of a tree is a series of liner scans split by random jumps. But the number of random jumps is much smaller comparing to other types of trees. LOUDS trees do not require any specific hardware support, providing that \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable bitmap\u003c/a\u003e is fully accelerated.\u003c/p\u003e\n'}).add({id:12,href:"/docs/data-zoo/wavelet-tree/",title:"Multiary Wavelet Trees",description:"",content:'\u003cp\u003eWavelet trees (WT) are data succinct rank/select dictionaries for large alphabets with many \u003ca href="http://arxiv.org/abs/1011.4532"\u003epractical applications\u003c/a\u003e. There is a good \u003ca href="http://alexbowe.com/wavelet-trees/"\u003eexplanation\u003c/a\u003e of what binary wavelet trees are and how they work. They provide rank() and select() over symbol sequences ($N$ symbols) drawn from arbitrary fixed-size alphabets ($K$ symbols) in $O(log(N) * log(K))$ operations, where logarithms are on the base of 2. Therefore, for large alphabets, $log(K)$ is quite a big value that leads to big hidden constants in practical implementations of the binary WT.\u003c/p\u003e\n\u003cp\u003eIn order to improve runtime efficiency of wavelet trees we have to lower this constant. And one of the way here is to use multiary cardinal trees instead of binary ones. In this case, for $M$-ary cardinal tree we will have $log(M)$ speedup factor over binary trees (tree height is $log(M)$-times smaller).\u003c/p\u003e\n\u003ch2 id="wavelet-tree-structure"\u003eWavelet Tree Structure\u003c/h2\u003e\n\u003cp\u003eLet we have a sequence of integers, say, 54.03.12.21.47.03.17.54.22.51 drawn from 6-bit alphabet. The following figure shows 4-ary wavelet tree for this sequence. Such WT has $6/log_2(4) = 3$ levels.\u003c/p\u003e\n\u003cp\u003eFirst we need to represent our sequence in a different format. Our WT is 4-ary and has 3 layers. We need to \u0026ldquo;split\u0026rdquo; the sequence in 3 layers horizontally where symbols of each layer are drawn from 2-bit alphabet. In other words, we need to recode our sequence from base of 10 to base of 4, and then write numbers vertically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that this is just a logical operation, it doesn\u0026rsquo;t require any transformation of the sequence itself.\u003c/p\u003e\n\u003cp\u003eOur WT is a 4-ary cardinal tree, each node has from 0 to 4 children. Each child represents one symbol from the layer\u0026rsquo;s alphabet. Note that in general case it isn\u0026rsquo;t necessary to draw all layers from the same alphabet, but it simplifies implementation.\u003c/p\u003e\n\u003cp\u003eIn order to build WT, perform the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAssign top layer (Layer 2) of the sequence to the root node of WT.\u003c/li\u003e\n\u003cli\u003eFor each symbol of Layer 1 put it to the subsequence of the node with the cardinal label matched with corresponding symbol from the same position in the Layer 2.\u003c/li\u003e\n\u003cli\u003eRepeat step (2) for symbols at Layer 0 but now select appropriate child at Level 1 of the tree, using pair of symbols from the same positions at Layer 1 and Layer 2 of the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the figure for details. Symbols in the WT and the sequence are colored to simplify understanding of symbols\' distribution.\u003c/p\u003e\n\u003cp\u003eNote that for an alphabet with K symbols, multiary WT has up to K leafs that can be very significant number. But for most practical cases this number is moderate. The larger number of distinct symbols in the sequence, the bigger tree is. Dynamic LOUDS with associated cardinality labels is used to code structure of WT.\u003c/p\u003e\n\u003cp\u003eAlso, it is not necessary to keep empty nodes in the tree (they are shown in gray on the figure).\u003c/p\u003e\n\u003ch2 id="insertion-and-access"\u003eInsertion and Access\u003c/h2\u003e\n\u003cp\u003eTo insert a value into WT we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the path from root to leaf for inserted values, insert tree nodes if necessary;\u003c/li\u003e\n\u003cli\u003efind correct position in the node\u0026rsquo;s subsequence to insert current symbol.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe path in the wavelet tree is determined by \u0026ldquo;layered\u0026rdquo; representation if inserted symbol. Computation of insertion position is a bit tricky.\u003c/p\u003e\n\u003cp\u003eLet we insert the value of 37 into position 7. Layered representation of 37 is \u0026ldquo;211\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLevel 2. Insert \u0026ldquo;2\u0026rdquo; into position 7 of root node\u0026rsquo;s subsequence of WT.\u003c/li\u003e\n\u003cli\u003eLevel 1. Next child is \u0026ldquo;2\u0026rdquo;. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(7 + 1, 2) - 1 = rank(8, 2) - 1 = 1\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003cli\u003eLevel 0. Next child is \u0026ldquo;1\u0026rdquo;, create it. Repeat the procedure for Layer 1. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(1 + 1, 1) - 1 = rank(2, 1) - 1 = 0\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_insert.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAccess is similar, but instead of to insert a symbol to a node\u0026rsquo;s subsequence, take the symbol form it and use it to select next child.\u003c/p\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute rank(position, symbol) we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the leaf in WT for the symbol;\u003c/li\u003e\n\u003cli\u003efind position in the leaf to compute the final rank.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_rank.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eComputation of select(rank, symbol) is different. If rank() is computed top-down, then select() is computed bottom-up.\u003c/p\u003e\n\u003cp\u003eLet we need to select position of the 2nd 3 in the original sequence. Layered representation for 3 is \u0026ldquo;003\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFind the leaf in WT for the given symbol.\u003c/li\u003e\n\u003cli\u003ePerform \u003ccode\u003eselect(2, \u0026quot;3\u0026quot;) = Pos0\u003c/code\u003e on the leaf\u0026rsquo;s sequence.\u003c/li\u003e\n\u003cli\u003eWalk up to parent for his leaf. Perform \u003ccode\u003eselect(Pos0 + 1, \u0026quot;0\u0026quot;) = Pos1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eStep up to the parent node (the root). Perform \u003ccode\u003eselect(Pos1 + 1, \u0026quot;0\u0026quot;) = Pos\u003c/code\u003e. This is the final result.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_select.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="implementations"\u003eImplementations\u003c/h2\u003e\n\u003cp\u003eIn Memoria, Multiary wavelet tree consists of four distinct data structures.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLOUDS to store wavelet tree structure.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of cardinal labels for tree nodes.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of sizes for tree node\u0026rsquo;s sub-sequences.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of node\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first three structures are implemented as single \u003ca href="/docs/data-zoo/louds-tree"\u003eLabeled Tree\u003c/a\u003e with two labels. The first one is cardinality of the node in its parent. The second one is size of node\u0026rsquo;s subsequence.\u003c/p\u003e\n\u003cp\u003eThe fourth data structure is a separate \u003ca href="/docs/data-zoo/searchable-seq"\u003eSearchable Sequence\u003c/a\u003e for small sized alphabets.\u003c/p\u003e\n\u003cp\u003eMemoria has two different implementations of WT algorithm. The first one is dynamic WT that provides access/insert/select/rank operations performing in O(log \u003cem\u003eN\u003c/em\u003e) time.\u003c/p\u003e\n\u003cp\u003eThe second one has all those four data structures implemented with \u003ca href="Memory_Allocation"\u003ePacked Allocator\u003c/a\u003e placed in a single raw memory block of limited size. This implementation has fast access/select/rank operations but slow insert operation with O(\u003cem\u003eN\u003c/em\u003e) time complexity.\u003c/p\u003e\n\u003cp\u003eCurrently Memoria provides only 256-ary wavelet tree for 32-bit sequences. Other configurations will be provided in upcoming releases of the framework.\u003c/p\u003e\n'}).add({id:13,href:"/docs/data-zoo/mutistream-tree/",title:"Mutistream Balanced Tree",description:"",content:'\u003ch2 id="the-problem"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eLet us consider data structure mapping an integer \u003ccode\u003eID\u003c/code\u003e to a dynamic vector. Nothing complicated is here but there is one requirement: it has to represent small data values compactly, without that overhead most file systems have. That means we can\u0026rsquo;t just use \u003ccode\u003eMap\u0026lt;ID, Vector\u0026lt;\u0026gt;\u0026gt;\u003c/code\u003e because like any other container, even empty \u003ccode\u003eVector\u0026lt;\u0026gt;\u003c/code\u003e consumes at least one memory block. So this overhead will be very significant for large number of small data values.\u003c/p\u003e\n\u003cp\u003eThe solution is to store all data values in a single dynamic vector and use an additional data structure for the dictionary of data values. This dictionary is a set of pairs in the form \u003ccode\u003e\u0026lt;ID, DataOffset\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="vector_map.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe dictionary can be easily represented via a balanced partial sum tree having two indexes, one for \u003ccode\u003eID\u003c/code\u003e and one for \u003ccode\u003eDataOffset\u003c/code\u003e as it is shown on the following figure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="double_index.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAn multi-index partial sums tree is just an ordinary \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e except that scalar values of sums regarded as vectors.\u003c/p\u003e\n\u003cp\u003eThere is no overhead for small data values in this model because both data structures (the dictionary and the dynamic vector) are represented in the most compact way. But now there is performance overhead because each data value search requires traversal of two balanced trees, the dictionary and the dynamic vector.\u003c/p\u003e\n\u003ch2 id="the-solution"\u003eThe Solution\u003c/h2\u003e\n\u003cp\u003eThe solution is to intermix the dictionary and the data within a single balanced search tree so that the dictionary entries are placed closer to its data.\u003c/p\u003e\n\u003cp\u003eLet us think that a balanced tree of a dictionary entry is the primary one. If all data values are small we can put all of them into the same tree leafs with their dictionary entry. But in general case data entries can be much larger than limited capacity of the leaf page. To maintain large data values we need complete dynamic vector sub-structure in the balanced tree of dictionary entries.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how it can be done:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree_nodes.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the leaf nodes of balanced tree we put two dynamic array:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ean array of dictionary entries (red, blue) and\u003c/li\u003e\n\u003cli\u003ean array of data (green).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the branch nodes we put three dynamic arrays:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethe array of partial sums for dictionary entries (red, blue),\u003c/li\u003e\n\u003cli\u003ethe array of partial sums for data lengths (green),\u003c/li\u003e\n\u003cli\u003ethe array of child node IDs.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis joined balanced partial sum tree contains all information from both source trees, dictionary and dynamic entry ones. It is not necessary to place dictionary entries in the same leafs with their data.\u003c/p\u003e\n\u003cp\u003eThen total balanced tree structure will look like this:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIt can be shown that entries can be placed in any place relative to their data providing that sequences of entries is ordered correctly. But logically linked entities should be placed closer to each other in terms of leaf nodes for performance reasons.\u003c/p\u003e\n'}).add({id:14,href:"/docs/data-zoo/packed-allocator/",title:"Packed Allocator",description:"",content:'\u003cp\u003eWe need to place several, possibly resizable (see below), objects into a single contiguous memory block of limited size. Classical malloc-like memory allocator is not suitable here because it doesn\u0026rsquo;t work well with resizable objects. Especially if they are allocated in a relatively small memory block. To maintain resizability efficiently we have to relocate other objects if some object is resized.\u003c/p\u003e\n\u003ch2 id="resizable-object-pattern"\u003eResizable Object Pattern\u003c/h2\u003e\n\u003cp\u003eResizable object is an object that has unbounded size. Usually it has the following pattern:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  int object_size_;\n  char[] variable_size_data_;\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, the last member \u003ccode\u003echar[] variable_size_data_\u003c/code\u003e is an unbounded array. For any object \u003ccode\u003esizeof()\u003c/code\u003e does\nnot count the last member if it is unbounded array. For example, \u003ccode\u003esizeof(ResizableObject) == sizeof(int)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf a custom allocator allocates more memory than necessary for resizable objects, this memory can be accessed via the last member. Usually such an object should know length of the memory block it is mapper to. But it can also be stored by memory allocator.\u003c/p\u003e\n\u003ch2 id="linear-contiguous-allocator"\u003eLinear Contiguous Allocator\u003c/h2\u003e\n\u003cp\u003eThe idea is to place all abjects contiguously in a memory block and shift them if some object is resized. Separate layout dictionary is used to locate objects in a block. Objects are accessed by their indexes, not by direct addresses in the block.\u003c/p\u003e\n\u003cp\u003eLayout dictionary is an ordered list of block offsets. If dictionary if large, \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e can be used to speedup access.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_contiguous_allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLayout dictionary is placed into the same memory block as object\u0026rsquo;s data.\u003c/p\u003e\n\u003ch2 id="allocator-aware-objects"\u003eAllocator-Aware Objects\u003c/h2\u003e\n\u003cp\u003eThe main property of resizable objects is that their size can be changed dynamically, that requires interaction with allocator. Consider the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  //...\n  int insert(int index, int value); // enlarge object\n  int remove(int index);            // shrink object\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have two methods affecting object\u0026rsquo;s size. And it is good to incapsulate required interaction with allocator within resizable objects. But the problem is we don\u0026rsquo;t want to store raw memory pointers to the allocator withing objects for various reasons. The main reason is we want allocators to be relocatable. If the allocator itself is relocated, all pointers have to be updated.\u003c/p\u003e\n\u003cp\u003eThe idea is to put allocator and objects into a single addressable memory block. In this case we can get address of allocator having only address of the object and its relative offset in the memory block. Let\u0026rsquo;s consider \u003ccode\u003ePackedAllocatable\u003c/code\u003e base class for any allocator-aware resizable object:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\nclass PackedAllocator;\n\nclass PackedAllocatable {\n  typedef PackedAllocator Allocator;\n  int allocator_offset_; // resizable object offset in the allocator\'s memory block\npublic:\n  Allocator* allocator() {\n    if (allocator_offset_ \u0026gt; 0) {\n      return reinterpret_cast\u0026lt;Allocator*\u0026gt;(reinterpret_cast\u0026lt;char*\u0026gt;(this) - allocator_offset_);\n    }\n    else return nullptr;\n  }\n\n  // Other methods go here...\n};\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the following figure how it may look like:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_brief.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn Memoria we call such allocator \u003cem\u003epacked\u003c/em\u003e, because it packs all objects and itself into single self-sufficient memory block that can be relocated or serialized. That doesn\u0026rsquo;t affect relative positions of objects within the memory block.\u003c/p\u003e\n\u003cp\u003eEach allocator-aware resizable object must derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e class that maintains relative offset of an object to the allocator. The following figure explains it in greater details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we have four resizable objects with sizes 4, 7, 11, and 9 respectively. Each object maintains its relative offset in the memory block, that is converted to a pointer to the allocator.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s increase object #1 by 8 units:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full_enlarged.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWe need to perform the following operations for objects #2 and #3.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eShift objects\' data right by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease objects\' offsets in layout dictionary by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease \u003ccode\u003ePackedAllocatable::allocator_offset_\u003c/code\u003e by 8 units.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNot all abjects are resizable, they don\u0026rsquo;t need to maintain a pointer to the allocator. But now allocator has to know which objects are instances of \u003ccode\u003ePackedAllocatable\u003c/code\u003e and which are not to update pointers properly.\u003c/p\u003e\n\u003ch2 id="recursive-allocator"\u003eRecursive Allocator\u003c/h2\u003e\n\u003cp\u003eThe next idea is to define packed allocator recursively by deriving from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case it is possible to embed any packed allocator into another allocator just as any resizable object.\u003c/p\u003e\n\u003ch2 id="packedallocator-api"\u003ePackedAllocator API\u003c/h2\u003e\n\u003cp\u003eSo we have:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ePackedAllocator\u003c/code\u003e managing resizable memory regions withing contiguous relocatable memory block.\u003c/li\u003e\n\u003cli\u003eAllocator-aware objects derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003ePacked allocator also derives from \u003ccode\u003ePackedAllocatable\u003c/code\u003e that mean it can be embedded into another packed allocator.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe last idea is to use \u003ccode\u003ePackedAllocator\u003c/code\u003e as a base class for resizable objects. That enables them to have more than one resizable section.\u003c/p\u003e\n\u003cp\u003eThe following code snippet explains basic PackedAllocator API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n\n  template \u0026lt;typename T\u0026gt;\n  T* get(int i);             //returns address of i-th memory block, i = 0...\n  template \u0026lt;typename T\u0026gt;\n  const T* get(int i) const; //returns address of i-th memory block, i = 0...\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i, int size); // allocates size bytes for i-th memory block and initializes\n                                // it properly if T derives from PackedAllocatable.\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i); // Allocates space for i-th memory block and initializes\n                      // it properly if T derives from PackedAllocatable. \n                      // Size of the block is got via T::empty_size() if T derives \n                      // form PackedAllocatable and sizeof(T) otherwise.\n\n  // resize memory block at address \'addr\', resize parent allocator if necessary\n  // throws PackedOOMException if top-most allocator runs out of memory\n  void resize(const void* addr, int new_size); \n\n  //the same as above but returns size of i-th block in bytes\n  void resize(int i, int new_size);\n\n  int size(int i) const; \n\n  void init(int entries); // initializes allocator with the specified number of \n                          // empty blocks (entries)\n \n  // returns size in bytes of empty allocator having specified number of entries\n  static int empty_size(int entries); \n\n  static int round(int size); // round size to alignment blocks. e.g. if alignment block is 8 bytes\n                              // round(1) = 8; round(12) = 16\n\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo interact with PackedAllocator each allocatable object provides two methods. One of them is for initialization, and another one is to query object size:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass SimpleResizableObject: public PackedAllocatable {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\nclass AdvancedResizableObject: public PackedAllocator {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that resizable objects must be \u003ca href="http://en.cppreference.com/w/cpp/types/is_trivial"\u003etrivial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote also that Memoria doesn\u0026rsquo;t currently use placement \u003ca href="http://en.cppreference.com/w/cpp/memory/new/operator_new"\u003enew\u003c/a\u003e и \u003ca href="http://en.cppreference.com/w/cpp/language/delete"\u003edelete\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// There is no way to specify custom allocator and other parameters here, only address \n// of the object to delete. PackedAllocator does not allow to get allocator address given\n// only address of a block it manages. so it provides explicit API for allocation \n// and deallocation.\n\nvoid operator delete (void *ptr); // placement delete operator\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee this \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/include/memoria/core/packed/tools/packed_allocator.hpp"\u003esource\u003c/a\u003e for more details about PackedAllocator implementation.\u003c/p\u003e\n\u003ch2 id="resizable-object-example-seachable-sequence"\u003eResizable Object Example: Seachable Sequence\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s consider a relatively simple but live example: \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable sequence\u003c/a\u003e. It is a sequence of symbols providing rank and select operations performed in logarithmic time. To provide such time complexity, data structure uses additional index that have to be quite complex for large alphabets. Let\u0026rsquo;s say that in general case the index is compressed and we don\u0026rsquo;t know it\u0026rsquo;s actual size ahead of time. The size of index is a variable value depending of a sequence content.\u003c/p\u003e\n\u003cp\u003eSo the sequence has at least two resizable blocks: index, and symbols.\u003c/p\u003e\n\u003cp\u003eBelow there is a code snipped explaining how update operations on the object interact with its allocator(s).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\ntemplate \u0026lt;int BitsPerSymbol\u0026gt;\nclass SearchableSequence: public PackedAllocator {\n  typedef PackedAllocator                              Base;\n  typedef SearchableSequence\u0026lt;BitsPerSymbol\u0026gt;            MyType;\n\n  typedef unsigned int                                 Symbol;\n\n  // Instantiate index only if the sequence is larger\n  // than this value.\n  static const int IndexSizeThreshold                  = 64;\n\n  // no members should be declared here\n  // use Metadata class instead\npublic:\n  class Metadata {\n    int size_;\n    int\u0026amp; size() {return size_;} \n    const int\u0026amp; size() const {return size_;} \n  };\n\n  class Index: public PackedAllocator {\n    // index data structure for this SearchableSequence\n  };\n\n  // This enum codes memory block indexes.\n  enum {\n    METADATA, // Metadata block\n    INDEX,    // Indexes block\n    SYMBOLS   // Symbols block\n  };\n\n  // returns address of Metadata object\n  Metadata* metadata() {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n  const Metadata* metadata() const {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n\n  // returns address of Index object\n  Index* index() {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n  const Index* index() const {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n\n  // returns address of symbols block\n  Symbol* symbols() {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  const Symbol* symbols() const {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  \n  // returns size in bytes of empty sequence. this method is used by \n  // PackedAllocator::allocateEmpty(int) to get object\'s default size\n  static int empty_size() \n  {\n    int allocator_size = Base::empty_size(); // size of allocator itself\n    int metadata_size  = Base::round(sizeof(Metadata)); // size of metadata block\n    int index_size     = 0; // index is empty for empty sequence\n    int symbols_size   = 0; // symbols block is also empty for empty sequence\n\n    return allocator_size + metadata_size + index_size + symbols_size;\n  }\n\n  void init() \n  {\n    Base::init(3); // the object has three resizable sections.\n    \n    // Allocate metadata block and initialize it\n    Base::template allocate\u0026lt;Metadata\u0026gt;(METADATA);\n\n    // Allocate empty block for index. Do not initialize it\n    Base::template allocate\u0026lt;Index\u0026gt;(INDEX, 0);\n    \n    // Allocate empty block for symbols. \n    Base::template allocate\u0026lt;Symbol\u0026gt;(SYMBOLS, 0);    \n  }\n\n  // change the value of idx-th symbol\n  void setSymbol(int idx, int symbol);\n\n  // insert new symbol at the specified position\n  int insert(int idx, int symbol) \n  {\n    enlarge(1);              // enalrge SYMBOLS block\n    insertSpace(idx, 1);     // shift symbols\n    setSymbol(idx, symbol);  // set new symbol value\n\n    reindex();               // update search index\n  }\n\n  int size() const \n  {\n    return metadata()-\u0026gt;size();\n  }\n\n  // update index for the searchable sequence\n  void reindex() \n  {\n     // check if the sequence if large enough to have index\n     if (size() \u0026gt; IndexSizeThreshold) \n     {\n       if (Base::size(INDEX) == 0)\n       {\n         Base::template allocate\u0026lt;Index\u0026gt;(0); // create empty index if it doesn\'t exist\n       }\n\n       Index* index = this-\u0026gt;index();\n\n       // compute index size for given symbols and resize the index.\n       index-\u0026gt;resize_for(this-\u0026gt;symbols(), size());\n\n       // rebuild the index\n       // note that any resize operation invalidates pointers to blocks\n       // going after the resized one.\n       index-\u0026gt;update(this-\u0026gt;symbols(), size());\n     }\n     else {\n       // the sequence if not full enough to have the index,\n       // free it.\n       Base::resize(INDEX, 0); // free index block\n     }\n  }\n  \nprivate:\n  // insert empty space into symbols\'s data block,\n  // shift symbols in the range [idx, size()) \'length\' positions right.\n  void insertSpace(int idx, int length);\n\n  // returns symbols block size for specified number of symbols\n  static int symbols_size(int symbols);\n\n  //enlarge symbols block by \'items\' elements.\n  void enlarge(int items) \n  {\n    // get new size for SYMBOLS block\n    int new_symbols_size = MyType::symbols_size(size() + items)\n    \n    // enlarge SYMBOLS block\n    Base::resize(SYMBOLS, new_symbols_size);\n  }\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis pattern is used intensively for packed data structures in Memoria.\u003c/p\u003e\n'}).add({id:15,href:"/docs/data-zoo/associative-memory-1/",title:"Associative Memory (Part 1)",description:null,content:'\u003ch2 id="what-is-associative-memory"\u003eWhat is Associative Memory\u003c/h2\u003e\n\u003cp\u003eAssociative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of \u003cem\u003eMemory\u003c/em\u003e. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of \u003cem\u003ebits\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="motivating-example-rdf"\u003eMotivating Example: RDF\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href="https://en.wikipedia.org/wiki/Resource_Description_Framework"\u003eResource Description Framework\u003c/a\u003e data is modelled with a special form of a labelled graph, consisting from \u003cem\u003efacts\u003c/em\u003e (represented as URIs) and \u003cem\u003etriples\u003c/em\u003e in a form of $(Subject, Predicate, Object)$ linking various facts together. Logical representation of this \u003cem\u003esemantic graph\u003c/em\u003e is a table, enumerating all the triples in the graph. The main operation on the graph is \u003cem\u003epattern matching\u003c/em\u003e using SQL-like query language \u003ca href="https://en.wikipedia.org/wiki/SPARQL"\u003eSPARQL\u003c/a\u003e. Another common mode of operations over graphs is traversal, but this mode is secondary for semantic graphs. Pattern-matching in semantic graphs is based of \u003cem\u003eself-joins\u003c/em\u003e over the triple tables:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="triples.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBecause of the flexibility of SQPRQL, self-joins can be performed on any combination of Subject, Predicate and Objects, and the join on objects is the main reason why basic representation of graphs for RDF is a relational table. And, if we want fast execution, this table has to be properly indexed.\u003c/p\u003e\n\u003cp\u003eThe simplest way to provide indexing over a triple table is to sort it in some order, but ordered table only allows ordered \u003cem\u003ecomposite\u003c/em\u003e keys. If a table is ordered like (S, P, O), that means $(Subject, Predicate, Object)$, \u003cem\u003ethen\u003c/em\u003e we can search first by Subject, \u003cem\u003ethen\u003c/em\u003e buy Predicate, and only then by Object. But not vise versa. If we want to search by an Object first, we need another table ordered by object: (O, X, Y). To be able to search in any order we need all possible permutations of S, P and O: it\u0026rsquo;s $3! = 6$ tables.\u003c/p\u003e\n\u003cp\u003eSo, fully indexed RDF triple store will need at least 6 triple tables. How many is it?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times the size of the original single-table store (not counting URIs and Objects if they are stored separately).\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times slower insertions if they are not paralleled. Of course we can make many insertions in parallel, but it\u0026rsquo;s \u003cem\u003e6 times more energy\u003c/em\u003e anyway.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModern triple stores model semantic graphs with \u003cem\u003equads\u003c/em\u003e, adding \u003cem\u003eGraph ID\u003c/em\u003e to a triple, so nodes can link to entire graphs, no just other nodes (self-referentiality). In addition to, higher-dimensional tables ($D \u0026gt; 4$) can also be provided to speedup certain queries. And, in general case, if we have $D$-dimensional data table, we need \u003cem\u003eup to\u003c/em\u003e $D!$ orderings of this table. That is 24 for $D=4$ and grows faster than an exponent.\u003c/p\u003e\n\u003cp\u003eSorting relational tables does not scale at all for higher-order graphs ($D \u0026gt; 3$), but we can do better.\u003c/p\u003e\n\u003ch2 id="definition-of-associative-memory"\u003eDefinition of Associative Memory\u003c/h2\u003e\n\u003cp\u003eSo, without loss of generality, associative memory is a $D$-dimensional relation $R$ with \u003cem\u003eset\u003c/em\u003e semantics, over integer numbers drawn from some finite set (domain). The main operation on the memory is \u003cem\u003elookup\u003c/em\u003e that can be performed using arbitrary number of dimensions, specifying \u003cem\u003ematch\u003c/em\u003e, \u003cem\u003erange\u003c/em\u003e or \u003cem\u003epoint\u003c/em\u003e lookup, or any combination of for any number of dimensions. \u003cem\u003eRecall\u003c/em\u003e is the result of \u003cem\u003elookup\u003c/em\u003e operation, and is enumeration of all entries in $R$ matching the query.\u003c/p\u003e\n\u003cp\u003eAssociative memory can be either \u003cem\u003estatic\u003c/em\u003e, if only lookups are allowed. Or \u003cem\u003edynamic\u003c/em\u003e, if it supports insert, update and delete operation for individual elements (Update operation can be reduced to delete + insert).\u003c/p\u003e\n\u003ch2 id="multiscale-decomposition"\u003eMultiscale Decomposition\u003c/h2\u003e\n\u003cp\u003eLet we have a $D$-dimensional relation $R = \\lbrace {r_0, r_1, r_2, \u0026hellip;, r_{N-1}}\\rbrace$, representing a $set$, where $r_i = (c_0, c_1, \u0026hellip;, c_{D-1})_i$ - а $D$-dimensional tuple and $N$ is a \u0026lsquo;size\u0026rsquo; of the table (number of elements in the set). $c_{i,j}$ is a table\u0026rsquo;s cell value from row $i$ and dimension (column) $j$. Each cell value $c_{i,j}$ has a domain of $H$ bits.\u003c/p\u003e\n\u003cp\u003eExample: A set of 8x8 images with 8 bits per pixel can be represented with 64-dimensional relation with $H = 8$. Maximal number of images in a set is $N \u0026lt;= 8^{64} = 2^{192}$. Given such table we can easily define an associative memory reducing content-addressable lookup to linear table scans and bit manipulations (time complexity is $O(N)$). And, actually, this is how it\u0026rsquo;s implemented for approximate nearest neighbour search on massively-parallel hardware. Parallel linear scan is fast, but it\u0026rsquo;s not scalable (fast memory is expensive) and it\u0026rsquo;s not energy-efficient.\u003c/p\u003e\n\u003cp\u003eFortunately, we can transform $O(N)$ into $O(P H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e, where $1 \u0026lt;= P \u0026lt;= 2^D$ \u0026ndash; average number of nodes per \u003cstrong\u003ebucket\u003c/strong\u003e (see below), that is, thanks to the \u003ca href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality"\u003e\u0026ldquo;Blessing of Dimensionality\u0026rdquo;\u003c/a\u003e, usually tends to 1. And $M$ is a \u003cem\u003erecall size\u003c/em\u003e, number of points returned by the query over $R$.\u003c/p\u003e\n\u003cp\u003eTo perform the multiscale decomposition of relation $R$, we need to perform the following transformation for each row $r_i$:\u003c/p\u003e\n\u003cp\u003eLet $c_{ij} = S_{ij} = (s_0, s_1,s_2,\u0026hellip;,s_{H-1})_{ij}$, where $s \\in \\lbrace{0, 1}\\rbrace$ is a bit-string representation of cell value $c$. Let $s_h = B(S, h)$ \u0026ndash; $h$-th bit of string $S$.\u003c/p\u003e\n\u003cp\u003eNow, $M(r)$ is a multiscale decomposition of a row $r = (S_0, S_2, \u0026hellip;, S_{D-1})$. Informally, $M(r)$ is a bit string consisting from a concatenation of shuffling of all bits form $S_j$:\u003c/p\u003e\n\u003cp\u003e$M(r) = B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip;B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip; B(S_0, 0) B(S_1, 0) \u0026hellip; B(S_{D-1}, 0)$.\u003c/p\u003e\n\u003cp\u003eThe symbol $|$ is added to graphically separate $H$ \u003cem\u003elayers\u003c/em\u003e of the multiscale representation from each other.\u003c/p\u003e\n\u003cp\u003eExample. Let $r = (100, 110, 001)$. Then $M(r) = 111|010|001$. Note, that in some sense, $M(r)$ is producing a point on a \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003e$Z$-order curve\u003c/a\u003e for $r$. This correspondence may help in some applications.\u003c/p\u003e\n\u003cp\u003eSo, multiscale decomposition of $T = M(R)$ converts a table with $D$ columns into a table with $H$ columns, which are called \u003cem\u003elayers\u003c/em\u003e here.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s assume, that the table $T$ is sorted in a bit-lexicographic order.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="tables.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is special about table $T$ is that every column $L_j$ contains some information form each column $D_j$ from table $R$. So, by searching in a column of $T$ we can search in all columns from $R$ at once. Table $T$ itself does not provide any speedup over the sorted $R$ because the number of rows is still the same as for table $R$. But quick look at the table will show that there are many \u003cem\u003erepetitions\u003c/em\u003e. So, we can transform $T$ into a tree, by hierarchically (here, from left to right) collapsing repetitive elements in tables\' columns. Now, a \u003cstrong\u003ebucket\u003c/strong\u003e is a list of all children of the same parent node sorted lexicographically. It can be shown, that there may be \u003cem\u003eat most\u003c/em\u003e $2^D$ elements in a bucket. So, search in such data structure is $O(2^D H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. If $D$ is small, say, 16 or less, this may dramatically improve performance relative to linear scan of $R$ (even if it\u0026rsquo;s sorted). See the \u003ca href="#analysis"\u003eAnalysis\u003c/a\u003e section below for additional properties and limitations.\u003c/p\u003e\n\u003cp\u003eNote that each path from root to leaf in the tree encodes a single row in the table $T$, and after the inverse multiscale decomposition, in $R$.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s demonstrate how search works. Let we want to enumerate all rows in $R$ with $D_2$ = 0111, so $Q = (X, Y, 0111)$, where $X$ and $Y$ are \u003cem\u003eplacehoders\u003c/em\u003e. First, we need a multiscale representation of $Q$, $M(Q) = xy0|xy1|xy1|xy1$. Now, we need to traverse the tree from root to leafs, according to this pattern:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="search.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHaving the multiscale query encoding, the tree traversal is straightforward. We are visiting sub-trees in-order, providing that current pattern matches the node\u0026rsquo;s label. Visiting the leaf (+ its label is matched) means full match. The excess number of nodes visited by a range query is called \u003cem\u003eoverwork\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this example, the selectivity of the query is pretty good. All leafs matching the query are being visited (visited nodes are drawn in the dotted-green line style). Note the nodes marked with (*). The traversal process visited some nodes \u003cem\u003enot leading to the full match\u003c/em\u003e. This is why type complexity estimation for this data structure is logarithmic \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. Its performance depends on how data is distributed in the tree.\u003c/p\u003e\n\u003ch2 id="using-louds-for-the-tree"\u003eUsing LOUDS for the Tree\u003c/h2\u003e\n\u003cp\u003eTable $T$ has the same size (in bits) as the table $R$, but the tree, if implemented using pointer-based structure, will add a lot to the table representation. Fortunately, we can use \u003ca href="/docs/data-zoo/louds-tree"\u003eLOUDS Tree\u003c/a\u003e to encode the tree. LOUDS tree has very small memory footprint, only 2 bits per node (+ a small overhead, like 10%, for rank/select indices). Search tree size estimation is at most $NH$ nodes, so the tree itself will take at most size of two columns of the table $R$ ($2NH$ + small overhead). In most cases LOUDS-encoded $T$ will take less space that original table $R$, including auxiliary data structures.\u003c/p\u003e\n\u003cp\u003eWhat is the most important for LOUDS Tree is that it\u0026rsquo;s structured in memory in a linear order. So, if for some reason a spatial tree traversal degrades into linear search, the tree will be pretty good at this. We just need to read many layers of the tree in parallel. Such I/O operations can be efficiently prefetched.\u003c/p\u003e\n\u003ch2 id="analysis"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eIt can be shown that the tree is very similar to a trie-based Quad Tree (for high dimensions), so most expected (average-case) and worst-case estimations also apply. In worst case, for high-dimensional trees, traversal degenerates into linear a search. Fortunately for LOUDS, it\u0026rsquo;s both memory-efficient for linear search \u003cem\u003eand\u003c/em\u003e for tree traversal. But on average, overwork is moderate, if doesn\u0026rsquo;t even tend to zero. And queries perform pretty well.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s return to RDF triple stores and compare things to each other. Let\u0026rsquo;s assume that $D = 3$ (number of dimensions) and $H = 32$ (resource identified size or search tree depth). So, in case of insertion of a triple, we have to perform 6 insertions into 6 triple tables (for each ordering) and 32 insertions in case of the tree (into each tree level). Reading is also slower: one lookup in a sorted table vs 32 lookups in the tree. It looks unreasonable to switch from triple tables (worst case logarithmic) to search tree (average case logarithmic), unless we are limited in memory and want to fit as many triples as possible into the available amount. But things start changing when we go into higher dimension ($D \u0026gt; 3$) and need more indices to speedup our queries.\u003c/p\u003e\n\u003cp\u003eSo far\u0026hellip;\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePoint-like operations (to check if some row exists in the relation $R$) will take $O(D log(N))$ time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRange search in the quad trees is logarithmic on average, but it\u0026rsquo;s relatively easy to build a worst-case example, when performance degrades to a linear scan. For example, if $D = 64$, maximal bucket size is $2^{64}$, that is much larger than any practical $N$ (number of entries in $R$). Unless the data is distributed uniformly among \u003cem\u003edifferent levels\u003c/em\u003e of the tree, we will end up having a few but very big buckets. So, special care must be taken on how we map our high-level data to dimensions of the tree. Random mapping is usually a safe bet, but always the best choice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the search tree \u0026ldquo;Blessing of Dimensionality\u0026rdquo; is fighting with \u0026ldquo;Curse of Dimensionality\u0026rdquo;, it\u0026rsquo;s kind of $0 \\cdot \\infty$. In higher dimensions data tends to be extremely \u003cem\u003esparse\u003c/em\u003e because volume size grows exponentially with number of dimensions. So, normally, even in high dimensions, buckets will tend to have small number of elements. The bigger the number \u0026ndash; the better, because it improves data compression and speeds up queries. But beware of the worst case, when the tree has one big bucket that all queries are visiting. It has also been observed for similar K-d trees, that with higher number of dimensions, \u003cem\u003eoverwork\u003c/em\u003e also tens to increase (the blessing vs curse situation, $0 \\cdot \\infty$, who wins?).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn case if LOUDS tree is dynamic and implemented using B-tree, \u003cem\u003einsert\u003c/em\u003e, \u003cem\u003eupdate\u003c/em\u003e and \u003cem\u003edelete\u003c/em\u003e operations to the search tree have $O(H log(N))$ time complexity for point-like updates and $O(H(log(N) + M))$ for batch updates of size $M$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe data structure representation in memory is very compact. It\u0026rsquo;s the size of original table $R$ + (up to) the size of two columns from $R$ for LOUDS tree + 5-10% of the tree to auxiliary data structures. Overhead of the tree is constant and is amortizing with higher number of dimensions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEven if we work with high-dimensional data, and we are losing to the curse of dimensionality, it\u0026rsquo;s possible to perform approximate queries. Many applications where high-dimensional data analysis is required, like AI, are essentially approximate. LOUDS tree allows to compactly and efficiently remember additional bits of information with each node, to facilitate approximate queries (if necessary).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS tree-based Associative memory seems to be impractical specifically for RDF triple stores, but if hardware accelerated, can be cost-effective at scale, providing also many other benefits, not just memory savings (which are huge for higher dimensions). The bottleneck is on the update operations, where insertion and deletion may require tens of B-tree updates. Fortunately, this operation is well-parallelizable so we can use thousands of small RISC-V cores equipped with special command for direct and energy-efficient implementation of essential operations (partial/prefix sums, rank and select). An array or cluster of such cores can even be embedded into \u003ca href="/subprojects/smart-storage"\u003estorage memory controller\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAdvanced data structures like LOUDS-based associative memory, considered to be impractical in the past, relative to more traditional things, like sorted tables and pointer-based data structures (trees, lists, graphs, \u0026hellip;) for main memory. But progress in computer hardware makes task-specific hardware accelerator a much more viable options, opening the road to completely new applications.\u003c/p\u003e\n\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-2"\u003enext post\u003c/a\u003e it will be shown how LOUDD-backed associative memory can be used for generic function approximation and inversion.\u003c/p\u003e\n'}).add({id:16,href:"/docs/data-zoo/associative-memory-2/",title:"Associative Memory (Part 2)",description:"",content:'\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-1"\u003eprevious part\u003c/a\u003e we saw how LOUDS tree can be used for generic compact and efficient associative associative memory over arbitrary number of dimensions. In this part we will see how LOUDS trees can be used for function approximation and inversion.\u003c/p\u003e\n\u003ch2 id="definitions"\u003eDefinitions\u003c/h2\u003e\n\u003cp\u003eLet $[0, 1] \\in R$ is the domain and range we are operating on. $D$ is a number of dimensions of our space, and for the sake of visualizability, $D = 2$.\u003c/p\u003e\n\u003cp\u003eLet $\\vec{X}$ is a vector encoding a point in $[0, 1]^D$, $\\vec{X} = \u0026lt;x_0, x_1, \u0026hellip;, x_{D-1}\u0026gt;$. Let $str(x [, h])$ is a function converting a real number from $[0, 1]$ into a binary string by taking a binary representation of a real number to a form like \u0026ldquo;$0.0010111010\u0026hellip;$\u0026rdquo;, and removing leading \u0026ldquo;$0.$\u0026rdquo; and trailing \u0026ldquo;$0\u0026hellip;$\u0026rdquo;. so, $str(0.181640625) = 001011101$. If $h$ argument is specified for $str(\\cdot, \\cdot)$, then resulting string is trimmed to $h$ binary digits, if it\u0026rsquo;s longer than that.\u003c/p\u003e\n\u003cp\u003eLet $len(x)$ is a number of digits in the result of $str(x)$. Note that $len(\\pi / 10) = \\infty$, so irrational numbers are literally infinite in this notation. Let $H$ is a maximal \u003cem\u003edepth\u003c/em\u003e of data, and there is some \u003cem\u003eimplicitly assumed\u003c/em\u003e arbitrary value for $h$, like 32 or 64, or even 128. So we can work with \u003cem\u003eapproximations\u003c/em\u003e of irrational and transcendent numbers, or with long rational numbers in a same way and without loss of generality.\u003c/p\u003e\n\u003cp\u003eLet $len(\\vec{X}) = max_{\\substack{i \\in \\lbrace 0,\u0026hellip;,D-1 \\rbrace }}(len(x_i))$.\u003c/p\u003e\n\u003cp\u003eLet $str(\\vec{X}) = (str(x_0),\u0026hellip;, str(x_{D-1}))$ is a string representation (a tuple) of $\\vec{X}$. And let we assume, elements of the tuple are implicitly extended with \u0026lsquo;$0$\u0026rsquo; from the right, if their length is less than $len(\\vec{X})$. In other words, all elements (binary string) of a tuple are implicitly of the same length.\u003c/p\u003e\n\u003cp\u003eLet $str(\\lbrace \\vec{X_0}, \\vec{X_1}, \u0026hellip; \\rbrace) = \\lbrace str(\\vec{X_0}), str(\\vec{X_1}), \u0026hellip; \\rbrace$. String representation of set of vectors is a set of string representation of individual vectors.\u003c/p\u003e\n\u003cp\u003eLet $M(\\vec{X}) = M(str(\\vec{X}))$ is a \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003emultiscale transformation\u003c/a\u003e of binary string representation of $\\vec{X}$. Informally, to compute $M(\\vec{X})$ we need to take all strings from its string representation (the tuple of binary strings) and produce another string of length $len(\\vec{X})  D$ by concatenating interleaved bits from each binary string in the tuple.\u003c/p\u003e\n\u003cp\u003eExample. Let $H = 3$ and $D = 2$. $\\vec{X} = \u0026lt;0.625, 0.25\u0026gt;$, $str(\\vec{X}) = (101, 01)$ and $M(\\vec{X}) = 10|01|10$. Note that signs $|$ are added to separate $H$ \u003cem\u003epath elements\u003c/em\u003e in the recording, they are here for the sake of visualization and are not a part of the representation. The string $10|01|10$ is also called \u003cem\u003epath expression\u003c/em\u003e because it\u0026rsquo;s a unique path in the multidimensional space decomposition \u003cem\u003eencoding\u003c/em\u003e the position of $\\vec{X}$ in this decomposition. Visually:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiscale1.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that the order in which \u003cem\u003epath elements\u003c/em\u003e of a path expression enumerate the volume is \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003eZ-order\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGiven a \u003cem\u003eset\u003c/em\u003e of $D$-dimensional vectors $\\lbrace \\vec{X} \\rbrace$, it\u0026rsquo;s multiscale transformation can be represented as a $D$-dimensional \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e. Such quad tree can be represented with a \u003ca href="/docs/data-zoo/louds-tree/#cardinal-trees"\u003ecardinal LOUDS tree\u003c/a\u003e of degree $2^D$. Here, implicit parameter $H$ is a \u003cem\u003emaximal height\u003c/em\u003e of the Quad Tree.\u003c/p\u003e\n\u003ch2 id="basic-asymptotic-complexity"\u003eBasic Asymptotic Complexity\u003c/h2\u003e\n\u003cp\u003eGiven that $N = |\\lbrace \\vec{X} \\rbrace|$, and given that LOUDS tree is dynamic (represented internally as a b-tree), the following complexity estimations apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInsertion and deletion\u003c/strong\u003e of a point is $O(H log(N))$. Update is semantically not defined because it\u0026rsquo;s a \u003cem\u003eset\u003c/em\u003e. Batch updates in Z-order are $O(H (log(N) + B))$, where $B$ is a batch size. Otherwise can be slightly worse, up to $O(H log(N) B)$ in the worst case.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint lookup\u003c/strong\u003e (membership query) is $O(D H log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRange and projection\u003c/strong\u003e queries are $O(D H log(N) + M)$ \u003cem\u003eon average\u003c/em\u003e, where $M$ is a recall size (number of vectors matching the query). In worst case tree traversal degrades to the linear scan of the entire set of vectors.\u003c/li\u003e\n\u003cli\u003eThe data structure is \u003cstrong\u003espace efficient\u003c/strong\u003e. 2 bits per LOUDS tree node + \u003cem\u003ecompressed bitmap\u003c/em\u003e of cardinal labels. For most usage scenarios, space complexity will be within \u003cstrong\u003e2x the raw bit size\u003c/strong\u003e of $str(\\lbrace \\vec{X} \\rbrace)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="functions-approximation-using-quad-trees"\u003eFunctions Approximation Using Quad Trees\u003c/h2\u003e\n\u003cp\u003eLet we have some function $y = f(x)$, and we also have the graph of this function on $[0, 1]^2$. If function $f(\\cdot)$ is elementary, or we have another way to compute it, it\u0026rsquo;s computable (for us). What if we have $f(\\cdot)$, but we want to compute inverse function: $x = f^{-1}(y)$? With compact quad trees we can \u003cem\u003eapproximate\u003c/em\u003e both functions out of the same \u003cem\u003efunction graph\u003c/em\u003e using compact quad trees:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="function.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere the tree has four layers shown upside down (drawing more detailed layers above less detailed ones). And if we want to compute $f(a)$, where $str(a) = a_3a_2a_1a_0$, the path expression will be $a_3x|a_2x|a_1x|a_0x$, where $x$ here is a \u003cem\u003eplaceholder sign\u003c/em\u003e. Now we need to traverse the tree as it defined in \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003eprevious part\u003c/a\u003e. If there is a point for $a$ of a function graph, it will be found in a logarithmic expected time. The path expression for $f^{-1}(b)$ will be $b_3x|b_2x|b_1x|b_0x$.\u003c/p\u003e\n\u003cp\u003eNote that compressed cardinal LOUDS tree will use less than 4 bits (+ some % of auxiliary data) \u003cem\u003eper square\u003c/em\u003e on the graph above. Sol, looking into this graph we already can say something specific about what will be the cost of approximation, depending on required precision (maximal $H$).\u003c/p\u003e\n\u003ch2 id="function-compression"\u003eFunction Compression\u003c/h2\u003e\n\u003cp\u003eLet we have a function that checks if some point is inside some ellipse:\u003c/p\u003e\n$$\ny = f(x_1, x_2): \\begin{cases} \n    1 \u0026\\text{if } (x_1, x_2) \\text{ is inside the the ellipse,} \\\\ \n    0 \u0026\\text{if it\'s outside.} \n\\end{cases}\n$$\n\u003cp\u003eThis function defines some \u003cem\u003earea\u003c/em\u003e on the graph. Let $N$ is the number of \u0026ldquo;pixels\u0026rdquo; we need to define the function $f(x,y)$ on a graph. Then, using compressed quad trees, we can do it with ${O(\\sqrt{N})}$ bits \u003cem\u003eon average\u003c/em\u003e for 2D space :\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="region.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe square root here is because we need \u0026ldquo;detailed\u0026rdquo; only for the border of the area, while \u0026ldquo;in-lands\u0026rdquo; needs much lower resolution.\u003c/p\u003e\n\u003ch2 id="blessing-of-dimensionality-vs-curse-of-dimensionality"\u003eBlessing of Dimensionality vs Curse of Dimensionality\u003c/h2\u003e\n\u003cp\u003eLet we have 2D space and we have a tree encoding the following structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="corners.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThere are four points in each corner of the coordinate plane. As it can be obvious from the picture, each new level of the tree will add four new bits for just for cardinal labels (not including LOUDS tree itself): three \u0026lsquo;0\u0026rsquo; and one \u0026lsquo;1\u0026rsquo; (in the corresponding corners). Now if we go to higher dimensions, for 3D we will have 8 new bits, for 8D \u0026ndash; 256 new bits and for 32D \u0026ndash; $2^32$ bits. This is \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e (CoD) for spatial data structures: volume grows exponentially with dimensions, so linear sizes \u0026ndash; too.\u003c/p\u003e\n\u003cp\u003eNevertheless, with higher dimensions, the volume is getting exponentially sparser, so we can use data compression techniques like RLE encoding to represent long sparse bitmaps for cardinal labels. This is \u003cstrong\u003eBlessing of Dimensionality\u003c/strong\u003e (BoD). For \u003cem\u003ecompressed\u003c/em\u003e LOUDS cardinal trees, the example above will require $O(D)$ bits per quadrant per tree layer for $D$-dimensional Quad Tree.\u003c/p\u003e\n\u003cp\u003eSo, the the whole idea of compression in this context is implicit (or in-place) \u003cstrong\u003eDimensionality Reduction\u003c/strong\u003e. Compressed data structure don\u0026rsquo;t degrade so fast as their uncompressed analogs, yet maintain the same \u003cem\u003elogical API\u003c/em\u003e. Nevertheless, data compression is not the final cure for CoD, because practical compression itself is not that powerful, especially in the case of using RLE for bitmap compression. So, in each practical case high-dimensional tree can become \u0026ldquo;unstable\u0026rdquo; and \u0026ldquo;explode\u0026rdquo; in size. Fortunately, such highly-dimensional data ($D \u0026gt; 16$) is rarely makes sense to work with directly (without prior dimensionality reduction).\u003c/p\u003e\n\u003cp\u003eFor example, for $D=8$ exponential effects in space are still pretty moderate (256-degree cardinal tree), yet 8 dimensions is already a good approximation of real objects in symbolic methods. High-dimensional structures are effectively \u003cem\u003eblack boxes\u003c/em\u003e for us, because our visual intuitions about properties of objects don\u0026rsquo;t work in \u003ca href="https://www.math.wustl.edu/~feres/highdim"\u003ehigh dimensions\u003c/a\u003e. Like, volume of cube is concentrating in it\u0026rsquo;s corners (because there is an exponentional number of corners). Or the volume of sphere is concentrating near its surface, and many more\u0026hellip; Making decisions in high dimensions suffer from noise in data and machine rounding, because points tend to be very close to each other. And, of course, computing Euclidian distance does not make (much) sense.\u003c/p\u003e\n\u003ch2 id="comparison-with-multi-layer-perceptrons"\u003eComparison with Multi-Layer Perceptrons\u003c/h2\u003e\n\u003cp\u003eNeural networks has been known to be a pretty good function approximators, especially for multi-dimensional cases. Let\u0026rsquo;s check how compressed spatial tree can be compared with multi-layer perceptrons (MLP). This type of artificial neural networks is by no means the best example of ANNs, yet it\u0026rsquo;s a pretty ideomatic member of this family.\u003c/p\u003e\n\u003cp\u003eIn the core of MLP is the idea of \u003cem\u003elinear separability\u003c/em\u003e. In a bacis case, there are two regions of multidimensional space that can\u0026rsquo;t be separated by a hyperplane from each other. MLP has multiple ($K$) layers, where first $K-1$ layers perform specific space transformations using linear (weights) and non-linear (thresholds) operators in such way that $K$-th layer can perform the linear separation:\u003c/p\u003e\n\n\u003cdiv style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"\u003e\n  \u003ciframe src="https://www.youtube.com/embed/k-Ann9GIbP4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eSo, this condition is simplified of the picture below. Here, we have two classes (\u003cem\u003ered\u003c/em\u003e and \u003cem\u003egreen\u003c/em\u003e dots) with complex non-linear boundary between those classes. After transforming the space towards linear separation of those classes and making inverse transformation, the initial hyperplane (here, a line) is broken in many places:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is important, is that each such like break is an inverse transformation of the original hyperplane. Those transformations has to be described, and description will take some space. So we can speak about the \u003cstrong\u003edescriptional (Kolmogorov) complexity of the decision boundary\u003c/strong\u003e. Or, in the other way, \u003cem\u003ehow many neurons (parameters) we need to encode the decision boundary\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eFrom Algorithmic Information Theory it\u0026rsquo;s known that arbitrary string $s$, drawn from a uniform distribution, will be \u003cem\u003eincompressible\u003c/em\u003e with high probability, or $K(s) \\to |s|$. In other words, most mathematically possible objects are \u003cem\u003erandom\u003c/em\u003e-looking, we hardly can find and exploit any structure in them.\u003c/p\u003e\n\u003cp\u003eReturning back to MLP, it\u0026rsquo;s expected that in \u0026ldquo;generic case\u0026rdquo; decision boundaries will be \u003cem\u003ecomplex\u003c/em\u003e: the line between classes will have many breaks, so, may transformations will be needed to describe it with required precision (and this is even not taking CoD into account).\u003c/p\u003e\n\u003cp\u003eDescribing decision boundaries (DB) with compressed spatial trees may look like a bad idea from the first glance. MLPs encode DB with superpositions of elementary functions (hyperplanes and non-linear units). Quad Trees do it with hyper-cubes, and it\u0026rsquo;s obvious that we may need a lot of hyper-cubes in place of just one arbitrary hyper-plane. If it\u0026rsquo;s the case, we say that hyper-planes \u003cem\u003egeneralize\u003c/em\u003e DB \u003cem\u003emuch better\u003c/em\u003e than hyper-cubes:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBut it should hardly be a surprise, if in a real life case it will be find out that descriptions or the network and the corresponding tree are roughly the same.\u003c/p\u003e\n\u003cp\u003eSo, Neural Networks may generalize much better in some cases than compressed quad trees and perform better in very high dimensional spaces (they suffer less from CoD because of better generalization), but trees have the following benefits:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf computational complexity of MLP is $\\Omega(W)$ (a lot of large matrix multiplications), where $W$ is number of parameters, complexity of inference in the quad tree is \u003cem\u003eroughly\u003c/em\u003e from $O(log(N))$, where $N$ is number of bits of information in the tree. So trees may be much faster than networks \u003cem\u003eon average\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eQuad Trees are dynamic. Neural Networks require retraining in case of updates, at the same time adding (or removing) an element to the tree is $O(log(N))$ \u003cem\u003eworst case\u003c/em\u003e. It may be vital for may applications operating on-line, like robotics.\u003c/li\u003e\n\u003cli\u003eQuad Trees support \u0026ldquo;inverse inference\u0026rdquo; mode, when we can specify classes (outputs) and see\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, compressed quad trees may be much better for complex dynamic domains with tight decision boundaries with moderate number of dimensions (8-24). It\u0026rsquo;s not that clear yet how trees will perform in real life applications. Memoria is providing (1) \u003cem\u003eexperimental\u003c/em\u003e compressed dynamic cardinal LOUDS tree for low dimensional spaces (2 - 64).\u003c/p\u003e\n\u003cp\u003e(1) Not yet ready at the time of writing.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eThe main point of hardware acceleration of compressed Quad Trees is that inference may be really cheap (on average). It\u0026rsquo;s just a bunch of memory lookups (it\u0026rsquo;s a \u003cem\u003ememory-bound\u003c/em\u003e problem). Matrix multipliers, from other size, are also pretty energy efficient. Nevertheless, \u003cem\u003etrees scale better with complexity of decision boundaries\u003c/em\u003e.\u003c/p\u003e\n'}).add({id:17,href:"/docs/storage/overview/",title:"Storage Engines Overview",description:"",content:""}).add({id:18,href:"/docs/storage/memory-store/",title:"Memory Store",description:"",content:""}).add({id:19,href:"/docs/storage/swmr-store/",title:"SWMR Store",description:"",content:""}).add({id:20,href:"/docs/storage/overlay-store/",title:"Overlay Store",description:"",content:""}).add({id:21,href:"/docs/d-phil/intro/",title:"Philosophy of Memoria - Intro",description:"",content:""}).add({id:22,href:"/docs/d-phil/ai/",title:"Memoria and Artificial Intelligence",description:"",content:'\u003cp\u003e\u003cstrong\u003eNote that this page is work in progress, as well as other parts of Memoria. So be patient. Thanks!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSince AI is getting a hot topic and many folks started asking me about participating in AGI startups, I decided to outline my view on the topic using easy language. The following is my (Victor Smirnov) personal view on AI that other committers and contributors may not necessary share or support.\u003c/p\u003e\n\u003ch2 id="too-long-dont-read"\u003eToo Long; Don\u0026rsquo;t Read\u003c/h2\u003e\n\u003cp\u003eMy views on AGI are largely influenced by or mostly consistent with the following theories:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eGraziano\u0026rsquo;s Attention Schema Theory (AST), explaining how self-referentiality contributes to phenomenal reports.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDamasio\u0026rsquo;s Somatic Markers Theory (SMT) explaining basics of emotional intelligence: guiding function of emotions in decision making.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJean-Louis Dessalles\' Simplicity Theory explaining subjective attractiveness in a fundamental way.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTononi\u0026rsquo;s Integrated Information Theory (IIT), linking conscious states with descriptional complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDaniel Dennett\u0026rsquo;s Cartesian Theater and Multiple Drafts. Consciousness is not an illusion in a strict sense. But traditional first-person view on it is heavily biased, that leads to various paradoxes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRay Solomonoff\u0026rsquo;s theory of Universal Induction (UI). This theory explains theoretical and practical limits of problem solving.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJurgen Schmidhuber\u0026rsquo;s Artificial Curiosity (AC) and his theory of intrinsic reward, explaining highest emotions in a mathematically universal way.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSchmidhuber\u0026rsquo;s Goedel Machine, explaining limits of recursive self-improvements.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAST gives clever insights into how certain important subjective phenomena emerges in a self-referential structures like \u0026ldquo;attention schemes\u0026rdquo; but it leaves some important questions unexplored. In this page I introduce a semi-formal self-referential machine, self-referential algorithms and self-referential data structures constituting self-models. There is nothing special about them from mathematical perspective, but they have certain properties by turning computational phenomena into descriptions. For example, how gradual incompleteness of self-referentiality leads to the lack of \u0026ldquo;process introspection\u0026rdquo; and, by that, leads to \u0026ldquo;inexplicable\u0026rdquo; elements in phenomenal reports, like qualia, which are\u003c/p\u003e\n\u003cp\u003eDamassio introduces guiding function of emotions in its SMT that manifests by affecting decision making in an unconscious way. Schmidhuber proposes universal scheme for certain higher emotions, but emotions are not feelings. Conceptual jump from emotions to feelings requires subjectivity, and I\u0026rsquo;ll try to show how feelings are self-models of emotions.\u003c/p\u003e\n\u003cp\u003eSelf-models can be learned directly from data via induction. Solomonoff\u0026rsquo;s UI is complete but not computable. Computable induction can\u0026rsquo;t be complete, but can be approximated with various techniques. If self-models are expressive enough to emulate universal computer, they can be used to host they inductive learning, giving functionally complete metacognition. It turns out that phenomenal consciousness is not just a side-effect of intelligence, it\u0026rsquo;s a substrate-independent AI by itself, capable for problem solving.\u003c/p\u003e\n\u003cp\u003eHard problem of consciousness then can be reformulated the following way: \u003cstrong\u003ecan all phenomenal reports be composable and decomposable in a computable way\u003c/strong\u003e. To prove this, it is necessary to provide minimalistic but universal self-model that can evolve with algorithmic induction into any human phenomenal report \u003cem\u003ein the limit\u003c/em\u003e. So, no subjective phenomena will be left unexplained in the limit.\u003c/p\u003e\n\u003cp\u003eMemoria provides decent environment for self-modeling. It has framework for advanced data structures like self-indexes, that can turn self-models into advanced databases, and allows deploying such structures at scale in distributed and decentralized manner. Memoria has vertically integrated AIO subsystem from raw block devices to networking and C++-based heterogeneous computations unifing CPUs, GPUs and ASICs. It\u0026rsquo;s primary goal to make approximations of Universal Induction practical, which is necessary to evolve self-models.\u003c/p\u003e\n\u003ch2 id="a-sketch-of-main-ideas"\u003eA Sketch of Main Ideas\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThere are theoretical schemes for AGI, but they are not finitely computable. Any finitely computable AGI will not be complete or, in other words, truly generic. These schemes can be approximated, if we are agree to sacrifice some generality to achieve desirable performance in certain domains.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHuman brain is not an AGI in that narrow sense, though it can be true AGI in the limit, providing infinite time and space. Being an approximation of AGI, it can solve some problem classes efficiently (fast), being slow on all other classes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOne of possible AGI approximation strategies is trading memory for speed. AGI is based on search in Turing-complete programs space that is too huge to be tractable for anything except toy domains.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo speedup this process we can apply restrictions to the program space, so the search will be performed only within small subset of the space. The method is no more complete but can be flexibly tuned for very wide variety of problem classes just by changing the system of restrictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn most cases the system of such restrictions is poorly compressible, so the only way to specify it, is by direct description via data structures. In other words, an AGI approximation, built on this principle, will have huge database. The bigger the database, the more generic intelligence such a system will have. There are theoretical results that practical AGI will be complex in any means, and present complexity of advanced ANNs is reflecting this principle.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThis database will be much more complex than those ones currently in use. It will be based on very advanced coding schemes and data structures. Memoria is all about that.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt\u0026rsquo;s not that easy to determine classes of problems brain-AGI can solve efficiently: human-like AGI approximation problem.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHard problem of AI reformulated: does phenomenal consciousness have any problem-solving abilities by itself? Yes, it does.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSentiments and emotional Intelligence: emotions, feelings and guiding function of emotions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFrom emotion to feeling. Self-referential machines have a \u0026ldquo;dualistic\u0026rdquo; property represent computations as data and vise versa. They can build self-model allowing them to act on the stored log of their own previous states in the way increasing global and local utility functions. Entire universal Turing machine can be implemented this way, performing self-referential operations on self-referential data structures.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eParticularly, this type of machine can catch certain fundamental computational phenomena and use them as descriptions. For example, inherent incompleteness of self-referentiality (infeasibility to build a complete self-model) results in simplified self-models systematically lacking some causality links. Everything systematic (even if this systemacy is stochastic) can be turned into description.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor example, if we can somehow arrange this lack of causal structure in self-descriptions in a multi-scale way, we can turn corresponding records in memory logs into multiscale data structures, analogous to \u003ca href="https://bitbucket.org/vsmirnov/memoria/wiki/Multiary%20Wavelet%20Tree"\u003ewavelet trees\u003c/a\u003e, instantly enabling many sophisticated algorithms on top of such data structures.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA set of such structures with \u0026ldquo;dualistic\u0026rdquo; properties emerging in self-referential computations can be viewed as a formal language, describing \u0026ldquo;phenomenal reports\u0026rdquo; in memory logs of self-referential machine. This formal language can be Turing-complete, so, in principle we can build another self-referential machine inside a self-referential machine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, a Normalized Information Distance (NID) scheme can be build on top of this formal language, measuring similarities and differences between everything in a pretty generic way. NID-based encoding enables differential encoding when new data is represented as old data + some delta of novelty. NID can be efficiently approximated with Normalized Compression Distance (NCD). Simpler schemes are also possible.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNCD aligns naturally with mutiscale data structures from (12) and efficiently generalizes them to any domain where there is a more-or-less efficient compression scheme, that is Multiscale NCD-decomposition. NCD works fine with lossy compression.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eParticularly, self-referential decomposition of an object can efficiently implement its Mutiscale NCD decomposition relative to the entire history of the self-referential machine. In such decomposition some information, that is completely new to the machine, will appear as a very rude approximation or \u0026ldquo;vagueness\u0026rdquo; in phenomenal reports. Nevertheless, it contains some bits of actual data that contribute to the difference between vague elements of phenomenal report guiding decision-making.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf we define needs and emotions somehow for the self-referential machine, then motivations and feelings are self-models of needs and emotions correspondingly. Both needs and emotions are kind of internal stimulus, so motivations and feelings are sub-type of senses.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAll senses have \u0026ldquo;motivational\u0026rdquo; component that is a manifestation of guiding function of emotions to decision making reflected in phenomenal reports. It looks similar an attractor in non-linear dynamics, or gravity in physics. So, \u0026ldquo;something\u0026rdquo; happens behind the scene, at the level that is not accessible, and at the accessible level we have results of this hidden process.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf needful portrait codes the set of agent\u0026rsquo;s goals, and emotional portrait codes some stimulus relatively to the needful portrait, then sentiment is a self-model of emotional portrait of the stimulus.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUntil this moment it was not implied that self-referential machine has any special properties enabling it to learn like a human just because it resembles our core cognitive abilities. Nothing special is about such machine relative to learning. It\u0026rsquo;s implied that self-models are learned separately from self-referentiality with some kind of approximated program induction mentioned above: (1) - (7).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBut if a self-referential machine is expressive enough, it can host such inductive learning partially or entirely, resembling not just cognition as thinking but also cognition as obtaining new knowledge in a unified way. By separating cognition to self-referentiality and induction over it we are making entire problem much simpler to tackle.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe idea of self-referential machine\u0026rsquo;s solution to problem of qualia in AI is that there is no such thing as \u0026ldquo;red color\u0026rdquo;, the minimal element of memory log of such machine is \u0026ldquo;I see the red color\u0026rdquo;, that is not static strucure encoded in memory, but also a dynamic computation (in another context). In other words, some form of observer is always implied, and it is finally the machine itself. Note that this is a form of pan-proto-experientalism but in the \u0026ldquo;software\u0026rdquo; when higher-level experience is composed from lower-level ones in a computable way. Note also that this \u0026ldquo;experience\u0026rdquo; is also physical, because both the machine and information processing on it are physical.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFrom the functional perspective, qualia are soft bounds in building self-model of stimulus. Self-model developing is a computationally very hard task, but this complexity is not evenly distributed. Some elements of phenomenal report emerges faster than others. Some elements will remain underdeveloped forever and, because of their stable hardness, may become descriptions themselves, like qualia.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNote that if we accept that \u0026ldquo;rational\u0026rdquo; problem solving ability can be unsound and incomplete, then we can reduce rational intelligence to emotional one and implement it on the same computational substrate. Then such rationality will be heavily biased in many ways, and reduces to specific type of phenomenal report like \u0026ldquo;I\u0026rsquo;m reasoning logically here\u0026rdquo;. Note that sound and complete inference not feasible for everything except very narrow sets of problems.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe last question is \u0026ldquo;why\u0026rdquo; self-referentiality. Any on-line learning system must maintain consistency of its knowledge base, so it must somehow reason about itself. Any sufficiently large computer has non-perfect hardware that becomes more error-prone with size and has some kind of \u0026ldquo;homeostasis\u0026rdquo; to maintain. Self-referentiality emerges naturally in such environments. Moreover, all modern IT systems not just stores part of their computational states into logs, but can even act on this stored state, thus having some rudimentary self-model. Artificial consciousness is inevitable for sufficiently large and generic AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNote that while self-hosted induction in a self-referential machine does not explain consciousness in a strict philosophical sense, it transforms the problem from philosophical to functional domain, by explaining phenomenal reports in a functional way. If brain is indeed a self-referential machine, then it can improve its own process introspection in a monotonic way. If all subjective phenomena believed explainable in the limit this way by a person, then she can see herself as a machine phenomenally. Or, in another words, conceptual gap between self-aware person and machine disappears. That is what we need for upcoming age of AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIt\u0026rsquo;s the lack of process introspection what make us so unique in our own eye at the first place. Know you machine!\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eForm another side, phenomenal consciousness is not finitely explainable indeed, as some philosophers insist. I mean that while we can have complete functional model of consciousness in a form of certain self-referential machine, this machine will never have complete self-model, so some internal phenomena may only believed explainable in the limit, but never (in a finite lifetime) actually be explained.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThere is one interesting property of self-referential computations. Phenomenal reports are not directly accessible from outside. It\u0026rsquo;s can be possible to read memory of a self-referential machine directly, but in order to reconstruct back memory logs into phenomenal reports, we have to run the machine. So the only generic way to access phenomenal reports is to ask the machine about its current experience.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSo far, form technical perspective, we have two complex problem. First is basic self-referential machine, tuned to process and generate human-compatible phenomenal reports. This is a key requirement that is needed for a machine to understand human and vise versa.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecond, is a feasible induction approximation to learn this machine from data, because it will be not manually programmable, except for toy problems. Self-referential machine must be expressible enough to host this induction approximation schemes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs it has been already said, universal induction can be approximated by trading time for memory, by restricting program search space. This system of restrictions will be poorly compressible, so the only way to represent it is direct description in a form of a database. If for some subset of tasks such system is compressible, then it reduces to a finite system of heuristics.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria has long-term goals in hosting applications targeted these two problems. It\u0026rsquo;s an integrated data engineering framework providing (A) vertically integrated Asynchronous IO engine, starting from raw block storage to networking and visualization, (B) C++-based heterogeneous computing, merging CPU cores, GPUs and ASICs in a single data flow framework, and (C) confluently-persistent decentralized data structures, both basic ones like arrays, maps, sets, vectors, tables, trees, graphs, and advanced ones like mutiary wavelet trees.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria seamlessly integrates IO- and Compute-intensive tasks by making data as close as possible to computational resources.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBut unlike other AI-related platforms, Memoria does not specialize only on A[G]I, allowing to solve wide range of practical tasks currently common to BigData.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMemoria\u0026rsquo;s design follows bottom-up evolutionary path to AGI. Instead of building \u0026ldquo;AGI-in-a-box\u0026rdquo;, Memoria\u0026rsquo;s approach consists in decomposing such complex thing as human-level metacognition into well-manageable building blocks that can be used together or independently, to bring new qualities into existing applications.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="types-of-ai"\u003eTypes of AI\u003c/h2\u003e\n\u003cp\u003eThere are many types of AI, and traditionally what we call Artificial Intelligence is a \u003cstrong\u003eGood Old-Fashioned AI\u003c/strong\u003e or \u003cstrong\u003eGOFAI\u003c/strong\u003e, or catching human ways of reasoning during \u003cem\u003eproblem solving\u003c/em\u003e in special notations, which for those times were logic languages and frames theory. The ultimate AI of this kind is an emotion-less \u0026ldquo;pure mind\u0026rdquo; or Expert System augmenting human reasoning through question-answering.\u003c/p\u003e\n\u003cp\u003eGOFAI has two main conceptual limitations: there was wide disagreement how to model higher mental functions and uncertainty. And if latter problem has finally been solved with probability theory, the question if computers can exactly model consciousness is still one of the most controversial in AI.\u003c/p\u003e\n\u003cp\u003eHistorically there were several main types of AI:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eStrong AI\u003c/strong\u003e. In sort, this is AI with true consciousness, having the same nature with humans consciousness. Or, more certainly, that human consciousness can in principle be modeled on a digital computer. If Strong AI hypothesis is true, we can upload our consciousness into a computer and survive death this way. Term \u0026ldquo;strong\u0026rdquo; may be misleading because it does not describe or define problem solving abilities of such an AI, only its substantial equivalence to a human being (and other animals, of course). Strong AI can be as weak as an newborn infant, and still be qualified as strong, because it has fully-functioning consciousness. So, \u0026ldquo;strong\u0026rdquo; here means power of philosophical argument.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWeak AI\u003c/strong\u003e. If you are quaking like a duck, flying like a duck and swimming like a duck, then you are a duck. Weak AI doesn\u0026rsquo;t try to explain mental states, only behaviors and verbal reports. And this type of AI can have much more problem solving abilities than humans typically have, so the naming it \u0026ldquo;weak\u0026rdquo; reflects its abilities as philosophical argument, not intelligence. \u003cstrong\u003eArtificial General Intelligence\u003c/strong\u003e or \u003cstrong\u003eAGI\u003c/strong\u003e mainly falls into this category if it doesn\u0026rsquo;t explicitly address the problem of consciousness.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eNarrow AI\u003c/strong\u003e or individual intelligent functions modeling, like computer vision or speech recognition, without intention to achieve good performance beyond certain domain. GOFAI is mainly falls into this category.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHuman-level AI\u003c/strong\u003e. An AI having general problem solving capabilities on par with average human, so it can replace humans in various domains. It\u0026rsquo;s not necessary an AGI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHuman-like AI\u003c/strong\u003e. An AI that looks and feels like a human, but not necessary has human-level problem solving abilities. Its main task is understand humans well and make human understand machines well. For example, when we are doing sentiment analysis, we are doing human-like AI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eHard AGI\u003c/strong\u003e. An AGI in a strict sense, or ultimate problem solver. This type of AGI is infeasible to build because it\u0026rsquo;s not finitely computable. But we can build its feasible approximations, which will be computable, but will not be completely generic. Note that \u0026ldquo;Hard\u0026rdquo; prefix here is used only distinguish this type of AI from the next one.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSoft AGI\u003c/strong\u003e. This is the type of AI that performs practically \u0026ldquo;well\u0026rdquo; in wide range of \u0026ldquo;complex domains\u0026rdquo;, but not necessary is a Hard AGI, especially because the latter is not finitely computable. Humans are \u0026ldquo;Human-like Strong Soft AGI\u0026rdquo; if we are trying to identify ourselves in this system of definitions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSuperintelligence\u003c/strong\u003e, or AI having problem solving abilities which are much beyond to what currently possible for non-augmented humans in wider range or domains and environments than it\u0026rsquo;s possible for humans. This type of AI is mostly feared of, because it potentially can enslave or even exterminate humanity just with its problem-solving abilities. Superintelligence is usually assumed to be a Hard AGI in the limit.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAs an example, when AI is reading certain emotional expressions, it\u0026rsquo;s Narrow AI. When it is reading emotions in general, it\u0026rsquo;s Weak AI. When AI is understanding feelings, it\u0026rsquo;s Strong AI. If someone is doing benevolent AI God, she is doing emotional superintelligence.\u003c/p\u003e\n\u003ch2 id="algorithmic-information-theory-basics"\u003eAlgorithmic Information Theory Basics\u003c/h2\u003e\n\u003cp\u003eExpressiveness and Visibility. Algorithms and data structures \u0026ndash; two edge cases of highly compressible strings and low compressible strings. What is in-between, is poorly visible for us. Compressed data structures: data that can answer more questions. Universal sequence prediction through compression. Reducing problems to sequence prediction. Correspondence with human behavior and cognition: artificial curiosity and simplicity principle, intrinsic motivation.\u003c/p\u003e\n\u003cp\u003eIncompressibility theorem and its implications for programming and AI: complex things are complex in any notation. For \u0026ldquo;practical\u0026rdquo; AGI there are approximation strategies throws ensemble (of narrow AIs) methods, that means that amount of intelligence, or the number of problem classes this AGI can solved efficiently, grows linearly with the size of ensemble. What we need is identify the set of problem classes human brain is able to solve efficiently. That is not an easy thing, but broad set of human cognitive material in form of literature, art, engineering and program sources can be used.\u003c/p\u003e\n\u003cp\u003eIn AIT we measure amount of information in bit string $S$ by the length of the shortest program $P$ generating or computing $S$. This length depends on properties of the string $S$ and on the language $L$ the program \u003cem\u003eP\u003c/em\u003e is written in. This shortest length we call \u003cstrong\u003eKolmogorov complexity\u003c/strong\u003e of $S: l = K_L(S)$. The main property of $K$ is that it does not depend on selection of $L$ more than a constant $C$ that is a length of interpreter form any $L1$ to $L2$. This additive constant $C$ depends on $L1$ and $L2$, but doesn\u0026rsquo;t depend on $S: K_{L1}(S) \u0026lt;= K_{L2}(S) + C(L1, L2)$. So, in AIT we drop $L$ and $C$ just keeping them in mind. $K$ has another important properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eAny universal programming language $L$ is a universal description language: any computable object $O$ can be first encoded as binary string $S$ and then described succinctly with some program $P$ generating cor computing $S$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLet $|S|$ is a length of string $S$ in bits. If for some object $O1$, $K(S1) \u0026laquo; |S1|$, then we say that $S1$ is \u003cstrong\u003ehighly compressible\u003c/strong\u003e or \u003cstrong\u003esimple\u003c/strong\u003e. If $K(S1) \\propto |S1|$, then we say that $S1$ is \u003cstrong\u003euncompressible\u003c/strong\u003e or \u003cstrong\u003ecomplex\u003c/strong\u003e. There are may intermediate descriptional complexity classes between highly compressible and uncompressible strings.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA string is completely \u003ca href="http://www.scholarpedia.org/article/Algorithmic_randomness"\u003erandom\u003c/a\u003e in AIT if it is uncompressible. The more compressible string is, the more causal structure can be found in it, the less random it is.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e$K(S)$ is uncomputable. To find the shortest program $P$ generating $S$ we have to enumerate all programs an evaluate then in a diagonal manner. Not all programs will halt eventually, and for any fixed moment in time we can\u0026rsquo;t know if some program $Pi$ is still computing $S$ or have already hanged. In other words, to get exact value of $K(S)$ we need to solve halting problem that is infeasible unless we have supercomputations. Best method to find an approximation of $K(S)$ is \u003ca href="http://www.scholarpedia.org/article/Universal_search"\u003eUniversal Search\u003c/a\u003e that\u0026rsquo;s is a kind of enumeration of running programs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNevertheless, $K(S)$ is \u003cstrong\u003esemicomputable\u003c/strong\u003e or \u003cstrong\u003ecomputable form above\u003c/strong\u003e in an any-time fashion: for any time $T$ we will have set of programs ${P_i}$ already completed. Just find the shortest one $P_i$ generating $S$. With bigger time, the shorter program can be found, if it exists. The only problem that we never know how close our current estimation of complexity of $S$ to its Kolmogorov complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eKolmogorov complexity of a string S in non-reducible. It can\u0026rsquo;t be reduced by means of any finite transformation of S. Complex things will always be complex. That means there is no \u0026ldquo;silver bullet of programming\u0026rdquo; \u0026ndash; so advanced universal programming language that all programs become simple by being written in it.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNevertheless, we can have \u003cem\u003edomain-specific\u003c/em\u003e languages or DSLs that make some limited subset of problems described much shorter than others.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSubjective simplicity has one-sided correspondence with Kolmogorov complexity: complex strings will always appear subjectively complex, simple strings can appear both subjectively simple and complex. The depends on how actual structures in the string fit the set of structures our brain can recognize.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href="https://books.google.com/books?id=JvXiBwAAQBAJ\u0026amp;pg=PA96\u0026amp;lpg=PA96\u0026amp;dq=incompressibility+theorem\u0026amp;source=bl\u0026amp;ots=yPy044Hkmq\u0026amp;sig=XjB49Gc-8FZc0dry4nBYj1T6Myw\u0026amp;hl=ru\u0026amp;sa=X\u0026amp;ved=0ahUKEwiMooPQ7NjYAhWp7oMKHdGpAJgQ6AEIKDAA#v=onepage\u0026amp;q=incompressibility%20theorem\u0026amp;f=false"\u003eIncompressibility theorem\u003c/a\u003e states that simple, or well-compressible, strings are rare. Most strings are uncompressible. This theorem suggests that arbitrarily selected object \u003cem\u003eO\u003c/em\u003e will have high Kolmogorov complexity, and most of such objects will be just random.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConditional complexity $K(S1 | S2)$ measures how much of new information there is in $S1$ relative to $S2$. Or, it\u0026rsquo;s the length shortest program computing $S1$ by given $S2$. \u003ca href="https://en.wikipedia.org/wiki/Normalized_compression_distance"\u003eNormalized information distance\u003c/a\u003e (NID) is a generalization of this principle and can measure similarity between arbitrary strings in the most universal way. An approximation of NID is a Normalized Compression Distance (NCD). Simple form of NCD is \u003ca href="https://en.wikipedia.org/wiki/Levenshtein_distance"\u003eLevenshtein Distance\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUniversal Search is theoretically optimal but infeasible method of estimating complexity of $S$ for everything except toy problems, because the space of programs is incredible large. Nevertheless, there are suboptimal but much more feasible methods:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/Genetic_programming"\u003eGenetic Programming\u003c/a\u003e and \u003ca href="https://en.wikipedia.org/wiki/Meta-optimization"\u003eMetaoptimization\u003c/a\u003e. Here we restrict both both sample program complexity and search space by \u003cstrong\u003eexploiting\u003c/strong\u003e certain properties of programs. When exploitation is not possible, these methods resort to random walk in the space of programs that is slower than Universal Search (because we can visit some programs many times).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eData/Image compression, both lossless and lossy. Pretty complete method of compression where we seriously restrict complexity of programs. For \u003ca href="https://en.wikipedia.org/wiki/LZ77_and_LZ78"\u003eexample\u003c/a\u003e to probabilistic finite automation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSoftware programming. We are looking for a program with certain properties (size, execution time) compactly describing certain complex domain, and our brain is doing it in a very efficient, though still not complete way. Unfortunately, we still don\u0026rsquo;t know exactly how it\u0026rsquo;s doing this search.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMachine Learning. For example, object classification. Instead of doing compression of $S$ in order to decompress it back again, we are looking for some model $M$ discriminating string $S$ from other strings from certain set ${S}$. Though classifier complexity is not necessary depend on complexity of $S$, it depends on complexity of the border between classes strings $S$ belong to. For a multi-layer perceptron this border is approximated with hyper-planes, more complex border requires more hyper-planes to describe it exactly. So we are looking for pretty simple models $M$ still having good classification accuracy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the last case (ML) we are not looking explicitly for the shortest dicriminative or generative models ever. It\u0026rsquo;s enough if the model fits our computational budgets. Nevertheless, program length can be exploited when we are talking about probabilistic prediction. Solomonoff\u0026rsquo;s \u003ca href="http://www.scholarpedia.org/article/Algorithmic_probability"\u003eAlgorithmic Probability\u003c/a\u003e can be used as universal prior for Bayesian sequence prediction like it\u0026rsquo;s done in Hutter\u0026rsquo;s \u003ca href="https://en.wikipedia.org/wiki/AIXI"\u003eAIXI\u003c/a\u003e agent.\u003c/p\u003e\n\u003cp\u003eThe idea behind Algorithmic Probability (ALP) is simple: we assign (exponentially) higher probabilities to shorter programs. Then, algorithmic probability of a string $S$ is a sum over probabilities of all programs $P_i$, computing $S$ as output. This does not corresponds directly to probabilities of physical events, but nevertheless may have some connections to higher-level cognition.\u003c/p\u003e\n\u003cp\u003eConsider a hypothetical physical experiment. Let\u0026rsquo;s we have a true random number generator generating binary sequences with uniform distribution. And in the first experiment this RNG produces the long, say 1024 symbols, series of zeroes. According to normal distribution, probability of such event will be so low, it unbelievable it can happen. Nevertheless, such unusual output of an RNG is a valid output. The reason why we are usually confused with such experimental results is that we implicitly assign higher subjective probabilities to events we can recognize some structure in. This and many other related phenomena are trying being generalized in \u003ca href="https://simplicitytheory.telecom-paristech.fr/"\u003eSimplicity Theory of Mind\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eProminent result of Solomonoff\u0026rsquo;s Alogrithmic Probability that it\u0026rsquo;s a complete and universal method for machine learning and it\u0026rsquo;s at the same time not finitely computable. Moreover, neither feasible machine learning method can be compete in this sense because in order to achieve completeness, we need supercomputations. Incompleteness means that for an arbitrarily given string, we can\u0026rsquo;t find and exploit for prediction all potential regularities it contains. Fortunately, it\u0026rsquo;s not always necessary in practice for a ML method to be complete in a strict sense. Incompleteness of ALP just means that true AGI is not finitely computable, and all feasible AGIs, like human-level AI, will be somewhat incomplete.\u003c/p\u003e\n\u003cp\u003eAnother interesting practical property of ALP is that it\u0026rsquo;s well-approximable \u003ca href="https://en.wikipedia.org/wiki/Ensemble_learning"\u003eensemble method\u003c/a\u003e. If we truncate the sum over all possible models of $S$ to just one shortest model, ALP will still perform well in many cases. This method is called \u003ca href="http://www.scholarpedia.org/article/Minimum_description_length"\u003eMinimum Description Length\u003c/a\u003e principle. From another side, we can restrict the model class we will be looking for simplest models of $S$. For example in \u003ca href="https://arxiv.org/abs/0909.0801"\u003eMC-AIXI\u003c/a\u003e authors propose to approximate ALP with ensemble over variable order Markov models computed via \u003ca href="https://en.wikipedia.org/wiki/Context_tree_weighting"\u003eContext Tree Weighting\u003c/a\u003e algorithm.\u003c/p\u003e\n\u003cp\u003eIn spite of so appealing properties of ALP and derived ensemble methods like Solomonoff\u0026rsquo;s Induction and AIXI, it\u0026rsquo;s not that easy to apply it to, for example, human-level intelligence. There are two main problems.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eBootstrap problem\u003c/strong\u003e or \u003cstrong\u003einitial training sequence problem\u003c/strong\u003e. In order ALP-based methods to work efficiently on practical problems we have to supply initial set of models either directly, or in the form of initial training sequence. It\u0026rsquo;s not that easy to determine such set of models for human brain. According to decent results, human brain is described with several tens of megabits of DNA code \u0026ndash; the Kolmogorov complexity of our brain. The models we are looking for are implicitly encoded in this DNA\u0026rsquo;s part. It seems that even if several megabits of this code is sufficient, this task may have unpredictable complexity.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eIncremental learning problem\u003c/strong\u003e. Even if we already selected basic machine and identified bootstrapping set of models, we need an approximation of Universal Search to infer new models from data. Though this process is well-defined mathematically, it\u0026rsquo;s not that easy to implement it in practice, because program space is too big for exhaustive search. In order to achieve desired performance we have to restrict this space somehow, either by restricting model class (as in MC-AIXI) or by using various heuristics as in Genetic Programming and Metaoptimization, or by a database of declarative and procedural restrictions applied to the search space. The latter case is the most flexible and the most complex one because such database may be very big in size even for simplest problems.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhat is important, these two problems can be developed independently, by different teams focused on their problems. This the way to go for Memoria. It will be focused on \u003cstrong\u003ecompressed data structures\u003c/strong\u003e, where compression is understood not just in space-saving context, by as an enabler of generic intelligence. Additionally, Universal Search approximations are very important for general intelligence.\u003c/p\u003e\n\u003cp\u003eHigh-level ideas behind ALP was informally explained by \u003ca href="https://www.youtube.com/watch?v=nXrTXiJM4Fg"\u003eMarvin Minsky\u003c/a\u003e. In short, Solomonoff\u0026rsquo;s approach to AGI is a top-down approach, when solid theoretical framework of AI is established first, and then we derive custom AIs with specific properties using approximation techniques. As opposite, classical bottom-up approach is playing around with different substances being inspired either from neuroscience or from cognitive science and waiting for intelligence to emerge in experiments. They can work together very well, as custom AIs are developed in a bottom-up experimental way and then merged into more powerful ensemble in top-down way by Solomonoff\u0026rsquo;s Induction.\u003c/p\u003e\n\u003ch2 id="self-referential-machine"\u003eSelf-Referential Machine\u003c/h2\u003e\n\u003cp\u003eIt\u0026rsquo;s just a language to describe certain computational phenomena succinctly, there is nothing special from computational perspective.\u003c/p\u003e\n\u003ch2 id="psychology-basics"\u003ePsychology Basics\u003c/h2\u003e\n\u003cp\u003ePsychological definitions of higher mental functions are not suitable for AI because they intermix subjective and objective planes, that leads to controversies. Better system of definitions is necessary.\u003c/p\u003e\n\u003ch2 id="emotional-intelligence"\u003eEmotional Intelligence\u003c/h2\u003e\n\u003cp\u003eWhat it is and how it works from AIT\u0026rsquo;s perspective. Rational intelligence as a custom case of emotional one, and not as a separate system. Rationality is illusion.\u003c/p\u003e\n\u003ch2 id="hard-problems-of-ai"\u003eHard Problems of AI\u003c/h2\u003e\n\u003ch2 id="process-introspection"\u003eProcess Introspection\u003c/h2\u003e\n\u003cp\u003eProcess introspection \u003ca href="https://medium.com/@victorsmirnov/how-to-compensate-introspection-illusion-62f357e9326c"\u003ecan be improved.\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id="strong-ai-is-a-basic-right"\u003eStrong AI is a Basic Right\u003c/h2\u003e\n\u003cp\u003eStrong AI is about us, not about machines that think and feel.\u003c/p\u003e\n\u003ch2 id="evolutionary-strategy-for-agi"\u003eEvolutionary Strategy for AGI\u003c/h2\u003e\n'}).add({id:23,href:"/docs/mbt/overview/",title:"MBT Overview",description:"",content:""}).add({id:24,href:"/docs/tests/overview/",title:"Tests -- Overview",description:"",content:""}).add({id:25,href:"/docs/datascope/overview/",title:"Datascope Overview",description:"",content:""}).add({id:26,href:"/docs/storage/",title:"Dbfs",description:"",content:""}).add({id:27,href:"/docs/tests/",title:"Tests List",description:"",content:""}).add({id:28,href:"/docs/d-phil/",title:"AI \u0026 ML List",description:"",content:""}).add({id:29,href:"/docs/data-zoo/",title:"Containers List",description:"",content:""}).add({id:30,href:"/docs/mbt/",title:"MBT",description:"",content:""}).add({id:31,href:"/docs/overview/",title:"Overview List",description:"Overview List",content:""}).add({id:32,href:"/docs/datascope/",title:"Datascope List",description:"Datascope List.",content:""}).add({id:33,href:"/docs/",title:"Docs",description:"Docs Memoria.",content:""}).add({id:34,href:"/docs/misc/",title:"Misceleneous Articles",description:"",content:""}),search.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()