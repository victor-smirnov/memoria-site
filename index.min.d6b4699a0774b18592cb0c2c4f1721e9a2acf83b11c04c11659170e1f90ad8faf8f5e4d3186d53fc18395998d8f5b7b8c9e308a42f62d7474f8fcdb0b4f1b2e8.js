var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement),f=suggestions.classList.contains('d-none');let c=0;b.keyCode===38&&!f?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&!f&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/overview/introduction/",title:"Introduction to Memoria",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eData dominates. If you\u0026rsquo;ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u0026ndash; Rob Pike in \u003ca href="http://www.lysator.liu.se/c/pikestyle.html"\u003e“Notes on Programming in C”\u003c/a\u003e, 1989.\u003c/p\u003e\n\u003ch3 id="how-it-started"\u003eHow it started\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eMemoria started back in 2007 out of a need of having a memory-efficient multi-dimensional spatial tree for function approximation, like \u003ca href="/docs/data-zoo/associative-memory-2/"\u003ethis one\u003c/a\u003e. Contrary to traditional approaches for function approximation, like neural networks, spatial trees have much smaller computational complexity (logarithmic on average) for inference and allow computing partial and inverse functions out of the same set of parameters. Advanced data structures to the rescue, \u003ca href="/docs/data-zoo/louds-tree/"\u003eLOUDS tree\u003c/a\u003e has 2 bits per tree node of space complexity + some small overhead. What is also important, is that LOUDS trees can be \u003cem\u003edynamic\u003c/em\u003e, allowing point-like updates, so that tree-based function approximation method can support precise in-place tuning.\u003c/p\u003e\n\u003ch3 id="advanced-data-structures"\u003eAdvanced data structures\u003c/h3\u003e\n\u003cp\u003eLOUDS tree internally is a \u003ca href="/docs/data-zoo/searchable-seq/"\u003e\u003cem\u003esearchable\u003c/em\u003e bit vector\u003c/a\u003e supporting two additional operations \u0026ndash; \u003ccode\u003erank()\u003c/code\u003e and \u003ccode\u003eselect()\u003c/code\u003e by using two additional arrays. And all of this can be implemented as a dynamic array (with logarithmic complexity of updates). The problem is that besides those two search operations and traditional point-like query and update operations, we also need dozens of service operations like \u003cem\u003ebatch updates\u003c/em\u003e. Implementation complexity is already skyrocketing. But besides that we also need efficient concurrent multi-threaded access, external memory, transactions and versioning. In real life, implementational complexity of even apparently simple data structure, like a bitmap, may be 100-1000 times larger that one may expect by reading its description in a textbook. What is the worst thing, you may find that you need may (dozens of) different data structures\u0026hellip;\u003c/p\u003e\n\u003ch3 id="should-i-tried-a-database"\u003eShould I tried a database?\u003c/h3\u003e\n\u003cp\u003eAn obvious idea is to try using a database as a host\u0026hellip; It\u0026rsquo;s not that simple. Databases can be transactional, analytical or hybrid, and most of the time they are heavily optimized for \u003cem\u003eone\u003c/em\u003e type of data: tabular, graph or document. Unix way: do \u003cem\u003eone\u003c/em\u003e thing but do it well. Multi-model databases exist, but they are not that multi-model one may expect. Instead of one, they are doing tree things (tables, graphs and documents), and these are probably not what you what to \u003cem\u003ereuse\u003c/em\u003e to speedup your development. Anyway one should definitely try this route before start even thinking about writing their own database engine. There were way less options back in 2000th when Memoria was started than now.\u003c/p\u003e\n\u003ch3 id="or-maybe-create-a-new-one"\u003eOr, maybe, create a new one?\u003c/h3\u003e\n\u003cp\u003eIf you still want to start building your own database, prepare to suffer. Neither OS, nor programming languages are not for that. C is great, but only until you need a generic collection library. And, trust me, you will need a lot of them. Java is glorious, but prepare for low-level programming over raw memory buffers with memory leaks and undefined behaviour, or GC will be killing you every day. C++ is excellent but you will be controlling UB manually all the way down. Golang is good but its monomorphic generics story has only started recently. The same is true for Rust. High performance IO story is fully ruled by networking people who \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003ekilled fibers\u003c/a\u003e. Memory mapping hates your high-performance NVMe SSD even on reading. There is no way to \u003cem\u003ereliably\u003c/em\u003e commit a transaction. Database engines are just \u003cem\u003etrying\u003c/em\u003e their best in this respect. And this is, more or less, guaranteed only for certain combination of storage device, OS and drivers. And I haven\u0026rsquo;t yet mentioned distributed computing. There is no reliable way to send a packet between computers. There is basically no part of your computer you can trust and rely on. And every failure may be fatal for your \u003cem\u003estate\u003c/em\u003e. You want to sleep well at nights, right?\u003c/p\u003e\n\u003ch3 id="what-memoria-is"\u003eWhat Memoria is\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eMemoria Framework is aiming to make our life in this world easier but you may be benefited too, depending on your needs and how well they fit into the project\u0026rsquo;s model.\u003c/p\u003e\n\u003cp\u003eMemoria has the following components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003e\u003cstrong\u003eHermes\u003c/strong\u003e\u003c/a\u003e - arbitrarily structured object graphs allocated in relocatable contiguous memory segments, suitable for memory mapping and inter-process communication, with focus on data storage purposes. Hermes objects and containers are string-externalizable and may look like \u0026ldquo;Json with types\u0026rdquo;. GRPC-style services and related tooling (IDL) is provided.\u003c/li\u003e\n\u003cli\u003eExtensible and customizable set of \u003ca href="/docs/overview/containers"\u003e\u003cstrong\u003eData Containers\u003c/strong\u003e\u003c/a\u003e, internally based on B+Trees crafted from reusable building blocks by the metaprogramming framework. The spectrum of possible containers is from plain dynamic arrays and sets, via row-wise/column-wise tables, multitude of graphs, to compressed spatial indexes and beyond. Everything that maps well to B+Trees can be a first-class citizen of data containers framework. Containers and Hermes data objects are deeply integrated with each other.\u003c/li\u003e\n\u003cli\u003ePluggable \u003ca href="/docs/overview/storage"\u003e\u003cstrong\u003eStorage Engines\u003c/strong\u003e\u003c/a\u003e based on Copy-on-Write principles. Storage is completely separated from containers via simple but efficient contracts. Out of the box, OLTP-optimized and HTAP-optimized storage, as well as In-Memory storage options, are provided, supporting advanced features like serializable transactions and Git-like branching.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/vm"\u003e\u003cstrong\u003eDSL execution engine\u003c/strong\u003e\u003c/a\u003e. Lightweight embeddable VM with Hermes-backed code model (classes, byte-code, resources) natively supporting Hermes data types. Direct Interpreter and AOT compilation to optimized C++.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/runtime"\u003e\u003cstrong\u003eRuntime Environments\u003c/strong\u003e\u003c/a\u003e. Single thread per CPU core, non-migrating fibers, high-performance IO on top of io-uring and hardware accelerators.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/mbt"\u003e\u003cstrong\u003eDevelopment Automation\u003c/strong\u003e\u003c/a\u003e tools. Clang-based build tools to extract metadata directly from C++ sources and generate boilerplate code.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe purpose of the project is to integrate all aspects and components described above into a single vertical framework, starting from \u003cem\u003ebare silicon\u003c/em\u003e up to networking and human-level interfaces. The framework may eventually grow up into a fully-featured \u003cem\u003emetaprogramming platform\u003c/em\u003e.\u003c/p\u003e\n\u003ch3 id="what-memoria-is-not"\u003eWhat Memoria is not\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eFirst of all, Memoria is not a database engine. It may contain one as a part of the Framework, but its scope will be limited (like etcd for k8s). Large-scale \u003cem\u003edistributed\u003c/em\u003e storage is intentionally outside of the scope of the project.\u003c/p\u003e\n'}).add({id:1,href:"/docs/overview/hermes/",title:"Hermes",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eIn ancient Greek mythology Hermes is a messenger between gods and humans.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id="basic-information"\u003eBasic information\u003c/h2\u003e\n\u003cp\u003eIn the context of Memoria Framework, Hermes is a solution to the \u0026ldquo;last mile\u0026rdquo; data modelling problem. \u003ca href="/docs/overview/containers/"\u003eContainers\u003c/a\u003e can be used to model large amounts of \u003cem\u003ehighly structured\u003c/em\u003e data. Hermes is intended to represent relatively small amount of \u003cem\u003eunstructured or semi-structured data\u003c/em\u003e and can be used together with containers. Notable feature of Hermes as a data format is that all objects and data types have canonical \u003cem\u003etextual representation\u003c/em\u003e, so Hermes data can be consumed and produced by humans. Hence, the name of the data format.\u003c/p\u003e\n\u003cp\u003eHermes defines an arbitrarily structured \u003cem\u003eobject graph\u003c/em\u003e that is allocated in a continuous memory segment (or a series of fixed size segments) working as an \u003cem\u003earena\u003c/em\u003e. Top-level object is a \u003cem\u003edocument\u003c/em\u003e. Document is a container for Hermes \u003cem\u003eobjects\u003c/em\u003e. Hermes objects internally use \u003cem\u003erelative pointers\u003c/em\u003e, so the data is \u003cem\u003erelocatable\u003c/em\u003e. Hermes objects in this form can be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estored in Memoria containers,\u003c/li\u003e\n\u003cli\u003ememory-mapped from files or between processes,\u003c/li\u003e\n\u003cli\u003eembedded into an executable as a form of a \u003cem\u003eresource\u003c/em\u003e,\u003c/li\u003e\n\u003cli\u003esent over a network or shared between host CPU and \u003cem\u003ehardware accelerators\u003c/em\u003e, even if the latter have a separate \u003cem\u003eaddress space\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHermes documents can be of arbitrary memory size, the format internally has 64-bit pointers (even on 32-bit architectures). It\u0026rsquo;s fine to have TB-size documents, but the format is not intended for that. Hermes documents are \u003cem\u003egarbage-collected\u003c/em\u003e with a simple \u003cem\u003ecopying GC\u003c/em\u003e algorithm. So the larger documents are, the more time will be spent in compactifications. Ideally, Hermes documents \u003cem\u003eshould\u003c/em\u003e (but not required to) fit into a single storage block, that is typically 4-8KB and may be up to 1MB in Memoria. In this case, accessing Hermes objects stored in containers will be in a \u003cem\u003ezero-copy\u003c/em\u003e way.\u003c/p\u003e\n\u003cp\u003eThere are three serialization formats for Hermes data.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eZero\u003c/strong\u003e serialization. Hermes data segment use relative addresses and can be externalized as is, as a raw memory block. All Hermes documents support fast \u003cem\u003eintegrity checking\u003c/em\u003e procedure to make sure that reading foreign segments is safe. This is the fastest format but not particularly the densest one. This format is mainly for \u003cem\u003edata storage\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eText\u003c/strong\u003e serialization. Human-readable, safe, and the slowest (but still fast in raw numbers).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBinary\u003c/strong\u003e serialization. The densest option, but faster than the textual one. Safe. Best for networking when human-readability is not a requirement.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHermes documents are \u003cem\u003emutable\u003c/em\u003e and support run-time immutability enforcement (\u0026ldquo;make it immutable\u0026rdquo;). Immutable documents may be safely shared between threads with minor restrictions.\u003c/p\u003e\n\u003ch2 id="hermes-object"\u003eHermes Object\u003c/h2\u003e\n\u003cp\u003eHermes memory is \u003cem\u003etagged\u003c/em\u003e. A tag is a type label and has various length, from 1 to 32 bytes. Most commonly used types, like \u003ccode\u003eInt\u003c/code\u003e have tag length of 1 byte. Rarely used types may have tags up 32 bytes, in that case it\u0026rsquo;s most likely a SHA256 \u003cem\u003ehash code\u003c/em\u003e of the \u003cem\u003etype\u0026rsquo;s declaration\u003c/em\u003e. Hermes assumes that type hash collisions will be an extremely rare event.\u003c/p\u003e\n\u003cp\u003eMemory for tag is allocated address-wise \u003cem\u003ebefore\u003c/em\u003e the object. Objects may have gaps between them in memory caused by alignment requirements. In that case, if a tag fits into this gap, it\u0026rsquo;s allocated there. With high probability, short tags (for commonly used objects) do not take extra memory.\u003c/p\u003e\n\u003cp\u003eFrom the API\u0026rsquo;s perspective, Hermes objects consist from two part. The first part is a private C++ object with verbose API conforming to certain rules. The second part is a public \u003ccode\u003eView\u003c/code\u003e for this object, this is much like \u003ccode\u003estd::string_view\u003c/code\u003e for string-like data. Mutable Hermes object receive a reference to the corresponding Document\u0026rsquo;s arena allocator to allocate new data in. View object encapsulates all these complexities.\u003c/p\u003e\n\u003cp\u003eViews are \u003cem\u003eowning\u003c/em\u003e in Hermes. Object\u0026rsquo;s view holds a reference to the corresponding Document (for memory allocation) with a fast, \u003cem\u003enon-atomic\u003c/em\u003e, reference counter. Hermes Views are nearly zero-cost, but not thread-safe.\u003c/p\u003e\n\u003cp\u003eDocuments as containers have additional levels of reference counting indirection, including atomic counters. Sharing document\u0026rsquo;s \u003cem\u003edata\u003c/em\u003e between threads may be permitted in some cases.\u003c/p\u003e\n\u003ch3 id="56-bit-types"\u003e56-bit types\u003c/h3\u003e\n\u003cp\u003e1-byte tag size has special consequences in Hermes: together with 64-bit integers, we are using \u003cem\u003e56-bit\u003c/em\u003e integers and identifiers to be able to fit the entire value into a 64-bit memory slot of a pointer. That, in many cases, saves memory.\u003c/p\u003e\n\u003ch2 id="datatypes"\u003eDatatypes\u003c/h2\u003e\n\u003cp\u003eHermes has explicit notion of a type, and Hermes types are pretty close in semantics to C++ classes, they may be \u003cem\u003eparametric\u003c/em\u003e in two ways. The first way is common with C++: \u003ccode\u003eMyType\u0026lt;Parameter\u0026gt;\u003c/code\u003e creates new instance of \u003ccode\u003eMyType\u003c/code\u003e parametrized by \u003ccode\u003eParameter\u003c/code\u003e. The second type of parametrization is trickier. Hermes type may have an associated \u003cem\u003estate\u003c/em\u003e that is considered as a shared state for all Hermes objects of this type. Type \u003ccode\u003eDecimal(10,2)\u003c/code\u003e have two \u003cem\u003etype constructor\u003c/em\u003e \u003ccode\u003e(10, 2)\u003c/code\u003e parameters: precision 10 and scale 2. Type constructor in Hermes does not create a new type, so there is no way to \u003cem\u003estatically\u003c/em\u003e specialize some code for \u003ccode\u003eDecimal(10, 2)\u003c/code\u003e. Object instances of type \u003ccode\u003eDecimal\u003c/code\u003e will have a pointer to a memory, storing the corresponding type constructor\u0026rsquo;s data.\u003c/p\u003e\n\u003cp\u003eGeneric types in Hermes are monomorphic.\u003c/p\u003e\n\u003cp\u003eTo distinguish between C++ types and Hermes types that may have type constructors, the letter are called \u003cem\u003eDatatypes\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eDatatypes are first-class objects in Hermes and the rest of Memoria. So, we may have collections of datatypes, we cap process them as data and so on. We can also parse RTTI declaration of some C++ types as \u003cem\u003enormalized\u003c/em\u003e datatypes and compute their corresponding 32-byte \u003cem\u003etype hash\u003c/em\u003e. (Note that this feature is currently may be compiler-specific).\u003c/p\u003e\n\u003ch2 id="document"\u003eDocument\u003c/h2\u003e\n\u003cp\u003eIn Hermes \u0026lsquo;document\u0026rsquo; has two meanings.\u003c/p\u003e\n\u003cp\u003eFirst, \u003ccode\u003eDocument\u003c/code\u003e is a container for Hermes data. Second, \u0026lsquo;document\u0026rsquo; is a specific set of predefined collections organizing objects into a tree-like structure, similar to Json. Below there is a short walk-through Hermes document text-serialization features giving us json-like experience.\u003c/p\u003e\n\u003ch3 id="null-object"\u003eNull object\u003c/h3\u003e\n\u003cp\u003eThis is the simplest document that has no object.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003enull\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="generic-object-array"\u003eGeneric object array\u003c/h3\u003e\n\u003cp\u003eGeneric array of Objects containing intgers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe same generic array of Objects containing numbers of different types (integer, unsigned integer, short and float):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2u, 3s, 4.567f]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="typed-integer-array"\u003eTyped integer array\u003c/h3\u003e\n\u003cp\u003eThe same array but of datatype \u003ccode\u003eArray\u0026lt;Int\u0026gt;\u003c/code\u003e using optimized memory layout:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e@Array\u0026lt;Int\u0026gt; = [1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eImportant note that notation above does not mean that collection \u003ccode\u003e[]\u003c/code\u003e will have the type specified before it. It means that we create a typed collection \u003cem\u003efrom\u003c/em\u003e a generic one. Parser may optimize this process by not creating the latter and supplying the data directly to the former.\u003c/p\u003e\n\u003cp\u003eThe point is that there may be may ways to create a Hermes object at parse time from Hermes data. For example, notation like \u003ccode\u003e\u0026quot;string value\u0026quot;@SomeDataType\u003c/code\u003e is a syntactic shortcut meaning that \u003ccode\u003e\u0026quot;string value\u0026quot;\u003c/code\u003e will be \u003cem\u003econverted\u003c/em\u003e to Hermes object of datatype \u003ccode\u003eSomeDataType\u003c/code\u003e at the \u003cem\u003eparse time\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e\u0026quot;19345068945625345634563564094564563458.609\u0026quot;@Decimal(50,3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTechnically, string representation of Hermes data is not a static format, like Json, but a \u003cem\u003edomain specific language\u003c/em\u003e. The purpose of this DSL is to make crafting complex Hermes data in a text form easier for humans.\u003c/p\u003e\n\u003ch3 id="generic-map-between-strings-and-objects"\u003eGeneric map between strings and objects\u003c/h3\u003e\n\u003cp\u003eGeneric map between strings and objects:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e{\n  \u0026quot;key1\u0026quot;: 1, \n  \u0026quot;key2\u0026quot;: [1, 2, true],\n  \u0026quot;key3\u0026quot;: @Array\u0026lt;Int\u0026gt; = [5,6,7,8]\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="tinyobjectmap"\u003eTinyObjectMap\u003c/h3\u003e\n\u003cp\u003eThere is a special memory-efficient variant of typed map, mapping from small integer in range [0, 51] to an Object. This map is very memory efficient, using only 16 bytes overhead per collection. It\u0026rsquo;s also very fast for reading, because it uses just one \u003ccode\u003ePopCnt()\u003c/code\u003e instruction (usually 1 cycle on modern CPUs) to find the slot in the hash array, given the key. Values up to 56 bits (small strings, short integers, floats) may be embedded into the hash array. Hash array has no empty slots.\u003c/p\u003e\n\u003cp\u003eGiven its runtime versatility, this type of a map is used extensively to represent C-like \u003cem\u003edynamic\u003c/em\u003e structures in Hermes, without creating a corresponding C++ objects. DSLs over Hermes may combine this type of map with \u003cem\u003ecode\u003c/em\u003e, resulting in dynamically typed \u003cem\u003eobjects\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="semantic-graph"\u003eSemantic Graph\u003c/h2\u003e\n\u003cp\u003eSemantic graph (SG) in Hermes adopts \u003ca href="https://www.w3.org/RDF/"\u003eRDF\u003c/a\u003e-\u003cem\u003elike\u003c/em\u003e data representation to Hermes\' object model. SG usually is a \u003cem\u003ebinary relation\u003c/em\u003e over \u003cem\u003efacts\u003c/em\u003e  in some \u003cem\u003edomain\u003c/em\u003e, plus some model of formal semantics reducible to binary relations. Note that Knowledge Graph (KG) is basically the same thing, but has more features to capture and represent real-life knowledge.\u003c/p\u003e\n\u003cp\u003eSG have appealing theoretical and practical properties, but using them \u0026ldquo;at scale\u0026rdquo; (for large data sets) is a major technological challenge. Graphs or any form do not map well to memory hierarchies (a lot of random access), so it\u0026rsquo;s a \u003cem\u003every expensive\u003c/em\u003e data representation and format.\u003c/p\u003e\n\u003cp\u003eBeing multi-model, Memoria will be supporting various forms of graphs and binary relations anyway, so first-class support for SG starting from Hermes seems a consistent decision.\u003c/p\u003e\n\u003ch2 id="hermespath"\u003eHermesPath\u003c/h2\u003e\n\u003cp\u003eHermes has minimalist but expressive query language for its document subset, modelled after \u003ca href="https://jmespath.org"\u003eJMESPath\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="templating-engine"\u003eTemplating engine\u003c/h2\u003e\n\u003cp\u003eTemplating (generating text) is possible with \u003ca href="https://jinja.palletsprojects.com/en/3.1.x/"\u003eJinja\u003c/a\u003e-like templating language, integrated with HermesPath as expression language.\u003c/p\u003e\n\u003ch2 id="schema"\u003eSchema\u003c/h2\u003e\n\u003cp\u003eHermes has schema processor to enforce declarative and imperative constraints on Hermes documents, semantic graphs and other types of structures. Schema processor is a major component of Hermes\' stack, that may work in an interactive mode, like a \u003cem\u003elanguage server\u003c/em\u003e for Hermes data structures.\u003c/p\u003e\n\u003ch2 id="hrpc"\u003eHRPC\u003c/h2\u003e\n\u003cp\u003eHermes has its own gRPC-like protocol supporting streaming but instead of protocol buffers, HRPC (Hermes-RPC) uses serialized Hermes documents as messages. HRPC:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCan work over any messaging transport. Default (and, currently, the only) implementation is using TCP. Can work on top of QUIC and HTTP/2/3, SCTP. Can work over many \u003cem\u003emessage queues\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eIs a session-based protocol. Session is initialized by client. For TCP transport, there is one HRPC Session per TCP connection.\u003c/li\u003e\n\u003cli\u003eBoth Client and Server may publish \u003cem\u003eservice endpoints\u003c/em\u003e and call them bidirectionally via Session object.\u003c/li\u003e\n\u003cli\u003eResources are identified with 256-bit UIDs.\u003c/li\u003e\n\u003cli\u003eProtocol implementation is versatile and, together with ability to share immutable Hermes documents between threads in a zero-copy way, can be used for \u003cem\u003esafe structured inter-thread messaging\u003c/em\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eConceptually, HRPC is pretty close to the actor model, but has RPC-like API with streaming.\u003c/p\u003e\n\u003cp\u003eSee \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/runtimes/include/memoria/hrpc/hrpc.hpp"\u003eheaders\u003c/a\u003e for basic API definitions. See \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/tests/hrpc"\u003eHRPC tests\u003c/a\u003e for the feature preview.\u003c/p\u003e\n\u003cp\u003eThe main practical difference between HRPC and gRPC is that the former does not require a lot of code-generation for application-level data structures. Basic types are supported by Hermes itself and \u003ca href="#tinyobjectmap"\u003estructured objects\u003c/a\u003e can be wrapped into flyweight C++ \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/runtimes/include/memoria/hrpc/schema.hpp"\u003ehandlers\u003c/a\u003e that will be optimized away at compile-time. HRPC does not require separate in-memory representation of messages.\u003c/p\u003e\n\u003ch2 id="interoperability-with-other-languages"\u003eInteroperability with other languages\u003c/h2\u003e\n\u003cp\u003eHermes is a \u003cem\u003eC++-centric data model\u003c/em\u003e, it relies heavily on RAII for memory management. Replicating it fully in other runtime environments may be difficult if even possible. To implement it fully, target runtime environment must support either RAII or at least ARC. D, Rust, Swift and CPython are in the green zone. For other environments like JavaScript, Java and Julia some functionality may be limited.\u003c/p\u003e\n\u003cp\u003eAnother important dependency is that Hermes\' datatypes may be pretty complex, like arbitrary precision numbers or safe integers with deterministic overflow semantics. Also, DSLs (HermesPath) may rely on extensive libraries of functions. Re-implementing it all in target language will lead to a lot of complex code duplication. So, bindings to Hermes will be relying on FFI to C++. The caveat is that binary code for full set of Hermes may be pretty large in size (10MB+) so running it in a browser (WASM) may be \u003cem\u003eimpractical\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="sources"\u003eSources\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eMain \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/include/memoria/core/hermes"\u003eheaders\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eMain \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/lib/hermes"\u003esources\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eHRPC \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/runtimes/include/memoria/hrpc/hrpc.hpp"\u003eheaders\u003c/a\u003e. String representation parser, HermesPath, Template engine.\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:2,href:"/docs/overview/containers/",title:"Containers",description:"",content:'\u003ch2 id="basic-information"\u003eBasic Information\u003c/h2\u003e\n\u003cp\u003eContainers in Memoria are basic units of data modelling. Idiomatically, they are very much like STL containers except that, counterintuitively, container objects do not \u003cem\u003eown\u003c/em\u003e their data, they use dedicated storage API for that. So data life-cycle is tied to the \u003cem\u003estorage\u003c/em\u003e object, not the \u003cem\u003econtainer\u003c/em\u003e object. In some cases (implementations) container objects may own their storage objects, but, typically, they don\u0026rsquo;t.\u003c/p\u003e\n\u003cp\u003eNote that there may be some terminological clash between Memoria containers discussed here and \u003ca href="/docs/overview/hermes"\u003eHermes\u003c/a\u003e containers. The latter are classical STL-like containers owning their data.\u003c/p\u003e\n\u003cp\u003eMemoria containers are block-organized. Blocks may be of arbitrary size, up to 2GB, there are no inherent limitations, but their size is typically multiple of the storage/memory block size. Like, 4K, 8K, \u0026hellip; The upper limit is storage-dependent. Disk-based storage engines do not allow blocks over 1MB in size for practical reasons.\u003c/p\u003e\n\u003cp\u003e\u003ca href="/docs/data-zoo/packed-allocator/"\u003ePacked Allocator\u003c/a\u003e is a mechanism that is used in Memoria to place data (allocate objects) in memory blocks.\u003c/p\u003e\n\u003cp\u003eBlocks are organized into linked data structures using \u003cem\u003eblock identifiers\u003c/em\u003e. Conceptually, an identifier may be of arbitrary \u003cem\u003efixed size\u003c/em\u003e type (integer, UUID, \u0026hellip;), but Memoria provides dedicated type set for that.\u003c/p\u003e\n\u003cp\u003eThe most common linked data structure used for containers is a \u003cem\u003evariant\u003c/em\u003e of B+Tree. This variant is mainly different from a \u003ca href="https://en.wikipedia.org/wiki/B%2B_tree"\u003estandard one\u003c/a\u003e is that there are no \u003cem\u003esibling links\u003c/em\u003e. There are also no \u003cem\u003eparent links\u003c/em\u003e in the tree, this is necessary for \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003epersistence\u003c/a\u003e. So, in some cases B+Trees in Memoria will be less efficient than standard ones.\u003c/p\u003e\n\u003cp\u003eLack of \u003cem\u003esibling links\u003c/em\u003e is not a big issue, because tree-walking overhead for B+Trees with large (4K+) blocks is pretty moderate.\u003c/p\u003e\n\u003cp\u003eLack of \u003cem\u003eparent links\u003c/em\u003e is more impacting, because iterators now need to keep \u003cem\u003efull path\u003c/em\u003e form root to the current node. Iterator is a stack-like data structure, not just a current block ID. A lot fo tree-updating code becomes much more complicated comparing to the variant with parent links, but this is the price we pay for having persistence, concurrency and parallelism.\u003c/p\u003e\n\u003ch2 id="concurrency-and-parallelism"\u003eConcurrency and Parallelism\u003c/h2\u003e\n\u003cp\u003eContainers in Memoria are \u003cstrong\u003enot\u003c/strong\u003e thread safe, and this is foundational design decision to make data structures simpler. All thread-safety, if any, are provided at the level of storage engines. And the main concurrency and parallelism mechanism Memoria relies on is \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003epersistent/functional\u003c/a\u003e data structures. This feature also comes with its design costs, limitations and overheads. But it also gives Memoria all of its batteries and superpowers.\u003c/p\u003e\n\u003cp\u003eSo, B+Tree-based containers in Memoria can be of two implementation types:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCopy-on-Write-based (CoW)\u003c/strong\u003e containers. Persistence is supported at the level of containers. This is the fastest option but at the expense of more complicated container design. All container types need to align with CoW semantics, that is well-encapsulated by the Framework.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEpithemeral (non-CoW)\u003c/strong\u003e containers. This type of containers do not explicitly support CoW semantics themselves, so the \u003cem\u003emay\u003c/em\u003e have parent and sibling links if necessary. But for the sake of code unification and reuse, they \u003cem\u003edon\u0026rsquo;t\u003c/em\u003e. CoW semantics may still be supported at the level of \u003cem\u003estorage engines\u003c/em\u003e. For example, there is a variant of storage engine on top of the \u003ca href="http://www.lmdb.tech/doc/"\u003eLMDB\u003c/a\u003e database that has strongly serialized CoW-based transactions. When working on top of such storage engines, containers do not need to provide their own CoW semantics.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="storage-agnostic"\u003eStorage-agnostic\u003c/h2\u003e\n\u003cp\u003eFrom a container\u0026rsquo;s perspective, block storage is completely decoupled and can be fully software-defined.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="io.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eFor more details on that see the \u003ca href="/docs/overview/storage/"\u003eStorage engines\u003c/a\u003e section.\u003c/p\u003e\n\u003ch2 id="definitive-example"\u003eDefinitive Example\u003c/h2\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// We will be using in-memory store\n#include \u0026lt;memoria/api/store/memory_store_api.hpp\u0026gt;\n// And a simple Set\u0026lt;\u0026gt; container\n#include \u0026lt;memoria/api/set/set_api.hpp\u0026gt;\n// Static initialization stuff\n#include \u0026lt;memoria/memoria.hpp\u0026gt;\n\nusing namespace memoria;\n\nint main(void) {\n    // Create a store first. All data is in a store object.\n    // We will be using an in-memory store\n    auto store = create_memory_store();\n\n    // In-memory store is a confluently-persistent CoW-enabled store,\n    // so it suppoerts Git-like branching for containers.\n    // We are opening new snapshot from the master branch.\n    auto snapshot = store-\u0026gt;master()-\u0026gt;branch();\n\n    // Now let\'s create a container, it will be a set of short strings.\n    // First, we need to define a datatype for container:\n    using DataType = Set\u0026lt;Varchar\u0026gt;;\n    // See Hermes docs for more information about datatypes.\n\n    // Now lets create a new set container in our snapshot\n    auto set_ctr = create\u0026lt;DataType\u0026gt;(snapshot, DataType{});\n\n    // .. and insert a few strings into it\n    for (size_t c = 0; c \u0026lt; 10; c++) {\n        set_ctr-\u0026gt;upsert(std::string(\u0026quot;Entry for \u0026quot;) + std::to_string(c));\n    }\n\n    // Now we are ready to iterate over inserted entries.\n    set_ctr-\u0026gt;for_each([](auto entry){\n        println(\u0026quot;{}\u0026quot;, entry);\n    });\n\n    // After we are done with inserting data, we can commit the snapshot\n    // so it will be avaliable for other threads to branch from.\n    snapshot-\u0026gt;commit();\n\n    // And we can store the data into a file\n    store-\u0026gt;store(\u0026quot;set-data.mma\u0026quot;);\n\n    // Here all the data will be destroyed in memory,\n    // but will remain in the file\n}\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis example mentions using \u003ca href="/docs/overview/hermes/#datatypes"\u003eHermes datatypes\u003c/a\u003e. Container type is defined by its \u003cem\u003eDatatype\u003c/em\u003e, that is, basically a combination of C++ class and some \u003cem\u003estate\u003c/em\u003e, that will be shared between all objects of this datatype. Note that all datatypes in Memoria are currently stateless, so they look exactly like C++ classes.\u003c/p\u003e\n\u003cp\u003eWorking code this this example can be found \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/examples/simple_set.cpp"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id="compositionality-and-instantiation-of-containers"\u003eCompositionality and Instantiation of Containers\u003c/h2\u003e\n\u003cp\u003eSTL containers are pretty lightweight and are instantiated at the site of usage. They support composition, so we can easily define composite multimap container like \u003ccode\u003estd::map\u0026lt;int64_t, std::vector\u0026lt;std::string\u0026gt;\u0026gt;\u003c/code\u003e. Memoria containers use b+trees aiming to support external memory and have no such luxury of composition. Moreover, arbitrary C++ objects like \u003ccode\u003estd::string\u003c/code\u003e \u003cem\u003ecan\u0026rsquo;t be\u003c/em\u003e used as datatypes, because their memory management is incompatible with internal b+tree machinery. So, instead of STL\u0026rsquo;s compositionality, Memoria provides complex, datatype-optimized containers. Like, STL implementation of multimap becomes \u003ccode\u003eMultimap\u0026lt;Int64, Varchar\u0026gt;\u003c/code\u003e. Framework provides complex containers for many practical use cases.\u003c/p\u003e\n\u003cp\u003eMemoria containers are instantiated in a library and usually comes in a pre-compiled form. The problem is that pre-instantiation of container for all combinations of datatypes is infeasible. Memoria libraries provide the most idiomatic and commonly used ones, and applications are free to instantiate their own variants, it\u0026rsquo;s fully supported.\u003c/p\u003e\n\u003cp\u003eThe following line may look like an instantiation of a template at the call site:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eauto set_ctr = create\u0026lt;DataType\u0026gt;(snapshot, DataType{});\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBut there is no actual instantiation here. Instead, datatype\u0026rsquo;s metrics (typehash code) is used to find actual instantiation in the instantiations registry.\u003c/p\u003e\n\u003cp\u003eIn other for this mechanism to be really lightweight at the call-cite and work properly, containers need to have abstract \u003cem\u003epublic\u003c/em\u003e virtual interfaces and \u003cem\u003eprivate\u003c/em\u003e implementations that are hidden from the application code.\u003c/p\u003e\n\u003cp\u003eContainers have pretty complex lifecicle and metadata systems, as well as integration with Hermes and datatypes, so developing a new container may be a challenge. Memoria uses in-house \u003ca href="/docs/overview/mbt/"\u003eBuild Tool\u003c/a\u003e to automate these processes.\u003c/p\u003e\n\u003ch2 id="metaprogramming-framework"\u003eMetaprogramming Framework\u003c/h2\u003e\n\u003cp\u003eThis is one of the most complex part of the Framework, especially because C++ isn\u0026rsquo;t that good with \u003cem\u003emetaprogramming in large\u003c/em\u003e. There were no much better alternatives back in the days when Memoria started, there are not that many of them now. This issue is going to be addressed low-level languages that support homoiconic compile-time metaprogramming, like Zig and (as it\u0026rsquo;s being promised) Mojo. Memoria\u0026rsquo;s \u003ca href="/docs/overview/vm"\u003eDSL subsystem\u003c/a\u003e is also addressing this issue too. Nevertheless, for container-level metaprogramming we currently only have what \u003cem\u003elatest\u003c/em\u003e C++ standard is offering.\u003c/p\u003e\n\u003cp\u003eMemoria Containers are build using Partial Class programming pattern. Partial Class definition may be split between many files. Containers classes are built from \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/containers/set/container"\u003eparts\u003c/a\u003e using the information provided from three places:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eContainer \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/prototypes"\u003eprototype\u003c/a\u003e. Prototype is a complex, Memoria-specific, form of a \u003cem\u003ebase class\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eContainer Datatype, like \u003ccode\u003eSet\u0026lt;Varchar\u0026gt;\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers-api/include/memoria/profiles"\u003eProfile\u003c/a\u003e. Profiles are an elaborate system of type traits (configurations) that define various \u003cem\u003ecommon\u003c/em\u003e parameters, like CoW/non-Cow, type of block identifier, type of snapshot identifier and so on. Dozens of them.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers/include/memoria/prototypes/bt/bt_factory.hpp"\u003eType Factory\u003c/a\u003e is a metaprogramming engine building container classes out of all this stuff above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="source-code-entry-points"\u003eSource code entry points\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers-api/include/memoria/api"\u003ePublic Containers API\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/tree/master/containers/include/memoria/containers"\u003ePrivate Containers Implementations\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/codegen/include/codegen_memoria.hpp"\u003eDefault Container Instantiations\u003c/a\u003e (Build Tool metadata)\u003c/li\u003e\n\u003c/ol\u003e\n'}).add({id:3,href:"/docs/overview/storage/",title:"Storage Engines",description:"",content:'\u003cp\u003eMemoria has pluggable storage engines. Block storage API is isolated behind the \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers-api/include/memoria/core/container/store.hpp#L132"\u003eIStore\u003c/a\u003e interface. Block life-cycle in the code is \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/containers-api/include/memoria/profiles/common/block_cow.hpp#L30"\u003emanaged\u003c/a\u003e with RAII. As far as some block is used (directly or \u003cem\u003eindirectly\u003c/em\u003e referenced) in the code, a storage engines knows about that and my, for example, keep it in a cache.\u003c/p\u003e\n\u003cp\u003eThere are four \u0026lsquo;core\u0026rsquo; storage engines in Memoria, covering basic usage: in-memory, disk-based OLTP-optimized, analytics-optimized and \u0026lsquo;static immutable files\u0026rsquo;. The may be many other, for secondary purposes, like, embedding into the executable image, and so on.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE THAT (1) all storage engines are currently in the experimental state, features declared here may be missing or incomplete, and (2) no any on-disk data format stability is \u003cem\u003ecurrently\u003c/em\u003e implied. Physical data layout may and will be changing from version to version.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id="persistent-data-structures"\u003ePersistent Data Structures\u003c/h2\u003e\n\u003cp\u003eA data structure is \u003cstrong\u003epersistent\u003c/strong\u003e if update operations (a single one or a batch of) produce new version of data structure. Old version may be kept around or removed as soon as they are not in use. Read access to old versions does not interfere with write access creating new versions. So very few if any synchronization between threads is required. Persistent data structures are mostly wait-free, that makes them a good fit for concurrent environments (both distributed and multi-core single-machine).\u003c/p\u003e\n\u003cp\u003eVirtually every dynamic data structure can be made persistent, but not every one can be made persistent \u003cem\u003enatively\u003c/em\u003e the way that creating a new version does not result in copying of entire data structure. For example, there are efficient copy-on-write (CoW) based algorithms for persistent trees without parent links (see below). But not for trees with parent or sibling links, linked lists or graphs. Fortunately, there is a way to \u0026ldquo;stack\u0026rdquo; non-persistent data structures on top of a persistent tree, and as a result, the former gains persistent properties of the latter by the expense of an \u003cem\u003eO(log N)\u003c/em\u003e extra memory accesses. Memoria follows this way.\u003c/p\u003e\n\u003ch3 id="persistent-trees"\u003ePersistent Trees\u003c/h3\u003e\n\u003cp\u003ePersistent balanced tree is very much like ordinary (non-persistent) tree, except instead of updating tree in-place, we create a new tree that shares the most with \u0026ldquo;parent\u0026rdquo; tree. For example, updating a leaf results in copying the entire path form the root to the leaf into a new tree:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere, balanced tree for Version 1 consists from yellow path and from the rest of Version\u0026rsquo;s 0 tree \u003cem\u003eexcluding yellow path\u003c/em\u003e. Version\u0026rsquo;s 2 tree consists from blue path and the rest of Version\u0026rsquo;s 1 tree \u003cem\u003eexcluding blue path\u003c/em\u003e. For insertions and deletions of leafs the idea is the same: we are copying modified path to a new tree and referencing the the rest on the old tree.\u003c/p\u003e\n\u003ch3 id="atomic-commitment"\u003eAtomic Commitment\u003c/h3\u003e\n\u003cp\u003eUpdates never touch old versions (CoW). Many update operations can be combined into a single version, effectively forming an atomic \u003cem\u003esnapshot\u003c/em\u003e.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-memory-p.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIf writers buffer a series of updates in a thread-local memory and, on update completion, publishes those updates to shared memory atomically, then we have \u003cstrong\u003esnapshot atomic commitment\u003c/strong\u003e. Other readers will see either finished and consistent new version of the data or nothing. So, if a version fits into local memory of a writer thread and we never delete versions, we have \u003ca href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#G-Set_(Grow-only_Set)"\u003eGrow-Only Set CRDT\u003c/a\u003e, that works well without explicit synchronization.\u003c/p\u003e\n\u003ch2 id="transactional-operations"\u003eTransactional Operations\u003c/h2\u003e\n\u003cp\u003eNote that versions in fully-persistent data structures are \u003cem\u003enot\u003c/em\u003e transactions, despite providing perfect isolation and atomicity. Each update (or a group of updates) create a new \u003cem\u003eversion\u003c/em\u003e of data, and to have transactions we must be able either:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSelect one version out of many at the transaction start time, or\u003c/li\u003e\n\u003cli\u003eMerge multiple versions into a single one at the commit time.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBut \u003cem\u003ewithin a single line of history\u003c/em\u003e, versions are essentially transactions.\u003c/p\u003e\n\u003ch3 id="reference-counting-and-garbage-collection"\u003eReference Counting and Garbage Collection\u003c/h3\u003e\n\u003cp\u003eDeletion of versions is special. To delete a version means to delete every persistent tree\u0026rsquo;s node that is not referenced in other versions, down to the leaves:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-tree-delete.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we are deleting Version\u0026rsquo;s 0 root node, as well as green and blue nodes, because they are overshadowed by corresponding paths in other versions. To determine which node is still referenced in other versions, we have to remember \u003cem\u003ereference counter\u003c/em\u003e for each node. On the deletion of a version, we recursively decrement nodes\' reference counters and delete them physically, once counters reach the zero.\u003c/p\u003e\n\u003cp\u003eIn case of deletions, persistent tree\u0026rsquo;s nodes are still a \u003ca href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#2P-Set_(Two-Phase_Set)"\u003e2P-Set CRDT\u003c/a\u003e, so insertions and deletions do not require synchronization. Unfortunately, reference counting requires linearizable memory model for counters. Memory reclamation in persistent trees does not scales well because of linerizability requirements for reference counting. But in practice it may or may not necessary be a problem in a concurrent environment, depending on the workload the memory reclamation is put on the shared memory subsystem.\u003c/p\u003e\n\u003cp\u003eNote that \u003cem\u003emain-memory\u003c/em\u003e persistent and functional data structures (found in many functional languages) usually rely on the runtime-provided garbage collection to reclaim unused versions. Using \u003cem\u003ereference counting\u003c/em\u003e to track unused blocks in Memoria may be seen as a form of deterministic garbage collection.\u003c/p\u003e\n\u003ch3 id="stacking-on-top-of-persistent-tree"\u003eStacking on Top of Persistent Tree\u003c/h3\u003e\n\u003cp\u003eStacking non-persistent dynamic data structures (No-CoW) on top of persistent tree is straightforward. Memoria transforms high-level data containers to a key-value mapping in a form of BlockID-\u0026gt;Data. This key-value mapping is then served through persistent tree, where each version is a \u003cstrong\u003esnapshot\u003c/strong\u003e or immutable point-in-time view to the set of low-level containers.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that Momoria currently does not have in-house storage engines providing CoW semantics at the storage level.\u003c/p\u003e\n\u003ch3 id="snapshot-history-graph"\u003eSnapshot History Graph\u003c/h3\u003e\n\u003cp\u003eVersions in persistent tree need not to be ordered linearly. We can \u003cem\u003ebranch\u003c/em\u003e new version from any other \u003cem\u003ecommitted\u003c/em\u003e version, effectively forming a tree- or graph-like structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="cow-history-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere, in Memoria, any path from root snapshot to a leaf (Head) is called a \u003cstrong\u003ebranch\u003c/strong\u003e. A data structure is \u003cstrong\u003efully persistent\u003c/strong\u003e (or, simply, persistent) if we can branch form any version. If we can merge versions in some way, the data structure is \u003cstrong\u003econfluently persistent\u003c/strong\u003e. Memoria doesn\u0026rsquo;t use confluently persistent trees under the hood. Instead, data blocks can be copied or imported from one snapshot to another, effectively providing the same function to applications.\u003c/p\u003e\n\u003cp\u003eIf a writer may branch only from the single head, or, there is only one linear branch in the history graph, such data structure is called \u003cstrong\u003epartially persistent\u003c/strong\u003e. This apparent limitation may be useful in certain scenarios (see below).\u003c/p\u003e\n\u003cp\u003eNote that at the level of containers merging operation is not defined in a general case. It might be obvious how to merge, say, relational tables because they are \u003cem\u003eunordered sets\u003c/em\u003e of rows in the context of OLTP transactions. And this type of merge can be fully automated. But it\u0026rsquo;s not clear how to merge ordered data structures like arbitrary text documents or vectors.\u003c/p\u003e\n\u003cp\u003eBecause of that, Memoria does not provide complete \u003cem\u003emerge\u003c/em\u003e operation at the level of snapshots. But it provides special facilities to define and check for write-write ad read-write conflicts (conflict materialization). Living the final decision and merge responsibility to applications.\u003c/p\u003e\n\u003cp\u003eFully automated confluent persistence requires using CRDT-like schemes on top of containers.\u003c/p\u003e\n\u003ch3 id="single-writer-mutiple-readers"\u003eSingle Writer Mutiple Readers\u003c/h3\u003e\n\u003cp\u003eSo-called \u003cem\u003eSingle Writer Multiple Readers\u003c/em\u003e, or SWMR for short, transactions can be build on the top of Memoria\u0026rsquo;s snapshots pretty easy. In the SWMR scheme there is only one writer at a time but can be multiple readers accessing already committed data. To support SWMR transactions we need a lock around snapshot history branch\u0026rsquo;s \u003cem\u003ehead\u003c/em\u003e to linearize all concurrent access to it:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="transactions-swmr.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eDespite being non-scalable in theory, because of the lock, SWMR scheme for transactions may show pretty high practical performance in certain cases:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWhen write transactions are short: point-like queries + point-like updates like for moving money from one account to another.\u003c/li\u003e\n\u003cli\u003eWhen writes does not depend on reads like in streaming: firehose is a writer ingesting events into the store, readers perform long-running analytical queries on \u0026ldquo;most recent\u0026rdquo; snapshots with point-in-time semantics.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThere are two main reasons for SWMR high performance:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHistory lock is taken only for the short period of time, just to protect snapshot history form concurrent modifications.\u003c/li\u003e\n\u003cli\u003eOtherwise, no additional locks are involved, except, possible, implicit locks in IO and caching subsystems of computers.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFor long-running readers the cost of these locks is amortized, so they can access dynamic shared data with efficiency of local immutable files.\u003c/p\u003e\n\u003ch3 id="multiple-writer-multiple-readers"\u003eMultiple Writer Multiple Readers\u003c/h3\u003e\n\u003cp\u003eIf read+write transactions may be long, but not interfere much with each other, we can execute them in separate branches. And, \u003cem\u003eif\u003c/em\u003e after completion, there are no read/write and write/write conflicts, just merge them into a single snapshot (new head). Conflicting transactions may be just rolled back automatically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="transactions-mwmr.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThis is actually how MVCC-based relational DBMS work under the hood. What we need for MWMR transactions is to somehow describe conflicting set and define corresponding merge operations for conflicting containers.\u003c/p\u003e\n\u003cp\u003eSWMR transactions are lightweight, performant and can be implemented on top of a linear snapshot history graph without much effort. But they must be short for keeping transaction latencies low. MWMR scheme can run multiple write transactions concurrently, but imply that \u003cem\u003esnapshot merging\u003c/em\u003e is fast and memory-efficient. Moreover, conflict materialization also consumes space and time, and neither of these are necessary for SWMR transactions.\u003c/p\u003e\n\u003ch3 id="streaming--batching"\u003eStreaming + Batching\u003c/h3\u003e\n\u003cp\u003eAs it has been said, SWMR scheme is streaming-friendly, allowing continuous stream of incoming updates and point-in-time view semantics for readers at the same time. The latter is implemented with minimum of local and distributed locks and allows read-only data access that is almost as efficient as to immutable local files.\u003c/p\u003e\n\u003cp\u003eOnce snapshot is committed, it will never be changed again. So, we can run iterative algorithms on the data without expecting that this data may change between iterations. At the same time, updates can be a accumulated in upcoming snapshots. And once readers are done with current ones, they can switch to the most recent snapshot, picking up the latest changes. So, updates can be ingested incrementally and processed as soon as they arrive and ready.\u003c/p\u003e\n\u003ch2 id="core-storage-engines"\u003eCore Storage Engines\u003c/h2\u003e\n\u003ch3 id="in-memory-store"\u003eIn-Memory Store\u003c/h3\u003e\n\u003cp\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/stores-api/include/memoria/api/store/memory_store_api.hpp"\u003eIMemoryStore\u003c/a\u003e is the main, the most feature-rich \u003cem\u003ecore\u003c/em\u003e storage engine in Memoria, and it\u0026rsquo;s the fastest option. Other storage engines \u003cem\u003emay be\u003c/em\u003e limited in functionality in one or another way. It\u0026rsquo;s a \u003ca href="https://en.wikipedia.org/wiki/Persistent_data_structure"\u003econfluently-persistent\u003c/a\u003e store with CoW implemented at the level of containers.\u003c/p\u003e\n\u003cp\u003eStore is transactional (within a single branch) and multi-threaded, MWMR, and is best suitable for compute-intensive tasks when data is well-fit into memory of a single machine and \u003cem\u003edurability\u003c/em\u003e of single snapshots is not required.\u003c/p\u003e\n\u003ch3 id="swmrstore"\u003eSWMRStore\u003c/h3\u003e\n\u003cp\u003e\u003ca href="https://github.com/victor-smirnov/memoria/blob/master/stores-api/include/memoria/api/store/swmr_store_api.hpp"\u003eSWMRStore\u003c/a\u003e is an SSD-optimized disk-based storage engine with durable commits. It supports basically the same set of essential functions as the in-memory store, but there may be only one writing transaction active at a time. History (series of snapshots) and branches are supported. Writers do not interfere with Readers (no locks). The following list summarizes this store\u0026rsquo;s features:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUses reference counting for memory management. Every snapshot can be deleted from history independently from others. But see remark (*) below.\u003c/li\u003e\n\u003cli\u003eOptimized mainly for analytics (read-intensive ops, streaming, batching) but also for good transactional performance.\u003c/li\u003e\n\u003cli\u003eSupports relaxed durability when only certain snapshots are marked as durable speeding up commits on consumer-grade SSDs. In case of a crash, the store will recovered up to the last durable snapshot.\u003c/li\u003e\n\u003cli\u003eSupport multi-phase commit protocols. SWMRStore instances can participate in \u003cem\u003edistributed transactions\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eAlthough different writers are serialized, writer itself \u003cem\u003emay be\u003c/em\u003e parallel, by opening multiple (sub-)snapshots and buffering writes in memory. This mode of operation is useful for heavy-weight data transformation operations and will be supported in the future.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003eSingle-Writer\u003c/code\u003e is actually not a \u003cem\u003efundamental\u003c/em\u003e limitation. If writers are serialized, there are pretty good algorithms for managing external (block) memory. Because of that, SWMRStore does not need an underling filesystem to allocate blocks, it may work on top of raw block devices (primary mode for a high-performance setup). MWMR mode can be implemented on top of any \u0026lsquo;read your writes\u0026rsquo; Key/Value store, the problem is that in order to support crash recovery we will have to either scan \u003cem\u003ethe entire repository\u003c/em\u003e to identify orphaned blocks, or accumulate them in a separate store, merge it in at commit time and provide recovery metadata. It may have sense in some cases, but currently MWMR is not a part of the core set of storage engines for block devices.\u003c/p\u003e\n\u003cp\u003e(*) Note that reference counters are not persistent and are not stored on each commit. Because of that, SWMRStore does not provide zero-time recovery. In case of crash or improper shutdown, we have to scan the store partially to rebuild counters. Fortunately, the amount of information that needs to be scanned this way is rather small (much less that 1%) and the store is readable at this time. Counters also take main memory, about 4 bytes per block.\u003c/p\u003e\n\u003ch3 id="oltpstore"\u003eOLTPStore\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003e(Note that this storage engine type is WIP and is not yet available for experiments)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOLTPStore is the main OLTP-optimied storage engine. It\u0026rsquo;s using the same memory management algorithm as \u003ca href="http://www.lmdb.tech/doc/"\u003eLMDB\u003c/a\u003e but implemented with means of Memoria containers. LMDB uses persistent CoW b-trees for its databases but it \u003cem\u003edoes not\u003c/em\u003e use counters for tracking references to blocks. In LMDB, when we clone a block, the cloned block\u0026rsquo;s ID is put into so-called \u0026lsquo;free list\u0026rsquo; under transaction ID this block was created in. This block will become eligible for reuse when when all readers which are alder than this block\u0026rsquo;s TxnID terminate. So, the snapshot history is cleared only from its \u003cem\u003etail\u003c/em\u003e. The main limitation is that long-running \u003cem\u003ereader\u003c/em\u003e will be blocking memory reclamation. Neither LMDB nor OLTPStore are suitable for analytical workloads (long-running queries). But it, \u003cem\u003etheoretically\u003c/em\u003e (after all optimizations) may show very hight sustained transaction rates.\u003c/p\u003e\n\u003cp\u003eThe following list summarizes OLTPStore\u0026rsquo;s features and limitations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eUnlike LMDB, OLTPStore \u003cem\u003edoes not\u003c/em\u003e use memory mapping. Instead, high-performance IO interfaces like Linux AIO or liburing are used instead.\u003c/li\u003e\n\u003cli\u003eUnlike SWMRStore, neither branches, nor snapshot history are supported.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id="overlaystore"\u003eOverlayStore\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003e(Note that this storage engine type is WIP and is not yet available for experiments)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThis storage engine is very similar to the MemoryStore except that every snapshot is a separate storage unit (mainly, a file).\u003c/p\u003e\n\u003cp\u003eTBC \u0026hellip;\u003c/p\u003e\n\u003ch3 id="note-on-high-availability"\u003eNote on High Availability\u003c/h3\u003e\n\u003cp\u003eIt can be expected that persistent data structures, providing \u003cem\u003elinearized\u003c/em\u003e update history out of the box within a single branch, are well-suitable for replication in a distributed mode. That\u0026rsquo;s true in general, we can encapsulate a branch into a \u003cem\u003epatch\u003c/em\u003e, transfer it via network and apply in a remote repository. But in case of high-performance SSDs it\u0026rsquo;s not that simple. SSDs may support very high write speeds, dozens of GB/s if using multiple drives, but the network is \u003cem\u003emuch\u003c/em\u003e slower than that. Sending physical blocks over a network, even with good compression, will be a limiting factor. Logical, application-level, replication should be used instead. Memoria will be providing specialized containers to help tracking update history that application is doing.\u003c/p\u003e\n\u003cp\u003eDespite performance limitations, block-level physical replication will be supported out of the box. It does not require any support from applications, it comes basically \u0026ldquo;for free\u0026rdquo;, and it \u003cem\u003emay be useful\u003c/em\u003e in some cases. For example, it will be useful for replication between drives within the same machine. But high-performance system designs should not rely on it.\u003c/p\u003e\n\u003ch2 id="cow-vs-lsm"\u003eCoW vs LSM\u003c/h2\u003e\n\u003cp\u003e\u003ca href="https://en.wikipedia.org/wiki/Log-structured_merge-tree"\u003eLSM\u003c/a\u003e is currently the most popular data structure for database storage. LSM has some theoretical and practical properties that make them irreplaceable. Nevertheless they also have serious theoretical and practical limitations, so proper understanding of how they work may save time and money.\u003c/p\u003e\n\u003cp\u003eLSM accumulate updates in an in-memory buffer (backing them also in a circular persistent buffer aka \u0026lsquo;transaction log\u0026rsquo;) and periodically drop this buffer to a durable storage as a single sorted files \u0026ndash; segments. Background process (garbage collector, GC) then picks up those segments and merges them into a large single file. GC\u0026rsquo;s performance is the main limiting factor of the storage engine. LSM have the following main properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eIt may handle very high peak write speeds\u003c/em\u003e, may be 10x-100x of the average rate. This is the reason why they are irreplaceable for internet applications (web-stores, etc) that should be able to handle \u003cem\u003esudden\u003c/em\u003e massive inflows of users.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWorst-case write complexity is \u003ccode\u003eO(N)\u003c/code\u003e\u003c/em\u003e. Applications may see extremely high update latencies, that is hard to mitigate. Read latency may also spike, see below.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eWe can trade write performance for read performance\u003c/em\u003e. If we merge segments not that aggressively, write performance will be better, but read performance will be degrading.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey rely on an underling filesystem for managing disk space\u003c/em\u003e. Filesystem\u0026rsquo;s performance may be a limiting factor, for example, it may introduce additional read/write latencies.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey require reserving a lot of extra free space (1-2x) for merges\u003c/em\u003e in a worst case.\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eThey theoretically have relatively low write amplification factor\u003c/em\u003e for SSDs, because all updates to disk are sequential.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBy and large, LSMs are very good in their niche, and sometimes they are the only option that fits the requirements (peak write speeds). But they also require extensive \u003cem\u003eexpert-level\u003c/em\u003e tuning and monitoring.\u003c/p\u003e\n\u003cp\u003eCoW trees are different:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTheir peak performance is lower than LSM, but their worst-case complexity is \u003cem\u003elogarithmic\u003c/em\u003e, instead of linear.\u003c/li\u003e\n\u003cli\u003eThey are block-structured and do not need an underling filesystem to allocate the space from. That make worst-case performance even more predictable.\u003c/li\u003e\n\u003cli\u003eThey do not \u003cem\u003eneed\u003c/em\u003e a GC, but specific implementations may use asynchronous space reclamation strategy for best-case performance reasons. Anyway, GC for CoW trees is much simpler than GC for LSM trees. CoW trees have much smaller tunable parameters space.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIdeally, the storage engine should support both data structures, for different cases and purposes. Memoria is using CoW exclusively and may implement LSM with means of the Framework on top of the OLTPStore as an underling filesystem, but if we \u003cem\u003eneed\u003c/em\u003e LSM, we can try using RocksDB first and it doesn\u0026rsquo;t fit, resort to a \u0026lsquo;native\u0026rsquo; solution.\u003c/p\u003e\n\u003ch2 id="distributed-vs-decentralized"\u003eDistributed vs Decentralized\u003c/h2\u003e\n\u003cp\u003eMemoria \u003cem\u003eis not\u003c/em\u003e explicitly addressing distributed environments and scale-out settings. Persistent data structures (PDS) may \u003cem\u003eseem\u003c/em\u003e working well here: eventually consistent K/V store would be enough to host blocks, the problem is in the \u003cem\u003ememory management\u003c/em\u003e. PDS require either deterministic (ARC-based) or tracing garbage collector that is pretty a challenge to build for a distributed environment. And, the most important, it has to be finely tuned to the specifics of selected hardware and application\u0026rsquo;s requirements.\u003c/p\u003e\n\u003cp\u003eNevertheless, Memoria is explicitly addressing \u003cem\u003edecentralized\u003c/em\u003e cease, when there is no single unit of control over a distributed environment. For Memoria\u0026rsquo;s perspective, decentralized environment is a mesh of nodes (or small clusters) running local SWMRStore-based engines and exchanging data \u003cem\u003eexplicitly\u003c/em\u003e with using \u003cem\u003epatches\u003c/em\u003e. Such architecture will be somewhat slower and more complex form the application\u0026rsquo;s perspective (much more control is required) but it does not need a distributed GC. Currently it \u003cem\u003eseems\u003c/em\u003e to be much more universal and fits both local and distributed usage cases.\u003c/p\u003e\n'}).add({id:4,href:"/docs/overview/runtime/",title:"Runtime Environments",description:"",content:'\u003cp\u003eMemoria tries to be a runtime-agnostic wherever it\u0026rsquo;s possible. There are two main dependencies: memory allocation and IO. Custom memory allocation is a rather easy thing. High-performance IO is much harder because we need facilities which are \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003enot properly supported\u003c/a\u003e by the programming language, operation system or both. Memoria relies on fibers for concurrency and they are expected to be \u003ca href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p0876r13.pdf"\u003esupported\u003c/a\u003e for C++26 and is already available in various frameworks and libraries.\u003c/p\u003e\n\u003cp\u003eThe goal of Runtime Environment (RE) is to provide efficient and deterministic UB-free code execution. Framework relies on the thread-per-core/message-passing model running non-migrating fibers. Resumable functions (aka \u0026lsquo;coroutines\u0026rsquo;) will be supported where appropriate, but they are viral and allocate dynamic memory for frames all the way down. Because of frame allocation, resumable functions are not as performant in C++, as it may be expected.\u003c/p\u003e\n\u003cp\u003eThere are 2.5 RE \u0026lsquo;backends\u0026rsquo; in Memoria:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="https://seastar.io/"\u003eSeastar\u003c/a\u003e framework. Best-in-class, high performance and well-supported. But \u003cem\u003eLinux-only\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eBoost Asio. Pretty high performance, perfect compatibility with various runtimes, cross platform, but no high-performance disk IO support.\u003c/li\u003e\n\u003cli\u003eReactor Engine. Initially it was meant to be in-house \u003cem\u003ecross-platform\u003c/em\u003e variant of Seastar with Boost Finer support. But later it lost its momentum and currently is meant to be replaced with combination of Seastar and Asio.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMemoria is not going to rely 100% on the Seastar or Asio API. Instead, the Framework will be trying to isolate specifics of underling API, making code porting easier. 100% cross-RE compatibility is not a goal.\u003c/p\u003e\n\u003cp\u003eTBС\u0026hellip;\u003c/p\u003e\n'}).add({id:5,href:"/docs/overview/vm/",title:"DSL \u0026 VM",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:6,href:"/docs/overview/mbt/",title:"Memoria Build Tool",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:7,href:"/docs/overview/qt_creator_instructions/",title:"Qt Creator Instructions",description:"",content:'\u003ch2 id="dependencies"\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eMemoria relies on third-party libraries that either may not be available on supported developenment platfroms or have outdated versions there. Vcpkg package manager is currently being used for dependency management. Memoria itself is avaialble via \u003ca href="https://github.com/victor-smirnov/memoria-vcpkg-registry"\u003ecustom Vcpkg registry\u003c/a\u003e. Conan recipies and source packages for Linux distributions (via CPack) may be provided in the future.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/docker/Dockerfile"\u003eDockerfile\u003c/a\u003e on how to configure development environment on Ubuntu 22.04. Standard development environment will be the latest Ubuntu LTS.\u003c/p\u003e\n\u003ch2 id="install-vcpkg-for-memoria"\u003eInstall VCPkg for Memoria\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd /path/to/checout/vcpkg/into\n$ git clone https://github.com/microsoft/vcpkg.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor now, supporting compiler is Clang. Gcc 10/11/12 are crashing on Memoria. Gcc 13.1 is known to work.\u003c/p\u003e\n\u003ch2 id="configuring-vcpkgs-provided-cmake-tool"\u003eConfiguring VCPkg\u0026rsquo;s provided cmake tool\u003c/h2\u003e\n\u003cp\u003eIn Options/Kits/Cmake tab add another cmake configuration by specifying full path VCPkg\u0026rsquo;s own cmake distribution.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-required-clang-compiler"\u003eConfigure Required clang compiler\u003c/h2\u003e\n\u003cp\u003eMemoria currently is built with clang compiler version 6.0 or newer. If you system already provides it, like most Linux distributions do, then this step is unnecessary. Otherwise, build clang yourself and configure it on the Options/Kits/Compiler tab:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-clang.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="add-new-kit-for-clang"\u003eAdd new Kit for Clang\u003c/h2\u003e\n\u003cp\u003eAdding new Kit is necessary if QtCreator did not recognize clang compiler automatically. Just create new kit by cloning and existing one and specify clang 17 as C and C++ compilers:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-newkit.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="vcpkgs-cmake-selection"\u003eVCPkg\u0026rsquo;s Cmake Selection\u003c/h2\u003e\n\u003cp\u003eNow specify that VCPkg\u0026rsquo;s provided cmake tool will be used for new Kit, and specify the path to VCPkg\u0026rsquo;s libraries definitions:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-kit-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eProvide your full path to vcpkg.cmake:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-vcpkg-toolchain.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-memorias-build-parameters"\u003eConfigure Memoria\u0026rsquo;s build parameters\u003c/h2\u003e\n\u003cp\u003eToggle BUILD_* options as specified on the screenshot. This will build Tests, as well as threads- and fibers-based Memoria allocators, with libbacktrace support in case of exceptions.\u003c/p\u003e\n\u003cp\u003eMore details on build options can be found in top-level CMakeLists.txt\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-project-cfg.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThat\u0026rsquo;s it!\u003c/p\u003e\n\u003cp\u003ePress Ctrl+B to start build process.\u003c/p\u003e\n'}).add({id:8,href:"/docs/data-zoo/overview/",title:"Core Data Structures -- Overview",description:"",content:"\u003cp\u003eThis sections contans detailed description of some core data structures Memoria\u0026rsquo;s containers are based on.\u003c/p\u003e\n"}).add({id:9,href:"/docs/data-zoo/partial-sum-tree/",title:"Partial Sums Tree",description:"",content:'\u003ch2 id="description"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s take a sequence of monotonically increasing numbers, and get delta sequence from it, as it is shown on the following figure. Partial Sum Tree is a tree of sums over this delta sequence.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="trees.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003ePacked tree in Memoria is a multiary balanced tree mapped to an array. For instance in a level order as it shown on the figure.\u003c/p\u003e\n\u003cp\u003eGiven a sequence of N numbers with monotonically increasing values, partial sum tree provides several important operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efindLT(value)\u003c/code\u003e finds position of maximal element less than \u003ccode\u003evalue\u003c/code\u003e, time complexity $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efindLE(value)\u003c/code\u003e finds position of maximal element less than or equals to \u003ccode\u003evalue\u003c/code\u003e, $T = O(log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esum(to)\u003c/code\u003e computes plain sum of values in the delta sequence in range [0, to), $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eadd(from, value)\u003c/code\u003e adds \u003ccode\u003evalue\u003c/code\u003e to all elements of original sequence in the range of [from, N), $T = O(log(N))$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is obvious that first two operations can be computed with binary search without partial sum tree and all that overhead it introduces.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003ePartial or prefix sum trees of higher degrees are especially suitable for hardware acceleration. DDR and HBM memory transfer data in batches and works best if data is processed also in batches. The idea here is to offload tree traversal operation from CPU to the memory controller or even to DRAM memory chips. Sum and compare operations are relatively cheap to implement, and no complex control is required.\u003c/p\u003e\n\u003cp\u003eBy offloading tree traversal to the memory controller (that usually works in front of caches), we can save precious cache space for more important data. By offloading summing and comparison to DRAM chips, we can better exploit internal memory parallelism and save memory bandwidth. In such distributed architecture, a single tree level scan can be performed with the latency and in the power budget of a \u003cem\u003esingle random memory access\u003c/em\u003e, saving energy and silicon for other computations.\u003c/p\u003e\n'}).add({id:10,href:"/docs/data-zoo/searchable-seq/",title:"Searchable Sequence",description:"",content:'\u003cp\u003eSearchable sequence or rank/select dictionary is a sequence of symbols that supports two operations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003erank(position, symbol)\u003c/code\u003e is number of occurrences of \u003ccode\u003esymbol\u003c/code\u003e in the sequence in the range [0, position)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect(rank, symbol)\u003c/code\u003e is a position of \u003ccode\u003erank\u003c/code\u003e-th occurrence of the \u003ccode\u003esymbol\u003c/code\u003e in the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsually the alphabet is {0, 1} because of its practical importance, but larger alphabets are of significant interest too. Especially in Bioinformatics and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eThere are many implementations of binary searchable sequences (bitmaps) providing fast query operations with $O(1)$ time complexity. Memoria uses partial sum indexes to speedup rank/select queries. They are asymptotically slower than other methods but have additional space overhead for the index.\u003c/p\u003e\n\u003cp\u003ePacked searchable sequence is a searchable sequences that has all its data structured packed into a single contiguous memory block with packed allocator. It consists from two data structures:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003emulti-index partial sum tree to speedup rank/select queries;\u003c/li\u003e\n\u003cli\u003earray of sequence\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for the case of searchable bitmap.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_seq.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that for a sequence with K-symbol alphabet, packed sum tree has K indexes, that results in significant overhead even for relatively small alphabets. For example 8-bit sequence has 256-index packed tree that takes more than 200% of raw data size if the tree is not compressed. To lower this overhead Memoria provides various compressed encodings for the index\u0026rsquo;s values.\u003c/p\u003e\n\u003ch2 id="creation-and-access"\u003eCreation and Access\u003c/h2\u003e\n\u003cp\u003eTo create partial sum tree for a sequence we first need to split it logically into blocks of fixed number of symbols (16 at the figure). Then sum different symbols in the block, each such vector is a simple partial sum tree leaf. Build other levels of the tree accordingly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSymbol update is relatively fast, it takes $O(log(N))$ time.\u003c/li\u003e\n\u003cli\u003eSymbol insertion is $O(N)$, it requires full rebuilding of partial sum tree.\u003c/li\u003e\n\u003cli\u003eSymbol access does not require the tree to perform, it takes $O(1)$ time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003erank(position, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven \u003ccode\u003eposition\u003c/code\u003e, determine sequence \u003ccode\u003eblock_number\u003c/code\u003e for that position, and \u003ccode\u003eblock_pos\u003c/code\u003e position in the block, $O(1)$;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eblock_rank\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) in the sum tree, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003ecount number of \u003ccode\u003esymbol\u003c/code\u003es in the block to \u003ccode\u003eblock_pos\u003c/code\u003e, $O(1)$;\u003c/li\u003e\n\u003cli\u003efinal rank is (2) + (3).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003eselect(rank, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven partial sum tree and a \u003ccode\u003esymbol\u003c/code\u003e, determine target sequence \u003ccode\u003eblock_number\u003c/code\u003e having \u003ccode\u003etotal_rank \u0026lt;= rank\u003c/code\u003e for the given \u003ccode\u003esymbol\u003c/code\u003e using \u003ca href="/docs/data-zoo/partial-sum-tree"\u003efindLE\u003c/a\u003e operation, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eFor the given block, compute \u003ccode\u003erank_prefix\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) for the given \u003ccode\u003esymbol\u003c/code\u003e, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eCompute \u003ccode\u003elocal_rank = rank - rank_prefix\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003eScan the block and find position of \u003ccode\u003esymbol\u003c/code\u003e having rank in the block = \u003ccode\u003elocal_rank\u003c/code\u003e, $O(1)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eActual implementation joins operations (1) and (2) into a single traverse of the sum tree.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eConsideration are the same as for \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial/prefix sum trees\u003c/a\u003e, especially, because searchable sequence contains partial sum tree as an indexing structure.\u003c/p\u003e\n\u003cp\u003eModern CPUs usually have direct implementations for rank (PopCount) for binary alphabets. Select operation may also be partially supported. To accelerate searchable sequences, it\u0026rsquo;s necessary to implement rang/select over arbitrary alphabets (1-8 bits per symbol).\u003c/p\u003e\n\u003cp\u003eSymbol blocks are also contiguous in memory and can be multiple of DRAM memory blocks. Rank/select machinery is simpler or comparable with machinery for addition and subtraction. Those operations can be efficiently implemented in a small silicon budget and at high frequency.\u003c/p\u003e\n'}).add({id:11,href:"/docs/data-zoo/compressed-symbol-seq/",title:"Compressed Symbol Sequence",description:"",content:'\u003cp\u003eCompressed symbol sequence takes space proportional to the \u003cem\u003einformation\u003c/em\u003e contained in it, not to the number of symbols at the first place. It\u0026rsquo;s achievable via using special encoding and Memoria is using the simplest one \u0026ndash; run-length encoding (RLE). The basic idea is simple: symbol sequence is represented as a \u003cem\u003eseries of short repeatable patterns\u003c/em\u003e. The main challenge is how to encode parameters succinctly in the way that minimizes overhead for poorly compressible sequences.\u003c/p\u003e\n\u003cp\u003eCurrently, the main \u003cem\u003efamily\u003c/em\u003e of compressible sequence encoding in Memoria is called SSRLE or Streaming Symbol RLE. The \u0026ldquo;streaming\u0026rdquo; in the name reflects the \u003cem\u003eintention\u003c/em\u003e to optimize it for streaming applications and hardware acceleration, but otherwise means nothing.\u003c/p\u003e\n\u003cp\u003eIn SSRLE the smallest unit of sequence is \u0026ldquo;Run\u0026rdquo; that is pair of \u003cem\u003epattern\u003c/em\u003e and its positive \u003cem\u003elength\u003c/em\u003e. The \u003cem\u003epattern\u003c/em\u003e also has positive length. Memory-wise, the smallest element of the sequence is \u003cem\u003eCodeUnit\u003c/em\u003e, that is currently 2 bytes (16 bits). So the smallest sequence size is 2 bytes. A \u003cem\u003eCodeWord\u003c/em\u003e is a sequence of \u003cem\u003eCodeUnit\u003c/em\u003es of the size from 1 to 4. A Run has to fit into a single CodeWord, that is currently from 16 to 64 bits. A \u003cem\u003eSegment\u003c/em\u003e is a series of \u003cem\u003eCodeWord\u003c/em\u003es up to 32 \u003cem\u003eCodeUnit\u003c/em\u003es (64 bytes). \u003cem\u003eCodeWord\u003c/em\u003e may not cross the boundary of segment. In case of a partially-filled segment, special CodeWord with zero pattern length may be used to denote \u0026ldquo;padding\u0026rdquo;. If SSRLE is hardware accelerated, Segment may be a unit of processing.\u003c/p\u003e\n\u003cp\u003eSpecific values for the set above define specific encoding from the \u003cem\u003efamily\u003c/em\u003e of SSRLE encodings.\u003c/p\u003e\n\u003cp\u003e\u003cfigure class="figure"\u003e\r\n    \u003cimg class="figure-img img-fluid lazyload blur-up" data-sizes="auto" src="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_20x0_resize_box_3.png" data-srcset="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_900x0_resize_box_3.png 900w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_800x0_resize_box_3.png 800w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_700x0_resize_box_3.png 700w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_600x0_resize_box_3.png 600w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_500x0_resize_box_3.png 500w" width="2750" height="563" alt="SSRLE Run"\u003e\r\n    \u003cnoscript\u003e\u003cimg class="figure-img img-fluid" sizes="100vw" srcset="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_900x0_resize_box_3.png 900w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_800x0_resize_box_3.png 800w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_700x0_resize_box_3.png 700w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_600x0_resize_box_3.png 600w,https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run_hu98e24c3457cce8fc7785421fb603da35_58041_500x0_resize_box_3.png 500w" src="https://memoria-framework.dev/docs/data-zoo/compressed-symbol-seq/ssrle_run.png" width="2750" height="563" alt="SSRLE Run"\u003e\u003c/noscript\u003e\r\n    \u003c/figure\u003e\r\n\u003c/p\u003e\n\u003cp\u003eThere are four field in a SSRLE run encoding:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCodeWord length\u003c/strong\u003e. Number code units representing CodeWord. Fixed 2 bits for all symbol types, up to 4 code units in a code word.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePattern Length\u003c/strong\u003e (PL). Number of symbols in the pattern.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSymbol Pattern\u003c/strong\u003e. Sequence of symbols.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRun Length\u003c/strong\u003e (RL). Length of the Run \u003cem\u003ein patterns\u003c/em\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSSRLE supports multiple symbol sizes (number of bits per symbol). In Memoria SSRLE is defined for symbol alphabets from 1 to 8 bits per symbol (\u003cem\u003eBPS\u003c/em\u003e).\u003c/p\u003e\n\u003cp\u003eFor BPS = 1 we have a compressed bitmap (cor compressed bit vector). In this case (code word is up to 64 bits), we need at most 6 bits to represent possible pattern lengths. The longest pattern then is \u003ccode\u003e64-(6 + 2) = 56\u003c/code\u003e bits. So for each 7 bytes of uncomressed bitmap we have 1 byte of \u003cem\u003ememory overhead\u003c/em\u003e, that is about 14%. SSRLE has rather large (yet acceptable) overhead, comparing to other compressed bitmap encodings. The point is that, unlike other representations, SSRLE supports repeatable patterns, not just long runs of \u003ccode\u003e0...\u003c/code\u003es and \u003ccode\u003e1...\u003c/code\u003es.\u003c/p\u003e\n\u003cp\u003eFor larger BPSs we will have different possible values of Pattern Length and Run Length, because symbols are wider in bits and patterns are shorter. The following table summarizes all available variants from different BPPs:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eBPS\u003c/th\u003e\n\u003cth\u003ePL in Bits\u003c/th\u003e\n\u003cth\u003eMax Pattern Length (in Symbols)\u003c/th\u003e\n\u003cth\u003eMax RL in bits\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e56\u003c/td\u003e\n\u003ctd\u003e55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e28\u003c/td\u003e\n\u003ctd\u003e55\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e18\u003c/td\u003e\n\u003ctd\u003e54\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e14\u003c/td\u003e\n\u003ctd\u003e54\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e11\u003c/td\u003e\n\u003ctd\u003e53\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e6\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e9\u003c/td\u003e\n\u003ctd\u003e52\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e52\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e8\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e51\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAuthoritative source for information about actual SSRLE parameters are \u003ca href="https://github.com/victor-smirnov/memoria/tree/master/core/include/memoria/core/ssrle"\u003esources\u003c/a\u003e.\u003c/p\u003e\n'}).add({id:12,href:"/docs/data-zoo/hierarchical-containers/",title:"Hierarchical Containers",description:"",content:'\u003cp\u003eIn this page it\u0026rsquo;s explained how various hierarchical containers (vectors with large values, multimaps, tables, wide tables, etc) are organized inside and how \u003ca href="/docs/data-zoo/searchable-seq/"\u003esearchable sequences\u003c/a\u003e make it possible.\u003c/p\u003e\n\u003ch2 id="multimap"\u003eMultimap\u003c/h2\u003e\n\u003cp\u003eLet, for simplicity, we have a multimap \u003ccode\u003estd::map\u0026lt;int64_t, std::vector\u0026lt;uint8_t\u0026gt;\u0026gt;\u003c/code\u003e (we call it a \u003cem\u003eMultimap\u003c/em\u003e here) that is internally a binary search tree (RB-/AVL-tree etc) with vectors as values. Vector\u003cT\u003e is internally a contiguous data structure but the search tree is basically a tree-shaped linked list or \u003cem\u003epointer chasing\u003c/em\u003e data structure. This data layout works perfectly for the \u003cem\u003eword\u003c/em\u003e-addressed main memory, but if we want to place out data structure into \u003cem\u003eblock\u003c/em\u003e-addressed external memory, the whole thing gets trickier, because the using of allocation now is pretty large comparing to RAM: 4KB and greater. In the external memory we can still combine Map (represented as B+Tree) with Dynamic Vector (represented as B+Tree) the same way how two those containers \u003ccode\u003estd::map\u0026lt;\u0026gt;\u003c/code\u003e and \u003ccode\u003estd::vector\u0026lt;\u0026gt;\u003c/code\u003e are combined with each other (via references) and get either a set of B+Trees or one \u0026lsquo;hierarchical\u0026rsquo; B+Tree, but the minimal size of value vector will be \u003cem\u003eone block\u003c/em\u003e that is pretty large. Practical applications can optimize this specific case using various techniques like placing short vectors right inside the parent container\u0026rsquo;s (Map in this case) leaf block, and only spilling a new B+Tree when there is no more room for that in the block. It works pretty well in practice, but we can do much better. Below it\u0026rsquo;s explained how we can use searchable sequences for fitting arbitrary-shaped hierarchical containers into a single (or \u0026lsquo;flat\u0026rsquo;) B+Tree.\u003c/p\u003e\n\u003ch3 id="data-structure"\u003eData structure\u003c/h3\u003e\n\u003cp\u003eLet for certainty we have the following Multimap: \u003ccode\u003e{1=[1,5,2,4], 4=[7,3,1], 6=[5,9,1,2,0], 7=[8,1]}\u003c/code\u003e. We represent is with \u003cem\u003ethree\u003c/em\u003e arrays: keys \u003ccode\u003eK[]\u003c/code\u003e, values \u003ccode\u003eV[]\u003c/code\u003e and \u003ca href="/docs/data-zoo/searchable-seq/"\u003esearchable symbol sequence\u003c/a\u003e \u003ccode\u003eS[]\u003c/code\u003e. \u003ccode\u003eSrle[]\u003c/code\u003e is an \u003ca href="/docs/data-zoo/compressed-symbol-seq/"\u003eRLE encoding\u003c/a\u003e for \u003ccode\u003eS[]\u003c/code\u003e (we need one \u003ccode\u003eS[]\u003c/code\u003e or \u003ccode\u003eSrle[]\u003c/code\u003e, \u003cem\u003enot\u003c/em\u003e both):\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multimap.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe rule is simple. The keys vector \u003ccode\u003eK[]\u003c/code\u003e should be built in the increasing key order to use binary search for fast lookup. The values vector \u003ccode\u003eV[]\u003c/code\u003e should be built in the \u003cem\u003ekey vector\u0026rsquo;s order\u003c/em\u003e by concatenating corresponding vectors.\u003c/p\u003e\n\u003cp\u003eThe symbol sequence \u003ccode\u003eS[]\u003c/code\u003e is constructed in the following way. For each key in key\u0026rsquo;s vector \u003ccode\u003eK[]\u003c/code\u003e we put \u003ccode\u003e0\u003c/code\u003e and for each value element in the \u003cem\u003eassociated value\u0026rsquo;s sub-vector\u003c/em\u003e \u003ccode\u003eV[]\u003c/code\u003e we put \u003ccode\u003e1\u003c/code\u003e as it\u0026rsquo;s shown at the picture above. We will be using \u003ca href="/docs/data-zoo/searchable-seq/"\u003e\u003ccode\u003erank()\u003c/code\u003e and \u003ccode\u003eselect()\u003c/code\u003e\u003c/a\u003e operations for implementing access and update operations over our Multimap.\u003c/p\u003e\n\u003cp\u003eTo find a value for a key we first need to find a position for this key in the sorted vector \u003ccode\u003eK[]\u003c/code\u003e by using binary search: \u003ccode\u003ePx = bsearch(S[], Kx)\u003c/code\u003e. Given the position \u003ccode\u003ePx\u003c/code\u003e for key \u003ccode\u003eKx\u003c/code\u003e we can find corresponding position \u003ccode\u003eSx\u003c/code\u003e in the \u003ccode\u003eS[]\u003c/code\u003e by using the \u003ccode\u003eselect\u003c/code\u003e operation: \u003ccode\u003eSx = select(S[], Px + 1, 0)\u003c/code\u003e \u0026ndash; we are looking for position of \u003ccode\u003ePx\u003c/code\u003e-th \u003ccode\u003e0\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e. \u003ccode\u003eSx + 1\u003c/code\u003e will be position of the first symbol \u003ccode\u003e1\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e, corresponding to the first value, associated with \u003ccode\u003eKx\u003c/code\u003e. In order to locate related position in \u003ccode\u003eV[]\u003c/code\u003e we need to calculate total size of all sub-vectors for keys \u003cem\u003ebefore\u003c/em\u003e \u003ccode\u003eKx\u003c/code\u003e: \u003ccode\u003eVx = rank(S[], Px, 1)\u003c/code\u003e. In order to find number of elements in specific sub-vector, associated with \u003ccode\u003eKx\u003c/code\u003e we can use \u003ccode\u003ecount()\u003c/code\u003e operation \u0026ndash; count number of symbols in a run, starting from the specified one: \u003ccode\u003eLx = count(S[], Px + 1, 1)\u003c/code\u003e \u0026ndash; counting number of \u003ccode\u003e1\u003c/code\u003e in \u003ccode\u003eS[]\u003c/code\u003e starting from \u003ccode\u003ePx + 1\u003c/code\u003e till we either hit the next \u003ccode\u003e0\u003c/code\u003e (next key\u0026rsquo;s symbol mark in the sequence) or the end of the sequence.\u003c/p\u003e\n\u003cp\u003eBy applying the same math we can derive corresponding operations for updating the Multimap structure: insert entries, remove entries, merge and split maps and so on. \u0026lsquo;Rank()\u0026rsquo;, \u0026lsquo;select()\u0026rsquo; and \u0026lsquo;count()\u0026rsquo; have logarithmic time complexity.\u003c/p\u003e\n\u003cp\u003eSpending one bit for every value element in the Multimap is not necessary. We can easily use RLE encoding \u003ccode\u003eSrle[]\u003c/code\u003e for \u003ccode\u003eS[]\u003c/code\u003e as it\u0026rsquo;s shown at the picture above. For large value sub-vectors (file system/object store) space saving may be significant.\u003c/p\u003e\n\u003ch3 id="representing-relational-tables"\u003eRepresenting relational tables\u003c/h3\u003e\n\u003cp\u003eNote that Multimap is sufficient for representing a row-wise \u003cem\u003eclustered\u003c/em\u003e relational table. \u003ccode\u003eK\u003c/code\u003e is a table\u0026rsquo;s \u003cem\u003eprimary key\u003c/em\u003e and corresponding sub-vector may contain row\u0026rsquo;s content in an unstructured form. If we want unclustered (regular) table without a primary key, we just not needed the \u003ccode\u003eK[]\u003c/code\u003e vector: \u003ccode\u003estd::vector\u0026lt;std::vector\u0026lt;V\u0026gt;\u0026gt;\u003c/code\u003e. Everything else is the same.\u003c/p\u003e\n\u003cp\u003eSuch table representation has two notable properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eTo \u003cem\u003escan\u003c/em\u003e a table we just need to scan three perfectly memory-aligned data structures concurrently.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTable may easily have \u003cem\u003every large\u003c/em\u003e rows, as well very small rows. There is no any intrinsic memory overhead for this.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="multimap-with-searchable-values"\u003eMultimap with searchable values\u003c/h2\u003e\n\u003cp\u003ePractically important case is \u003ccode\u003estd::map\u0026lt;K, std::set\u0026lt;V\u0026gt;\u0026gt;\u003c/code\u003e or, here, SearchableMultimap used, for instance, for implementing sparse graphs. If \u003ccode\u003eK\u003c/code\u003e and \u003ccode\u003eV\u003c/code\u003e are graph node\u0026rsquo;s identifiers, SearchableMultimap may be used for storing node\u0026rsquo;s neighbours in the graph.\u003c/p\u003e\n\u003cp\u003eThe easiest way to make sub-vectors efficiently searchable is just to sort them. And this is the only option if identifiers are not numbers. Because we know the size of sub-vector for a given key, we can binary-search in this sub-vector only.\u003c/p\u003e\n\u003cp\u003eThe second option is to use the same technique that is used for \u003ca href="/docs/data-zoo/partial-sum-tree/"\u003epartial sum trees\u003c/a\u003e. If identifiers are numbers supporting \u003ccode\u003e+\u003c/code\u003e and \u003ccode\u003e-\u003c/code\u003e binary operations, we can build \u003cem\u003edelta sequences\u003c/em\u003e for each sub-array individually and concatenate them into \u003ccode\u003eV[]\u003c/code\u003e. Unlike locally-sorted sub-vectors, such delta-sequence is globally-searchable. We just need to add \u003cem\u003eprefix\u003c/em\u003e \u003ccode\u003eNx = sum(V[], Vx)\u003c/code\u003e for key the \u003ccode\u003eKx\u003c/code\u003e to the \u003ccode\u003eKx\u003c/code\u003e\u0026rsquo;s value.\u003c/p\u003e\n\u003ch2 id="wide-table"\u003eWide Table\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eWide Table\u003c/em\u003e is a data structure of the form \u003ccode\u003estd::map\u0026lt;RKT, std::map\u0026lt;CKT, std::vector\u0026lt;V\u0026gt;\u0026gt;\u0026gt;\u003c/code\u003e where \u003ccode\u003eRKT\u003c/code\u003e is a row key type and \u003ccode\u003eCKT\u003c/code\u003e is a column key type. Each row of a Wide Table may have arbitrary, practically unlimited, number of columns. This is rarely needed in practice for \u003cem\u003erelational\u003c/em\u003e tables. But may be needed for other \u003cem\u003eapplication-level\u003c/em\u003e data structures built on top of wide tables. Basically, wide table is a sparse matrix or a sparse graph.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="wide-table.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIt\u0026rsquo;s rather easy to get wide table from a Multimap, we just need an alphabet with three symbols instead of two: \u003ccode\u003e0\u003c/code\u003e means row row key RK, \u003ccode\u003e1\u003c/code\u003e means column key CK and \u003ccode\u003e2\u003c/code\u003e means column data. Everything else, including navigational operations are basically the same.\u003c/p\u003e\n\u003ch2 id="generalized-hierarchical-container"\u003eGeneralized Hierarchical Container\u003c/h2\u003e\n\u003cp\u003eThe pattern above can be generalized. Single-level containers, like Map or Vector need a symbol sequence with zero-size alphabet (or no sequence at all). Each new layer add one symbol to the alphabet. Memoria relies heavily on this property for complex containers. For an L-level container we have L vectors and a searchable symbol sequence (so, L + 1 vectors total).\u003c/p\u003e\n\u003ch2 id="multistream-btree"\u003eMultistream B+Tree.\u003c/h2\u003e\n\u003cp\u003eThere are basically two strategies of implementing generalized hierarchical containers for external memory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cem\u003eOne B+Tree per vector or sequence\u003c/em\u003e. This is the easiest option because we just need two separately engineered B+Trees. There are two main drawbacks here. First, small structures may take up to \u003ccode\u003eL + 1\u003c/code\u003e blocks of memory. Second, for each query we need to search in multiple B+Trees (from root down to leafs).\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eCombine all vectors in one B+Tree\u003c/em\u003e, enforcing locality principle: data accessed together shout be in the same block or very close to each other. It can make certain important queries faster, but by the expense of other types of queries.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNo strategy is universally the best, but the second one is better if we want to optimize things for \u003cem\u003ereading\u003c/em\u003e. Memoria may use them both for different reasons, but it primarily relies on the second one \u0026ndash; \u003cem\u003emultistream B+Tree\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThe basic idea is simple.\u003c/p\u003e\n\u003cp\u003eIf we are representing a Multimap with a multistream B+Tree, we need to split \u003ccode\u003eSrle[]\u003c/code\u003e, \u003ccode\u003eK[]\u003c/code\u003e and \u003ccode\u003eV[]\u003c/code\u003e in such way that all related array elements be put into the same leaf node. In this case, for example, if we found specific key, associated values will be either in this leaf (most likely) or in the next one (pretty quick operation).\u003c/p\u003e\n\u003cp\u003eBranch node\u0026rsquo;s structure is similar. We need one array for maximum keys for each child (Key Stream), and two arrays for sums of \u003ccode\u003e0\u003c/code\u003e and \u003ccode\u003e1\u003c/code\u003e in the subtree\u0026rsquo;s \u003ccode\u003eSrle\u003c/code\u003e sequence (Srle Stream). And, of course, child node ID array:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree_nodes.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBranch nodes for \u003ccode\u003eSrle\u003c/code\u003e sequence form a \u003ca href="/docs/data-zoo/partial-sum-tree/"\u003epartial/prefix sum tree\u003c/a\u003e, that can be used for efficient implementation of \u0026lsquo;rank()\u0026rsquo; and \u0026lsquo;select()\u0026rsquo; operations.\u003c/p\u003e\n\u003cp\u003eKeys stream in this specific case may be searched in a usual way for max-type B+tree.\u003c/p\u003e\n\u003cp\u003eNote that in this example branch nodes do not show Value Stream, because values are not searchable, so we don\u0026rsquo;t need to store separate \u003cem\u003eindex\u003c/em\u003e info for values.\u003c/p\u003e\n\u003cp\u003eNote also that mutistream B+Tree implementation in Memoria is slightly different in the way how streams are represented in tree blocks, here we omitted some details for brevity.\u003c/p\u003e\n'}).add({id:13,href:"/docs/data-zoo/louds-tree/",title:"Level Order Unary Degree Sequence (LOUDS) ",description:"",content:'\u003cp\u003eLevel Order Unary Degree Sequence or LOUDS is a special form of ordered tree encoding. To get it we first need to enumerate all nodes of a tree in level order as it is shown of the following figure.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds.png"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThen for each node we write its degree in unitary encoding. For example, degree of the fist node is 3, then its unitary encoding is \u0026lsquo;1110\u0026rsquo;. To finish LOUDS string we need to prepend substring \u0026lsquo;10\u0026rsquo; to it as it is shown on the figure. Given an ordered tree on N nodes LOUDS takes no more than 2N + 1 bits. This is very succinct implicit data structure.\u003c/p\u003e\n\u003cp\u003eLOUDS is a bit vector. We also need the following operations on it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erank1(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;1\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank0(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;0\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect1(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;1\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect0(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;0\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDifferent ways of tree node numbering for LOUDS are possible, Memoria uses the simplest one. Tree node positions are coded by \u0026lsquo;1\u0026rsquo;.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003enode_num = rank1(i + 1)\u003c/code\u003e \u0026ndash; gets tree node number at position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ei = select1(node_num)\u003c/code\u003e \u0026ndash; finds position of a node in LOUDS given its number in the tree.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHaving this node numbering we can define the following tree navigation operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efist_child(i) = select0(rank1(i + 1)) + 1\u003c/code\u003e \u0026ndash; finds position of the first child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_child(i) = select0(rank1(i + 1) + 1) - 1\u003c/code\u003e \u0026ndash; finds position of the last child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparent(i) = select1(rank0(i + 1))\u003c/code\u003e \u0026ndash; finds position of the parent for the node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echildren(i) = last_child(i) - first_child(i)\u003c/code\u003e \u0026ndash; return number of children for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echild(i, num) = first_child(i) + num\u003c/code\u003e \u0026ndash; returns position of num-th child for the node at the position \u003ccode\u003ei\u003c/code\u003e, \u003ccode\u003enum \u0026gt;= 0\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_node(i) = LOUDS[i] == 1 ? true : false\u003c/code\u003e \u0026ndash; checks if \u003ccode\u003ei\u003c/code\u003e-th position in tree node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that navigation operations only defined for positions \u003ccode\u003ei\u003c/code\u003e for those \u003ccode\u003eis_leaf(i) == true\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cp\u003eLet we find number of the first child for the node 8.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eselect1(8) = 11\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efirst_child(11) = select0(rank1(11 + 1)) + 1 = select0(8) + 1 = 19\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank1(19 + 1) = 12\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first child for node 8 is node 12.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how the parent() operation works:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds-parent.png"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="limitations"\u003eLimitations\u003c/h2\u003e\n\u003cp\u003eNode numbers are valid only until insertion (or deletion) into the tree. Additional data structure is necessary to keep track of node positions after tree structure updates.\u003c/p\u003e\n\u003ch2 id="labelled-tree"\u003eLabelled Tree\u003c/h2\u003e\n\u003cp\u003eLabelled tree is a LOUDS tree with a fixed set of numbers (or \u0026lsquo;labels\u0026rsquo;) associated with each node. It is implemented as multistream balanced tree where the first stream is dynamic bit vector with rank/select support, and the rest are streams for each label.\u003c/p\u003e\n\u003cp\u003eStreams elements distribution is very simple for this data structures. Each label belongs to a tree node that is coded by position of \u0026lsquo;1\u0026rsquo; in LOUDS. Each leaf of balanced tree has some subsequence of the LOUDS.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="labeled_tree_leaf.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLet we have a LOUDS sub-stream of size N with M 1s for a given leaf. Then this leaf must contain M labels in each label stream. The following expressions links together node and level positions withing a leaf:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elabel_idx = rank1(node_idx + 1) - 1\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enode_idx = select1(label_idx + 1)\u003c/code\u003e;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhere \u003ccode\u003elabel_idx\u003c/code\u003e is in [0, M) and \u003ccode\u003enode_idx\u003c/code\u003e is in [0, N)\u003c/p\u003e\n\u003cp\u003eLabeledTree uses balanced partial sum tree as a basic data structure. Because of that it optionally supports partial sums of labels is specified tree node range. The most interesting range is \u003ccode\u003e[0, node_idx)\u003c/code\u003e. See \u003ca href="/docs/data-zoo/wavelet-tree"\u003eMultiary Wavelet Tree\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2 id="cardinal-trees"\u003eCardinal Trees\u003c/h2\u003e\n\u003cp\u003eIn ordered trees like in the example above, all children nodes are naturally ordered, and a node may have arbitrary number of children. In the cardinal tree of degree $D$, a node always have $D$ children, but some children can be omitted. And this information is stored in the tree, like \u0026ldquo;child $i$ is absent\u0026rdquo;. Binary search trees are cardinal trees of degree 2.\u003c/p\u003e\n\u003cp\u003eCardinal tree of degree 4 (and greater, where degree is a power of 2) is a trie-based (or region-based) \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCardinal LOUDS tree can be implemented either as a labelled tree, where node labels (from 0 to $D-1$) are \u003cem\u003ecardinal labels\u003c/em\u003e (for sparse cardinal trees like spatial trees), or by using a searchable bitmap specifying which children are present. The bitmap can be \u003cem\u003ecompressed\u003c/em\u003e, saving space for sparse cases.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS trees are especially important type of compact/succinct trees, because all children of a node are stored linearly in memory, that is DRAM-friendly: traversal of a tree is a series of liner scans split by random jumps. But the number of random jumps is much smaller comparing to other types of trees. LOUDS trees do not require any specific hardware support, providing that \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable bitmap\u003c/a\u003e is fully accelerated.\u003c/p\u003e\n'}).add({id:14,href:"/docs/data-zoo/wavelet-tree/",title:"Multiary Wavelet Trees",description:"",content:'\u003cp\u003eWavelet trees (WT) are data succinct rank/select dictionaries for large alphabets with many \u003ca href="http://arxiv.org/abs/1011.4532"\u003epractical applications\u003c/a\u003e. There is a good \u003ca href="http://alexbowe.com/wavelet-trees/"\u003eexplanation\u003c/a\u003e of what binary wavelet trees are and how they work. They provide rank() and select() over symbol sequences ($N$ symbols) drawn from arbitrary fixed-size alphabets ($K$ symbols) in $O(log(N) * log(K))$ operations, where logarithms are on the base of 2. Therefore, for large alphabets, $log(K)$ is quite a big value that leads to big hidden constants in practical implementations of the binary WT.\u003c/p\u003e\n\u003cp\u003eIn order to improve runtime efficiency of wavelet trees we have to lower this constant. And one of the way here is to use multiary cardinal trees instead of binary ones. In this case, for $M$-ary cardinal tree we will have $log(M)$ speedup factor over binary trees (tree height is $log(M)$-times smaller).\u003c/p\u003e\n\u003ch2 id="wavelet-tree-structure"\u003eWavelet Tree Structure\u003c/h2\u003e\n\u003cp\u003eLet we have a sequence of integers, say, 54.03.12.21.47.03.17.54.22.51 drawn from 6-bit alphabet. The following figure shows 4-ary wavelet tree for this sequence. Such WT has $6/log_2(4) = 3$ levels.\u003c/p\u003e\n\u003cp\u003eFirst we need to represent our sequence in a different format. Our WT is 4-ary and has 3 layers. We need to \u0026ldquo;split\u0026rdquo; the sequence in 3 layers horizontally where symbols of each layer are drawn from 2-bit alphabet. In other words, we need to recode our sequence from base of 10 to base of 4, and then write numbers vertically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that this is just a logical operation, it doesn\u0026rsquo;t require any transformation of the sequence itself.\u003c/p\u003e\n\u003cp\u003eOur WT is a 4-ary cardinal tree, each node has from 0 to 4 children. Each child represents one symbol from the layer\u0026rsquo;s alphabet. Note that in general case it isn\u0026rsquo;t necessary to draw all layers from the same alphabet, but it simplifies implementation.\u003c/p\u003e\n\u003cp\u003eIn order to build WT, perform the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAssign top layer (Layer 2) of the sequence to the root node of WT.\u003c/li\u003e\n\u003cli\u003eFor each symbol of Layer 1 put it to the subsequence of the node with the cardinal label matched with corresponding symbol from the same position in the Layer 2.\u003c/li\u003e\n\u003cli\u003eRepeat step (2) for symbols at Layer 0 but now select appropriate child at Level 1 of the tree, using pair of symbols from the same positions at Layer 1 and Layer 2 of the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the figure for details. Symbols in the WT and the sequence are colored to simplify understanding of symbols\' distribution.\u003c/p\u003e\n\u003cp\u003eNote that for an alphabet with K symbols, multiary WT has up to K leafs that can be very significant number. But for most practical cases this number is moderate. The larger number of distinct symbols in the sequence, the bigger tree is. Dynamic LOUDS with associated cardinality labels is used to code structure of WT.\u003c/p\u003e\n\u003cp\u003eAlso, it is not necessary to keep empty nodes in the tree (they are shown in gray on the figure).\u003c/p\u003e\n\u003ch2 id="insertion-and-access"\u003eInsertion and Access\u003c/h2\u003e\n\u003cp\u003eTo insert a value into WT we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the path from root to leaf for inserted values, insert tree nodes if necessary;\u003c/li\u003e\n\u003cli\u003efind correct position in the node\u0026rsquo;s subsequence to insert current symbol.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe path in the wavelet tree is determined by \u0026ldquo;layered\u0026rdquo; representation if inserted symbol. Computation of insertion position is a bit tricky.\u003c/p\u003e\n\u003cp\u003eLet we insert the value of 37 into position 7. Layered representation of 37 is \u0026ldquo;211\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLevel 2. Insert \u0026ldquo;2\u0026rdquo; into position 7 of root node\u0026rsquo;s subsequence of WT.\u003c/li\u003e\n\u003cli\u003eLevel 1. Next child is \u0026ldquo;2\u0026rdquo;. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(7 + 1, 2) - 1 = rank(8, 2) - 1 = 1\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003cli\u003eLevel 0. Next child is \u0026ldquo;1\u0026rdquo;, create it. Repeat the procedure for Layer 1. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(1 + 1, 1) - 1 = rank(2, 1) - 1 = 0\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_insert.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAccess is similar, but instead of to insert a symbol to a node\u0026rsquo;s subsequence, take the symbol form it and use it to select next child.\u003c/p\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute rank(position, symbol) we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the leaf in WT for the symbol;\u003c/li\u003e\n\u003cli\u003efind position in the leaf to compute the final rank.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_rank.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eComputation of select(rank, symbol) is different. If rank() is computed top-down, then select() is computed bottom-up.\u003c/p\u003e\n\u003cp\u003eLet we need to select position of the 2nd 3 in the original sequence. Layered representation for 3 is \u0026ldquo;003\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFind the leaf in WT for the given symbol.\u003c/li\u003e\n\u003cli\u003ePerform \u003ccode\u003eselect(2, \u0026quot;3\u0026quot;) = Pos0\u003c/code\u003e on the leaf\u0026rsquo;s sequence.\u003c/li\u003e\n\u003cli\u003eWalk up to parent for his leaf. Perform \u003ccode\u003eselect(Pos0 + 1, \u0026quot;0\u0026quot;) = Pos1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eStep up to the parent node (the root). Perform \u003ccode\u003eselect(Pos1 + 1, \u0026quot;0\u0026quot;) = Pos\u003c/code\u003e. This is the final result.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_select.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="implementations"\u003eImplementations\u003c/h2\u003e\n\u003cp\u003eIn Memoria, Multiary wavelet tree consists of four distinct data structures.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLOUDS to store wavelet tree structure.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of cardinal labels for tree nodes.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of sizes for tree node\u0026rsquo;s sub-sequences.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of node\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first three structures are implemented as single \u003ca href="/docs/data-zoo/louds-tree"\u003eLabeled Tree\u003c/a\u003e with two labels. The first one is cardinality of the node in its parent. The second one is size of node\u0026rsquo;s subsequence.\u003c/p\u003e\n\u003cp\u003eThe fourth data structure is a separate \u003ca href="/docs/data-zoo/searchable-seq"\u003eSearchable Sequence\u003c/a\u003e for small sized alphabets.\u003c/p\u003e\n\u003cp\u003eMemoria has two different implementations of WT algorithm. The first one is dynamic WT that provides access/insert/select/rank operations performing in O(log \u003cem\u003eN\u003c/em\u003e) time.\u003c/p\u003e\n\u003cp\u003eThe second one has all those four data structures implemented with \u003ca href="Memory_Allocation"\u003ePacked Allocator\u003c/a\u003e placed in a single raw memory block of limited size. This implementation has fast access/select/rank operations but slow insert operation with O(\u003cem\u003eN\u003c/em\u003e) time complexity.\u003c/p\u003e\n\u003cp\u003eCurrently Memoria provides only 256-ary wavelet tree for 32-bit sequences. Other configurations will be provided in upcoming releases of the framework.\u003c/p\u003e\n'}).add({id:15,href:"/docs/data-zoo/packed-allocator/",title:"Packed Allocator",description:"",content:'\u003cp\u003eWe need to place several, possibly resizable (see below), objects into a single contiguous memory block of limited size. Classical malloc-like memory allocator is not suitable here because it doesn\u0026rsquo;t work well with resizable objects. Especially if they are allocated in a relatively small memory block. To maintain resizability efficiently we have to relocate other objects if some object is resized.\u003c/p\u003e\n\u003ch2 id="resizable-object-pattern"\u003eResizable Object Pattern\u003c/h2\u003e\n\u003cp\u003eResizable object is an object that has unbounded size. Usually it has the following pattern:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  int object_size_;\n  char[] variable_size_data_;\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, the last member \u003ccode\u003echar[] variable_size_data_\u003c/code\u003e is an unbounded array. For any object \u003ccode\u003esizeof()\u003c/code\u003e does\nnot count the last member if it is unbounded array. For example, \u003ccode\u003esizeof(ResizableObject) == sizeof(int)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf a custom allocator allocates more memory than necessary for resizable objects, this memory can be accessed via the last member. Usually such an object should know length of the memory block it is mapper to. But it can also be stored by memory allocator.\u003c/p\u003e\n\u003ch2 id="linear-contiguous-allocator"\u003eLinear Contiguous Allocator\u003c/h2\u003e\n\u003cp\u003eThe idea is to place all abjects contiguously in a memory block and shift them if some object is resized. Separate layout dictionary is used to locate objects in a block. Objects are accessed by their indexes, not by direct addresses in the block.\u003c/p\u003e\n\u003cp\u003eLayout dictionary is an ordered list of block offsets. If dictionary if large, \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e can be used to speedup access.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_contiguous_allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLayout dictionary is placed into the same memory block as object\u0026rsquo;s data.\u003c/p\u003e\n\u003ch2 id="allocator-aware-objects"\u003eAllocator-Aware Objects\u003c/h2\u003e\n\u003cp\u003eThe main property of resizable objects is that their size can be changed dynamically, that requires interaction with allocator. Consider the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  //...\n  int insert(int index, int value); // enlarge object\n  int remove(int index);            // shrink object\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have two methods affecting object\u0026rsquo;s size. And it is good to incapsulate required interaction with allocator within resizable objects. But the problem is we don\u0026rsquo;t want to store raw memory pointers to the allocator withing objects for various reasons. The main reason is we want allocators to be relocatable. If the allocator itself is relocated, all pointers have to be updated.\u003c/p\u003e\n\u003cp\u003eThe idea is to put allocator and objects into a single addressable memory block. In this case we can get address of allocator having only address of the object and its relative offset in the memory block. Let\u0026rsquo;s consider \u003ccode\u003ePackedAllocatable\u003c/code\u003e base class for any allocator-aware resizable object:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\nclass PackedAllocator;\n\nclass PackedAllocatable {\n  typedef PackedAllocator Allocator;\n  int allocator_offset_; // resizable object offset in the allocator\'s memory block\npublic:\n  Allocator* allocator() {\n    if (allocator_offset_ \u0026gt; 0) {\n      return reinterpret_cast\u0026lt;Allocator*\u0026gt;(reinterpret_cast\u0026lt;char*\u0026gt;(this) - allocator_offset_);\n    }\n    else return nullptr;\n  }\n\n  // Other methods go here...\n};\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the following figure how it may look like:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_brief.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn Memoria we call such allocator \u003cem\u003epacked\u003c/em\u003e, because it packs all objects and itself into single self-sufficient memory block that can be relocated or serialized. That doesn\u0026rsquo;t affect relative positions of objects within the memory block.\u003c/p\u003e\n\u003cp\u003eEach allocator-aware resizable object must derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e class that maintains relative offset of an object to the allocator. The following figure explains it in greater details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we have four resizable objects with sizes 4, 7, 11, and 9 respectively. Each object maintains its relative offset in the memory block, that is converted to a pointer to the allocator.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s increase object #1 by 8 units:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full_enlarged.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWe need to perform the following operations for objects #2 and #3.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eShift objects\' data right by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease objects\' offsets in layout dictionary by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease \u003ccode\u003ePackedAllocatable::allocator_offset_\u003c/code\u003e by 8 units.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNot all abjects are resizable, they don\u0026rsquo;t need to maintain a pointer to the allocator. But now allocator has to know which objects are instances of \u003ccode\u003ePackedAllocatable\u003c/code\u003e and which are not to update pointers properly.\u003c/p\u003e\n\u003ch2 id="recursive-allocator"\u003eRecursive Allocator\u003c/h2\u003e\n\u003cp\u003eThe next idea is to define packed allocator recursively by deriving from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case it is possible to embed any packed allocator into another allocator just as any resizable object.\u003c/p\u003e\n\u003ch2 id="packedallocator-api"\u003ePackedAllocator API\u003c/h2\u003e\n\u003cp\u003eSo we have:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ePackedAllocator\u003c/code\u003e managing resizable memory regions withing contiguous relocatable memory block.\u003c/li\u003e\n\u003cli\u003eAllocator-aware objects derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003ePacked allocator also derives from \u003ccode\u003ePackedAllocatable\u003c/code\u003e that mean it can be embedded into another packed allocator.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe last idea is to use \u003ccode\u003ePackedAllocator\u003c/code\u003e as a base class for resizable objects. That enables them to have more than one resizable section.\u003c/p\u003e\n\u003cp\u003eThe following code snippet explains basic PackedAllocator API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n\n  template \u0026lt;typename T\u0026gt;\n  T* get(int i);             //returns address of i-th memory block, i = 0...\n  template \u0026lt;typename T\u0026gt;\n  const T* get(int i) const; //returns address of i-th memory block, i = 0...\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i, int size); // allocates size bytes for i-th memory block and initializes\n                                // it properly if T derives from PackedAllocatable.\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i); // Allocates space for i-th memory block and initializes\n                      // it properly if T derives from PackedAllocatable. \n                      // Size of the block is got via T::empty_size() if T derives \n                      // form PackedAllocatable and sizeof(T) otherwise.\n\n  // resize memory block at address \'addr\', resize parent allocator if necessary\n  // throws PackedOOMException if top-most allocator runs out of memory\n  void resize(const void* addr, int new_size); \n\n  //the same as above but returns size of i-th block in bytes\n  void resize(int i, int new_size);\n\n  int size(int i) const; \n\n  void init(int entries); // initializes allocator with the specified number of \n                          // empty blocks (entries)\n \n  // returns size in bytes of empty allocator having specified number of entries\n  static int empty_size(int entries); \n\n  static int round(int size); // round size to alignment blocks. e.g. if alignment block is 8 bytes\n                              // round(1) = 8; round(12) = 16\n\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo interact with PackedAllocator each allocatable object provides two methods. One of them is for initialization, and another one is to query object size:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass SimpleResizableObject: public PackedAllocatable {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\nclass AdvancedResizableObject: public PackedAllocator {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that resizable objects must be \u003ca href="http://en.cppreference.com/w/cpp/types/is_trivial"\u003etrivial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote also that Memoria doesn\u0026rsquo;t currently use placement \u003ca href="http://en.cppreference.com/w/cpp/memory/new/operator_new"\u003enew\u003c/a\u003e и \u003ca href="http://en.cppreference.com/w/cpp/language/delete"\u003edelete\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// There is no way to specify custom allocator and other parameters here, only address \n// of the object to delete. PackedAllocator does not allow to get allocator address given\n// only address of a block it manages. so it provides explicit API for allocation \n// and deallocation.\n\nvoid operator delete (void *ptr); // placement delete operator\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee this \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/include/memoria/core/packed/tools/packed_allocator.hpp"\u003esource\u003c/a\u003e for more details about PackedAllocator implementation.\u003c/p\u003e\n\u003ch2 id="resizable-object-example-seachable-sequence"\u003eResizable Object Example: Seachable Sequence\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s consider a relatively simple but live example: \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable sequence\u003c/a\u003e. It is a sequence of symbols providing rank and select operations performed in logarithmic time. To provide such time complexity, data structure uses additional index that have to be quite complex for large alphabets. Let\u0026rsquo;s say that in general case the index is compressed and we don\u0026rsquo;t know it\u0026rsquo;s actual size ahead of time. The size of index is a variable value depending of a sequence content.\u003c/p\u003e\n\u003cp\u003eSo the sequence has at least two resizable blocks: index, and symbols.\u003c/p\u003e\n\u003cp\u003eBelow there is a code snipped explaining how update operations on the object interact with its allocator(s).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\ntemplate \u0026lt;int BitsPerSymbol\u0026gt;\nclass SearchableSequence: public PackedAllocator {\n  typedef PackedAllocator                              Base;\n  typedef SearchableSequence\u0026lt;BitsPerSymbol\u0026gt;            MyType;\n\n  typedef unsigned int                                 Symbol;\n\n  // Instantiate index only if the sequence is larger\n  // than this value.\n  static const int IndexSizeThreshold                  = 64;\n\n  // no members should be declared here\n  // use Metadata class instead\npublic:\n  class Metadata {\n    int size_;\n    int\u0026amp; size() {return size_;} \n    const int\u0026amp; size() const {return size_;} \n  };\n\n  class Index: public PackedAllocator {\n    // index data structure for this SearchableSequence\n  };\n\n  // This enum codes memory block indexes.\n  enum {\n    METADATA, // Metadata block\n    INDEX,    // Indexes block\n    SYMBOLS   // Symbols block\n  };\n\n  // returns address of Metadata object\n  Metadata* metadata() {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n  const Metadata* metadata() const {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n\n  // returns address of Index object\n  Index* index() {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n  const Index* index() const {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n\n  // returns address of symbols block\n  Symbol* symbols() {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  const Symbol* symbols() const {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  \n  // returns size in bytes of empty sequence. this method is used by \n  // PackedAllocator::allocateEmpty(int) to get object\'s default size\n  static int empty_size() \n  {\n    int allocator_size = Base::empty_size(); // size of allocator itself\n    int metadata_size  = Base::round(sizeof(Metadata)); // size of metadata block\n    int index_size     = 0; // index is empty for empty sequence\n    int symbols_size   = 0; // symbols block is also empty for empty sequence\n\n    return allocator_size + metadata_size + index_size + symbols_size;\n  }\n\n  void init() \n  {\n    Base::init(3); // the object has three resizable sections.\n    \n    // Allocate metadata block and initialize it\n    Base::template allocate\u0026lt;Metadata\u0026gt;(METADATA);\n\n    // Allocate empty block for index. Do not initialize it\n    Base::template allocate\u0026lt;Index\u0026gt;(INDEX, 0);\n    \n    // Allocate empty block for symbols. \n    Base::template allocate\u0026lt;Symbol\u0026gt;(SYMBOLS, 0);    \n  }\n\n  // change the value of idx-th symbol\n  void setSymbol(int idx, int symbol);\n\n  // insert new symbol at the specified position\n  int insert(int idx, int symbol) \n  {\n    enlarge(1);              // enalrge SYMBOLS block\n    insertSpace(idx, 1);     // shift symbols\n    setSymbol(idx, symbol);  // set new symbol value\n\n    reindex();               // update search index\n  }\n\n  int size() const \n  {\n    return metadata()-\u0026gt;size();\n  }\n\n  // update index for the searchable sequence\n  void reindex() \n  {\n     // check if the sequence if large enough to have index\n     if (size() \u0026gt; IndexSizeThreshold) \n     {\n       if (Base::size(INDEX) == 0)\n       {\n         Base::template allocate\u0026lt;Index\u0026gt;(0); // create empty index if it doesn\'t exist\n       }\n\n       Index* index = this-\u0026gt;index();\n\n       // compute index size for given symbols and resize the index.\n       index-\u0026gt;resize_for(this-\u0026gt;symbols(), size());\n\n       // rebuild the index\n       // note that any resize operation invalidates pointers to blocks\n       // going after the resized one.\n       index-\u0026gt;update(this-\u0026gt;symbols(), size());\n     }\n     else {\n       // the sequence if not full enough to have the index,\n       // free it.\n       Base::resize(INDEX, 0); // free index block\n     }\n  }\n  \nprivate:\n  // insert empty space into symbols\'s data block,\n  // shift symbols in the range [idx, size()) \'length\' positions right.\n  void insertSpace(int idx, int length);\n\n  // returns symbols block size for specified number of symbols\n  static int symbols_size(int symbols);\n\n  //enlarge symbols block by \'items\' elements.\n  void enlarge(int items) \n  {\n    // get new size for SYMBOLS block\n    int new_symbols_size = MyType::symbols_size(size() + items)\n    \n    // enlarge SYMBOLS block\n    Base::resize(SYMBOLS, new_symbols_size);\n  }\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis pattern is used intensively for packed data structures in Memoria.\u003c/p\u003e\n'}).add({id:16,href:"/docs/data-zoo/associative-memory-1/",title:"Associative Memory (Part 1)",description:null,content:'\u003ch2 id="what-is-associative-memory"\u003eWhat is Associative Memory\u003c/h2\u003e\n\u003cp\u003eAssociative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of \u003cem\u003eMemory\u003c/em\u003e. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of \u003cem\u003ebits\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="motivating-example-rdf"\u003eMotivating Example: RDF\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href="https://en.wikipedia.org/wiki/Resource_Description_Framework"\u003eResource Description Framework\u003c/a\u003e data is modelled with a special form of a labelled graph, consisting from \u003cem\u003efacts\u003c/em\u003e (represented as URIs) and \u003cem\u003etriples\u003c/em\u003e in a form of $(Subject, Predicate, Object)$ linking various facts together. Logical representation of this \u003cem\u003esemantic graph\u003c/em\u003e is a table, enumerating all the triples in the graph. The main operation on the graph is \u003cem\u003epattern matching\u003c/em\u003e using SQL-like query language \u003ca href="https://en.wikipedia.org/wiki/SPARQL"\u003eSPARQL\u003c/a\u003e. Another common mode of operations over graphs is traversal, but this mode is secondary for semantic graphs. Pattern-matching in semantic graphs is based of \u003cem\u003eself-joins\u003c/em\u003e over the triple tables:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="triples.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBecause of the flexibility of SQPRQL, self-joins can be performed on any combination of Subject, Predicate and Objects, and the join on objects is the main reason why basic representation of graphs for RDF is a relational table. And, if we want fast execution, this table has to be properly indexed.\u003c/p\u003e\n\u003cp\u003eThe simplest way to provide indexing over a triple table is to sort it in some order, but ordered table only allows ordered \u003cem\u003ecomposite\u003c/em\u003e keys. If a table is ordered like (S, P, O), that means $(Subject, Predicate, Object)$, \u003cem\u003ethen\u003c/em\u003e we can search first by Subject, \u003cem\u003ethen\u003c/em\u003e buy Predicate, and only then by Object. But not vise versa. If we want to search by an Object first, we need another table ordered by object: (O, X, Y). To be able to search in any order we need all possible permutations of S, P and O: it\u0026rsquo;s $3! = 6$ tables.\u003c/p\u003e\n\u003cp\u003eSo, fully indexed RDF triple store will need at least 6 triple tables. How many is it?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times the size of the original single-table store (not counting URIs and Objects if they are stored separately).\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times slower insertions if they are not paralleled. Of course we can make many insertions in parallel, but it\u0026rsquo;s \u003cem\u003e6 times more energy\u003c/em\u003e anyway.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModern triple stores model semantic graphs with \u003cem\u003equads\u003c/em\u003e, adding \u003cem\u003eGraph ID\u003c/em\u003e to a triple, so nodes can link to entire graphs, no just other nodes (self-referentiality). In addition to, higher-dimensional tables ($D \u0026gt; 4$) can also be provided to speedup certain queries. And, in general case, if we have $D$-dimensional data table, we need \u003cem\u003eup to\u003c/em\u003e $D!$ orderings of this table. That is 24 for $D=4$ and grows faster than an exponent.\u003c/p\u003e\n\u003cp\u003eSorting relational tables does not scale at all for higher-order graphs ($D \u0026gt; 3$), but we can do better.\u003c/p\u003e\n\u003ch2 id="definition-of-associative-memory"\u003eDefinition of Associative Memory\u003c/h2\u003e\n\u003cp\u003eSo, without loss of generality, associative memory is a $D$-dimensional relation $R$ with \u003cem\u003eset\u003c/em\u003e semantics, over integer numbers drawn from some finite set (domain). The main operation on the memory is \u003cem\u003elookup\u003c/em\u003e that can be performed using arbitrary number of dimensions, specifying \u003cem\u003ematch\u003c/em\u003e, \u003cem\u003erange\u003c/em\u003e or \u003cem\u003epoint\u003c/em\u003e lookup, or any combination of for any number of dimensions. \u003cem\u003eRecall\u003c/em\u003e is the result of \u003cem\u003elookup\u003c/em\u003e operation, and is enumeration of all entries in $R$ matching the query.\u003c/p\u003e\n\u003cp\u003eAssociative memory can be either \u003cem\u003estatic\u003c/em\u003e, if only lookups are allowed. Or \u003cem\u003edynamic\u003c/em\u003e, if it supports insert, update and delete operation for individual elements (Update operation can be reduced to delete + insert).\u003c/p\u003e\n\u003ch2 id="multiscale-decomposition"\u003eMultiscale Decomposition\u003c/h2\u003e\n\u003cp\u003eLet we have a $D$-dimensional relation $R = \\lbrace {r_0, r_1, r_2, \u0026hellip;, r_{N-1}}\\rbrace$, representing a $set$, where $r_i = (c_0, c_1, \u0026hellip;, c_{D-1})_i$ - а $D$-dimensional tuple and $N$ is a \u0026lsquo;size\u0026rsquo; of the table (number of elements in the set). $c_{i,j}$ is a table\u0026rsquo;s cell value from row $i$ and dimension (column) $j$. Each cell value $c_{i,j}$ has a domain of $H$ bits.\u003c/p\u003e\n\u003cp\u003eExample: A set of 8x8 images with 8 bits per pixel can be represented with 64-dimensional relation with $H = 8$. Maximal number of images in a set is $N \u0026lt;= 8^{64} = 2^{192}$. Given such table we can easily define an associative memory reducing content-addressable lookup to linear table scans and bit manipulations (time complexity is $O(N)$). And, actually, this is how it\u0026rsquo;s implemented for approximate nearest neighbour search on massively-parallel hardware. Parallel linear scan is fast, but it\u0026rsquo;s not scalable (fast memory is expensive) and it\u0026rsquo;s not energy-efficient.\u003c/p\u003e\n\u003cp\u003eFortunately, we can transform $O(N)$ into $O(P H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e, where $1 \u0026lt;= P \u0026lt;= 2^D$ \u0026ndash; average number of nodes per \u003cstrong\u003ebucket\u003c/strong\u003e (see below), that is, thanks to the \u003ca href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality"\u003e\u0026ldquo;Blessing of Dimensionality\u0026rdquo;\u003c/a\u003e, usually tends to 1. And $M$ is a \u003cem\u003erecall size\u003c/em\u003e, number of points returned by the query over $R$.\u003c/p\u003e\n\u003cp\u003eTo perform the multiscale decomposition of relation $R$, we need to perform the following transformation for each row $r_i$:\u003c/p\u003e\n\u003cp\u003eLet $c_{ij} = S_{ij} = (s_0, s_1,s_2,\u0026hellip;,s_{H-1})_{ij}$, where $s \\in \\lbrace{0, 1}\\rbrace$ is a bit-string representation of cell value $c$. Let $s_h = B(S, h)$ \u0026ndash; $h$-th bit of string $S$.\u003c/p\u003e\n\u003cp\u003eNow, $M(r)$ is a multiscale decomposition of a row $r = (S_0, S_2, \u0026hellip;, S_{D-1})$. Informally, $M(r)$ is a bit string consisting from a concatenation of shuffling of all bits form $S_j$:\u003c/p\u003e\n\u003cp\u003e$M(r) = B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip;B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip; B(S_0, 0) B(S_1, 0) \u0026hellip; B(S_{D-1}, 0)$.\u003c/p\u003e\n\u003cp\u003eThe symbol $|$ is added to graphically separate $H$ \u003cem\u003elayers\u003c/em\u003e of the multiscale representation from each other.\u003c/p\u003e\n\u003cp\u003eExample. Let $r = (100, 110, 001)$. Then $M(r) = 111|010|001$. Note, that in some sense, $M(r)$ is producing a point on a \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003e$Z$-order curve\u003c/a\u003e for $r$. This correspondence may help in some applications.\u003c/p\u003e\n\u003cp\u003eSo, multiscale decomposition of $T = M(R)$ converts a table with $D$ columns into a table with $H$ columns, which are called \u003cem\u003elayers\u003c/em\u003e here.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s assume, that the table $T$ is sorted in a bit-lexicographic order.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="tables.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is special about table $T$ is that every column $L_j$ contains some information form each column $D_j$ from table $R$. So, by searching in a column of $T$ we can search in all columns from $R$ at once. Table $T$ itself does not provide any speedup over the sorted $R$ because the number of rows is still the same as for table $R$. But quick look at the table will show that there are many \u003cem\u003erepetitions\u003c/em\u003e. So, we can transform $T$ into a tree, by hierarchically (here, from left to right) collapsing repetitive elements in tables\' columns. Now, a \u003cstrong\u003ebucket\u003c/strong\u003e is a list of all children of the same parent node sorted lexicographically. It can be shown, that there may be \u003cem\u003eat most\u003c/em\u003e $2^D$ elements in a bucket. So, search in such data structure is $O(2^D H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. If $D$ is small, say, 16 or less, this may dramatically improve performance relative to linear scan of $R$ (even if it\u0026rsquo;s sorted). See the \u003ca href="#analysis"\u003eAnalysis\u003c/a\u003e section below for additional properties and limitations.\u003c/p\u003e\n\u003cp\u003eNote that each path from root to leaf in the tree encodes a single row in the table $T$, and after the inverse multiscale decomposition, in $R$.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s demonstrate how search works. Let we want to enumerate all rows in $R$ with $D_2$ = 0111, so $Q = (X, Y, 0111)$, where $X$ and $Y$ are \u003cem\u003eplacehoders\u003c/em\u003e. First, we need a multiscale representation of $Q$, $M(Q) = xy0|xy1|xy1|xy1$. Now, we need to traverse the tree from root to leafs, according to this pattern:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="search.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHaving the multiscale query encoding, the tree traversal is straightforward. We are visiting sub-trees in-order, providing that current pattern matches the node\u0026rsquo;s label. Visiting the leaf (+ its label is matched) means full match. The excess number of nodes visited by a range query is called \u003cem\u003eoverwork\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this example, the selectivity of the query is pretty good. All leafs matching the query are being visited (visited nodes are drawn in the dotted-green line style). Note the nodes marked with (*). The traversal process visited some nodes \u003cem\u003enot leading to the full match\u003c/em\u003e. This is why type complexity estimation for this data structure is logarithmic \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. Its performance depends on how data is distributed in the tree.\u003c/p\u003e\n\u003ch2 id="using-louds-for-the-tree"\u003eUsing LOUDS for the Tree\u003c/h2\u003e\n\u003cp\u003eTable $T$ has the same size (in bits) as the table $R$, but the tree, if implemented using pointer-based structure, will add a lot to the table representation. Fortunately, we can use \u003ca href="/docs/data-zoo/louds-tree"\u003eLOUDS Tree\u003c/a\u003e to encode the tree. LOUDS tree has very small memory footprint, only 2 bits per node (+ a small overhead, like 10%, for rank/select indices). Search tree size estimation is at most $NH$ nodes, so the tree itself will take at most size of two columns of the table $R$ ($2NH$ + small overhead). In most cases LOUDS-encoded $T$ will take less space that original table $R$, including auxiliary data structures.\u003c/p\u003e\n\u003cp\u003eWhat is the most important for LOUDS Tree is that it\u0026rsquo;s structured in memory in a linear order. So, if for some reason a spatial tree traversal degrades into linear search, the tree will be pretty good at this. We just need to read many layers of the tree in parallel. Such I/O operations can be efficiently prefetched.\u003c/p\u003e\n\u003ch2 id="analysis"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eIt can be shown that the tree is similar to a trie-based Quad Tree (for high dimensions), so many expected (average-case) and worst-case estimations also apply. In worst case, for high-dimensional trees, traversal degenerates into linear a search. Fortunately for LOUDS, it\u0026rsquo;s both memory-efficient for linear search \u003cem\u003eand\u003c/em\u003e for tree traversal. But on average, overwork is moderate, if doesn\u0026rsquo;t even tend to zero, so queries should perform pretty well.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s return to RDF triple stores and compare things to each other. Let\u0026rsquo;s assume that $D = 3$ (number of dimensions) and $H = 32$ (resource identifier\u0026rsquo;s size or search tree depth). So, in case of insertion of a triple, we have to perform 6 insertions into 6 triple tables (for each ordering) and 32 insertions in case of the tree (into each tree level). Reading is also slower: one lookup in a sorted table vs 32 lookups in the tree. It looks unreasonable to switch from triple tables (worst case logarithmic) to search tree (average case logarithmic), unless we are limited in memory and want to fit as many triples as possible into the available amount. But things start changing when we go into higher dimension ($D \u0026gt; 3$) and need more indices to speedup our queries.\u003c/p\u003e\n\u003cp\u003eSo far\u0026hellip;\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePoint-like operations (to check if some row exists in the relation $R$) will take $O(D log(N))$ time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRange search in the quad trees is logarithmic on average, but it\u0026rsquo;s relatively easy to build a worst-case example, when performance degrades to a linear scan. For example, if $D = 64$, maximal bucket size is $2^{64}$, that is much larger than any practical $N$ (number of entries in $R$). Unless the data is distributed uniformly among \u003cem\u003edifferent levels\u003c/em\u003e of the tree, we will end up having a few but very big buckets. So, special care must be taken on how we map our high-level data to dimensions of the tree. Random mapping is usually a safe bet, but always the best choice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the search tree \u0026ldquo;Blessing of Dimensionality\u0026rdquo; is fighting with \u0026ldquo;Curse of Dimensionality\u0026rdquo;, it\u0026rsquo;s kind of $0 \\cdot \\infty$. In higher dimensions data tends to be extremely \u003cem\u003esparse\u003c/em\u003e because volume size grows exponentially with number of dimensions. So, normally, even in high dimensions, buckets will tend to have small number of elements. The bigger the number \u0026ndash; the better, because it improves data compression and speeds up queries. But beware of the worst case, when the tree has one big bucket that all queries are visiting. It has also been observed for similar K-d trees, that with higher number of dimensions, \u003cem\u003eoverwork\u003c/em\u003e also tens to increase (the blessing vs curse situation, $0 \\cdot \\infty$, who wins?).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn case if LOUDS tree is dynamic and implemented using B-tree, \u003cem\u003einsert\u003c/em\u003e, \u003cem\u003eupdate\u003c/em\u003e and \u003cem\u003edelete\u003c/em\u003e operations to the search tree have $O(H log(N))$ time complexity for point-like updates and $O(H(log(N) + M))$ for batch updates of size $M$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe data structure representation in memory is very compact. It\u0026rsquo;s the size of original table $R$ + (up to) the size of two columns from $R$ for LOUDS tree + 5-10% of the tree to auxiliary data structures. Overhead of the tree is constant and is amortizing with higher number of dimensions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEven if we work with high-dimensional data, and we are losing to the curse of dimensionality, it\u0026rsquo;s possible to perform approximate queries. Many applications where high-dimensional data analysis is required, like AI, are essentially approximate. LOUDS tree allows to compactly and efficiently remember additional bits of information with each node, to facilitate approximate queries (if necessary).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS tree-based Associative memory seems to be impractical specifically for RDF triple stores, but if hardware accelerated, can be cost-effective at scale, providing also many other benefits, not just memory savings (which are huge for higher dimensions). The bottleneck is on the update operations, where insertion and deletion may require tens of B-tree updates. Fortunately, this operation is well-parallelizable so we can use thousands of small RISC-V cores equipped with special command for direct and energy-efficient implementation of essential operations (partial/prefix sums, rank and select). An array or cluster of such cores can even be embedded into \u003ca href="/subprojects/smart-storage"\u003estorage memory controller\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAdvanced data structures like LOUDS-based associative memory, considered to be impractical in the past, relative to more traditional things, like sorted tables and pointer-based data structures (trees, lists, graphs, \u0026hellip;) for main memory. But progress in computer hardware makes task-specific hardware accelerator a much more viable options, opening the road to completely new applications.\u003c/p\u003e\n\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-2"\u003enext post\u003c/a\u003e it will be shown how LOUDD-backed associative memory can be used for generic function approximation and inversion.\u003c/p\u003e\n'}).add({id:17,href:"/docs/data-zoo/associative-memory-2/",title:"Associative Memory (Part 2)",description:"",content:'\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-1"\u003eprevious part\u003c/a\u003e we saw how LOUDS tree can be used for generic compact and efficient associative associative memory over arbitrary number of dimensions. In this part we will see how LOUDS trees can be used for function approximation and inversion.\u003c/p\u003e\n\u003ch2 id="definitions"\u003eDefinitions\u003c/h2\u003e\n\u003cp\u003eLet $[0, 1] \\in R$ is the domain and range we are operating on. $D$ is a number of dimensions of our space, and for the sake of visualizability, $D = 2$.\u003c/p\u003e\n\u003cp\u003eLet $\\vec{X}$ is a vector encoding a point in $[0, 1]^D$, $\\vec{X} = \u0026lt;x_0, x_1, \u0026hellip;, x_{D-1}\u0026gt;$. Let $str(x [, h])$ is a function converting a real number from $[0, 1]$ into a binary string by taking a binary representation of a real number to a form like \u0026ldquo;$0.0010111010\u0026hellip;$\u0026rdquo;, and removing leading \u0026ldquo;$0.$\u0026rdquo; and trailing \u0026ldquo;$0\u0026hellip;$\u0026rdquo;. so, $str(0.181640625) = 001011101$. If $h$ argument is specified for $str(\\cdot, \\cdot)$, then resulting string is trimmed to $h$ binary digits, if it\u0026rsquo;s longer than that.\u003c/p\u003e\n\u003cp\u003eLet $len(x)$ is a number of digits in the result of $str(x)$. Note that $len(\\pi / 10) = \\infty$, so irrational numbers are literally infinite in this notation. Let $H$ is a maximal \u003cem\u003edepth\u003c/em\u003e of data, and there is some \u003cem\u003eimplicitly assumed\u003c/em\u003e arbitrary value for $h$, like 32 or 64, or even 128. So we can work with \u003cem\u003eapproximations\u003c/em\u003e of irrational and transcendent numbers, or with long rational numbers in a same way and without loss of generality.\u003c/p\u003e\n\u003cp\u003eLet $len(\\vec{X}) = max_{\\substack{i \\in \\lbrace 0,\u0026hellip;,D-1 \\rbrace }}(len(x_i))$.\u003c/p\u003e\n\u003cp\u003eLet $str(\\vec{X}) = (str(x_0),\u0026hellip;, str(x_{D-1}))$ is a string representation (a tuple) of $\\vec{X}$. And let we assume, elements of the tuple are implicitly extended with \u0026lsquo;$0$\u0026rsquo; from the right, if their length is less than $len(\\vec{X})$. In other words, all elements (binary string) of a tuple are implicitly of the same length.\u003c/p\u003e\n\u003cp\u003eLet $str(\\lbrace \\vec{X_0}, \\vec{X_1}, \u0026hellip; \\rbrace) = \\lbrace str(\\vec{X_0}), str(\\vec{X_1}), \u0026hellip; \\rbrace$. String representation of set of vectors is a set of string representation of individual vectors.\u003c/p\u003e\n\u003cp\u003eLet $M(\\vec{X}) = M(str(\\vec{X}))$ is a \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003emultiscale transformation\u003c/a\u003e of binary string representation of $\\vec{X}$. Informally, to compute $M(\\vec{X})$ we need to take all strings from its string representation (the tuple of binary strings) and produce another string of length $len(\\vec{X})  D$ by concatenating interleaved bits from each binary string in the tuple.\u003c/p\u003e\n\u003cp\u003eExample. Let $H = 3$ and $D = 2$. $\\vec{X} = \u0026lt;0.625, 0.25\u0026gt;$, $str(\\vec{X}) = (101, 01)$ and $M(\\vec{X}) = 10|01|10$. Note that signs $|$ are added to separate $H$ \u003cem\u003epath elements\u003c/em\u003e in the recording, they are here for the sake of visualization and are not a part of the representation. The string $10|01|10$ is also called \u003cem\u003epath expression\u003c/em\u003e because it\u0026rsquo;s a unique path in the multidimensional space decomposition \u003cem\u003eencoding\u003c/em\u003e the position of $\\vec{X}$ in this decomposition. Visually:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiscale1.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that the order in which \u003cem\u003epath elements\u003c/em\u003e of a path expression enumerate the volume is \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003eZ-order\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGiven a \u003cem\u003eset\u003c/em\u003e of $D$-dimensional vectors $\\lbrace \\vec{X} \\rbrace$, it\u0026rsquo;s multiscale transformation can be represented as a $D$-dimensional \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e. Such quad tree can be represented with a \u003ca href="/docs/data-zoo/louds-tree/#cardinal-trees"\u003ecardinal LOUDS tree\u003c/a\u003e of degree $2^D$. Here, implicit parameter $H$ is a \u003cem\u003emaximal height\u003c/em\u003e of the Quad Tree.\u003c/p\u003e\n\u003ch2 id="basic-asymptotic-complexity"\u003eBasic Asymptotic Complexity\u003c/h2\u003e\n\u003cp\u003eGiven that $N = |\\lbrace \\vec{X} \\rbrace|$, and given that LOUDS tree is dynamic (represented internally as a b-tree), the following complexity estimations apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInsertion and deletion\u003c/strong\u003e of a point is $O(H log(N))$. Update is semantically not defined because it\u0026rsquo;s a \u003cem\u003eset\u003c/em\u003e. Batch updates in Z-order are $O(H (log(N) + B))$, where $B$ is a batch size. Otherwise can be slightly worse, up to $O(H log(N) B)$ in the worst case.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint lookup\u003c/strong\u003e (membership query) is $O(D H log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRange and projection\u003c/strong\u003e queries are $O(D H log(N) + M)$ \u003cem\u003eon average\u003c/em\u003e, where $M$ is a recall size (number of vectors matching the query). In worst case tree traversal degrades to the linear scan of the entire set of vectors.\u003c/li\u003e\n\u003cli\u003eThe data structure is \u003cstrong\u003espace efficient\u003c/strong\u003e. 2 bits per LOUDS tree node + \u003cem\u003ecompressed bitmap\u003c/em\u003e of cardinal labels. For most usage scenarios, space complexity will be within \u003cstrong\u003e2x the raw bit size\u003c/strong\u003e of $str(\\lbrace \\vec{X} \\rbrace)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="functions-approximation-using-quad-trees"\u003eFunctions Approximation Using Quad Trees\u003c/h2\u003e\n\u003cp\u003eLet we have some function $y = f(x)$, and we also have the graph of this function on $[0, 1]^2$. If function $f(\\cdot)$ is elementary, or we have another way to compute it, it\u0026rsquo;s computable (for us). What if we have $f(\\cdot)$, but we want to compute inverse function: $x = f^{-1}(y)$? With compact quad trees we can \u003cem\u003eapproximate\u003c/em\u003e both functions out of the same \u003cem\u003efunction graph\u003c/em\u003e using compact quad trees:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="function.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere the tree has four layers shown upside down (drawing more detailed layers above less detailed ones). And if we want to compute $f(a)$, where $str(a) = a_3a_2a_1a_0$, the path expression will be $a_3x|a_2x|a_1x|a_0x$, where $x$ here is a \u003cem\u003eplaceholder sign\u003c/em\u003e. Now we need to traverse the tree as it defined in \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003eprevious part\u003c/a\u003e. If there is a point for $a$ of a function graph, it will be found in a logarithmic expected time. The path expression for $f^{-1}(b)$ will be $b_3x|b_2x|b_1x|b_0x$.\u003c/p\u003e\n\u003cp\u003eNote that compressed cardinal LOUDS tree will use less than 4 bits (+ some % of auxiliary data) \u003cem\u003eper square\u003c/em\u003e on the graph above. Sol, looking into this graph we already can say something specific about what will be the cost of approximation, depending on required precision (maximal $H$).\u003c/p\u003e\n\u003ch2 id="function-compression"\u003eFunction Compression\u003c/h2\u003e\n\u003cp\u003eLet we have a function that checks if some point is inside some ellipse:\u003c/p\u003e\n$$\ny = f(x_1, x_2): \\begin{cases} \n    1 \u0026\\text{if } (x_1, x_2) \\text{ is inside the the ellipse,} \\\\ \n    0 \u0026\\text{if it\'s outside.} \n\\end{cases}\n$$\n\u003cp\u003eThis function defines some \u003cem\u003earea\u003c/em\u003e on the graph. Let $N$ is the number of \u0026ldquo;pixels\u0026rdquo; we need to define the function $f(x,y)$ on a graph. Then, using compressed quad trees, we can do it with ${O(\\sqrt{N})}$ bits \u003cem\u003eon average\u003c/em\u003e for 2D space :\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="region.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe square root here is because we need \u0026ldquo;detailed\u0026rdquo; only for the border of the area, while \u0026ldquo;in-lands\u0026rdquo; needs much lower resolution.\u003c/p\u003e\n\u003ch2 id="blessing-of-dimensionality-vs-curse-of-dimensionality"\u003eBlessing of Dimensionality vs Curse of Dimensionality\u003c/h2\u003e\n\u003cp\u003eLet we have 2D space and we have a tree encoding the following structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="corners.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThere are four points in each corner of the coordinate plane. As it can be obvious from the picture, each new level of the tree will add four new bits for just for cardinal labels (not including LOUDS tree itself): three \u0026lsquo;0\u0026rsquo; and one \u0026lsquo;1\u0026rsquo; (in the corresponding corners). Now if we go to higher dimensions, for 3D we will have 8 new bits, for 8D \u0026ndash; 256 new bits and for 32D \u0026ndash; $2^32$ bits. This is \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e (CoD) for spatial data structures: volume grows exponentially with dimensions, so linear sizes \u0026ndash; too.\u003c/p\u003e\n\u003cp\u003eNevertheless, with higher dimensions, the volume is getting exponentially sparser, so we can use data compression techniques like RLE encoding to represent long sparse bitmaps for cardinal labels. This is \u003cstrong\u003eBlessing of Dimensionality\u003c/strong\u003e (BoD). For \u003cem\u003ecompressed\u003c/em\u003e LOUDS cardinal trees, the example above will require $O(D)$ bits per quadrant per tree layer for $D$-dimensional Quad Tree.\u003c/p\u003e\n\u003cp\u003eSo, the the whole idea of compression in this context is implicit (or in-place) \u003cstrong\u003eDimensionality Reduction\u003c/strong\u003e. Compressed data structure don\u0026rsquo;t degrade so fast as their uncompressed analogs, yet maintain the same \u003cem\u003elogical API\u003c/em\u003e. Nevertheless, data compression is not the final cure for CoD, because practical compression itself is not that powerful, especially in the case of using RLE for bitmap compression. So, in each practical case high-dimensional tree can become \u0026ldquo;unstable\u0026rdquo; and \u0026ldquo;explode\u0026rdquo; in size. Fortunately, such highly-dimensional data ($D \u0026gt; 16$) is rarely makes sense to work with directly (without prior dimensionality reduction).\u003c/p\u003e\n\u003cp\u003eFor example, for $D=8$ exponential effects in space are still pretty moderate (256-degree cardinal tree), yet 8 dimensions is already a good approximation of real objects in symbolic methods. High-dimensional structures are effectively \u003cem\u003eblack boxes\u003c/em\u003e for us, because our visual intuitions about properties of objects don\u0026rsquo;t work in \u003ca href="https://www.math.wustl.edu/~feres/highdim"\u003ehigh dimensions\u003c/a\u003e. Like, volume of cube is concentrating in it\u0026rsquo;s corners (because there is an exponentional number of corners). Or the volume of sphere is concentrating near its surface, and many more\u0026hellip; Making decisions in high dimensions suffer from noise in data and machine rounding, because points tend to be very close to each other. And, of course, computing Euclidian distance does not make (much) sense.\u003c/p\u003e\n\u003ch2 id="comparison-with-multi-layer-perceptrons"\u003eComparison with Multi-Layer Perceptrons\u003c/h2\u003e\n\u003cp\u003eNeural networks has been known to be a pretty good function approximators, especially for multi-dimensional cases. Let\u0026rsquo;s check how compressed spatial tree can be compared with multi-layer perceptrons (MLP). This type of artificial neural networks is by no means the best example of ANNs, yet it\u0026rsquo;s a pretty ideomatic member of this family.\u003c/p\u003e\n\u003cp\u003eIn the core of MLP is the idea of \u003cem\u003elinear separability\u003c/em\u003e. In a bacis case, there are two regions of multidimensional space that can\u0026rsquo;t be separated by a hyperplane from each other. MLP has multiple ($K$) layers, where first $K-1$ layers perform specific space transformations using linear (weights) and non-linear (thresholds) operators in such way that $K$-th layer can perform the linear separation:\u003c/p\u003e\n\n\u003cdiv style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"\u003e\n  \u003ciframe src="https://www.youtube.com/embed/k-Ann9GIbP4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eSo, this condition is simplified of the picture below. Here, we have two classes (\u003cem\u003ered\u003c/em\u003e and \u003cem\u003egreen\u003c/em\u003e dots) with complex non-linear boundary between those classes. After transforming the space towards linear separation of those classes and making inverse transformation, the initial hyperplane (here, a line) is broken in many places:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is important, is that each such like break is an inverse transformation of the original hyperplane. Those transformations has to be described, and description will take some space. So we can speak about the \u003cstrong\u003edescriptional (Kolmogorov) complexity of the decision boundary\u003c/strong\u003e. Or, in the other way, \u003cem\u003ehow many neurons (parameters) we need to encode the decision boundary\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eFrom Algorithmic Information Theory it\u0026rsquo;s known that arbitrary string $s$, drawn from a uniform distribution, will be \u003cem\u003eincompressible\u003c/em\u003e with high probability, or $K(s) \\to |s|$. In other words, most mathematically possible objects are \u003cem\u003erandom\u003c/em\u003e-looking, we hardly can find and exploit any structure in them.\u003c/p\u003e\n\u003cp\u003eReturning back to MLP, it\u0026rsquo;s expected that in \u0026ldquo;generic case\u0026rdquo; decision boundaries will be \u003cem\u003ecomplex\u003c/em\u003e: the line between classes will have many breaks, so, may transformations will be needed to describe it with required precision (and this is even not taking CoD into account).\u003c/p\u003e\n\u003cp\u003eDescribing decision boundaries (DB) with compressed spatial trees may look like a bad idea from the first glance. MLPs encode DB with superpositions of elementary functions (hyperplanes and non-linear units). Quad Trees do it with hyper-cubes, and it\u0026rsquo;s obvious that we may need a lot of hyper-cubes in place of just one arbitrary hyper-plane. If it\u0026rsquo;s the case, we say that hyper-planes \u003cem\u003egeneralize\u003c/em\u003e DB \u003cem\u003emuch better\u003c/em\u003e than hyper-cubes:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBut it should hardly be a surprise, if in a real life case it will be find out that descriptions or the network and the corresponding tree are roughly the same.\u003c/p\u003e\n\u003cp\u003eSo, Neural Networks may generalize much better in some cases than compressed quad trees and perform better in very high dimensional spaces (they suffer less from CoD because of better generalization), but trees have the following benefits:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf computational complexity of MLP is $\\Omega(W)$ (a lot of large matrix multiplications), where $W$ is number of parameters, complexity of inference in the quad tree is \u003cem\u003eroughly\u003c/em\u003e from $O(log(N))$, where $N$ is number of bits of information in the tree. So trees may be much faster than networks \u003cem\u003eon average\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eQuad Trees are dynamic. Neural Networks require retraining in case of updates, at the same time adding (or removing) an element to the tree is $O(log(N))$ \u003cem\u003eworst case\u003c/em\u003e. It may be vital for may applications operating on-line, like robotics.\u003c/li\u003e\n\u003cli\u003eQuad Trees support \u0026ldquo;inverse inference\u0026rdquo; mode, when we can specify classes (outputs) and see\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, compressed quad trees may be much better for complex dynamic domains with tight decision boundaries with moderate number of dimensions (8-24). It\u0026rsquo;s not that clear yet how trees will perform in real life applications. Memoria is providing (1) \u003cem\u003eexperimental\u003c/em\u003e compressed dynamic cardinal LOUDS tree for low dimensional spaces (2 - 64).\u003c/p\u003e\n\u003cp\u003e(1) Not yet ready at the time of writing.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eThe main point of hardware acceleration of compressed Quad Trees is that inference may be really cheap (on average). It\u0026rsquo;s just a bunch of memory lookups (it\u0026rsquo;s a \u003cem\u003ememory-bound\u003c/em\u003e problem). Matrix multipliers, from other size, are also pretty energy efficient. Nevertheless, \u003cem\u003etrees scale better with complexity of decision boundaries\u003c/em\u003e.\u003c/p\u003e\n'}).add({id:18,href:"/docs/data-zoo/",title:"Containers List",description:"",content:""}).add({id:19,href:"/docs/overview/",title:"Overview List",description:"Overview List",content:""}).add({id:20,href:"/docs/",title:"Docs",description:"Docs Memoria.",content:""}),search.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()