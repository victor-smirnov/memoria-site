var suggestions=document.getElementById('suggestions'),search=document.getElementById('search');search!==null&&document.addEventListener('keydown',inputFocus);function inputFocus(a){a.ctrlKey&&a.key==='/'&&(a.preventDefault(),search.focus()),a.key==='Escape'&&(search.blur(),suggestions.classList.add('d-none'))}document.addEventListener('click',function(a){var b=suggestions.contains(a.target);b||suggestions.classList.add('d-none')}),document.addEventListener('keydown',suggestionFocus);function suggestionFocus(b){const d=suggestions.querySelectorAll('a'),e=[...d],a=e.indexOf(document.activeElement),f=suggestions.classList.contains('d-none');let c=0;b.keyCode===38&&!f?(b.preventDefault(),c=a>0?a-1:0,d[c].focus()):b.keyCode===40&&!f&&(b.preventDefault(),c=a+1<e.length?a+1:a,d[c].focus())}(function(){var a=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:'id',store:["href","title","description"],index:["title","description","content"]}});a.add({id:0,href:"/docs/overview/introduction/",title:"Introduction to Memoria",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eData dominates. If you\u0026rsquo;ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u0026ndash; Rob Pike in \u003ca href="http://www.lysator.liu.se/c/pikestyle.html"\u003e“Notes on Programming in C”\u003c/a\u003e, 1989.\u003c/p\u003e\n\u003ch3 id="how-it-started"\u003eHow it started\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eMemoria started back in 2007 out of a need of having a memory-efficient multi-dimensional spatial tree for function approximation, lake \u003ca href="/docs/data-zoo/associative-memory-2/"\u003ethis one\u003c/a\u003e. Contrary to traditional approaches for function approximation, like neural networks, spatial trees have much smaller computational complexity (logarithmic on average) for inference and allow computing partial and inverse functions out of the same set of parameters. Advanced data structures to the rescue, \u003ca href="/docs/data-zoo/louds-tree/"\u003eLOUDS tree\u003c/a\u003e has 2 bits per tree node of space complexity + some small overhead. What is also important, is that LOUDS trees can be \u003cem\u003edynamic\u003c/em\u003e, allowing point-like updates, so that tree-based function approximation method can support precise in-place tuning.\u003c/p\u003e\n\u003ch3 id="advanced-data-structures"\u003eAdvanced data structures\u003c/h3\u003e\n\u003cp\u003eLOUDS tree internally is a \u003ca href="/docs/data-zoo/searchable-seq/"\u003e\u003cem\u003esearchable\u003c/em\u003e bit vector\u003c/a\u003e supporting two additional operations \u0026ndash; \u003ccode\u003erank()\u003c/code\u003e and \u003ccode\u003eselect()\u003c/code\u003e by using two additional arrays. And all of this can be implemented as a dynamic array (with logarithmic complexity of updates). The problem is that besides those two search operations and traditional point-like query and update operations, we also need dozens of service operations like \u003cem\u003ebatch updates\u003c/em\u003e. Implementation complexity is already skyrocketing. But besides that we also need efficient concurrent multi-threaded access, external memory, transactions and versioning. In real life, implementational complexity of even apparently simple data structure, like a bitmap, may be 100-1000 times larger that one may expect by reading its description in a textbook. What is the worst thing, you may find that you need may (dozens of) different data structures\u0026hellip;\u003c/p\u003e\n\u003ch3 id="should-i-tried-a-database"\u003eShould I tried a database?\u003c/h3\u003e\n\u003cp\u003eAn obvious idea is to try using a database as a host\u0026hellip; It\u0026rsquo;s not that simple. Databases can be transactional, analytical or hybrid, and most of the time they are heavily optimized for \u003cem\u003eone\u003c/em\u003e type of data: tabular, graph or document. Unix way: do \u003cem\u003eone\u003c/em\u003e thing but do it well. Multi-model databases exist, but they are not that multi-model one may expect. Instead of one, they are doing tree things (tables, graphs and documents), and these are probably not what you what to \u003cem\u003ereuse\u003c/em\u003e to speedup your development. Anyway one should definitely try this route before start even thinking about writing their own database engine. There were way less options back in 2000th when Memoria was started than now.\u003c/p\u003e\n\u003ch3 id="or-maybe-create-a-new-one"\u003eOr, maybe, create a new one?\u003c/h3\u003e\n\u003cp\u003eIf you still want to start building your own database, prepare to suffer. Neither OS, nor programming languages are not for that. C is great, but only until you need a generic collection library. And, trust me, you will need a lot of them. Java is glorious, but prepare for low-level programming over raw memory buffers with memory leaks and undefined behaviour, or GC will be killing you every day. C++ is excellent but you will be controlling UB manually all the way down. Golang is good but its monomorphic generics story has only started recently. The same is true for Rust. High performance IO story is fully ruled by networking people who \u003ca href="https://github.com/victor-smirnov/green-fibers/wiki/Dialectics-of-fibers-and-coroutines-in-Cxx-and-successor-languages"\u003ekilled fibers\u003c/a\u003e. Memory mapping hates your high-performance NVMe SSD even on reading. There is no way to \u003cem\u003ereliably\u003c/em\u003e commit a transaction. Database engines are just \u003cem\u003etrying\u003c/em\u003e their best in this respect. And this is, more or less, guaranteed only for certain combination of storage device, OS and drivers. And I haven\u0026rsquo;t yet mentioned distributed computing. There is no reliable way to send a packet between computers. There is basically no part of your computer you can trust and rely on. And every failure may be fatal for your \u003cem\u003estate\u003c/em\u003e. You want to sleep well at nights, right?\u003c/p\u003e\n\u003ch3 id="what-memoria-is"\u003eWhat Memoria is\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eMemoria Framework is aiming to make our life in this world easier but you may be benefited too, depending on your needs and how well they fit into the project\u0026rsquo;s model.\u003c/p\u003e\n\u003cp\u003eMemoria has the following components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href="/docs/overview/hermes"\u003e\u003cstrong\u003eHermes\u003c/strong\u003e\u003c/a\u003e - arbitrarily structured object graphs allocated in relocatable contiguous memory segments, suitable for memory mapping and inter-process communication, with focus on data storage purposes. Hermes objects and containers are string-externalizable and may look like \u0026ldquo;Json with types\u0026rdquo;. GRPC-style services and related tooling (IDL) is provided.\u003c/li\u003e\n\u003cli\u003eExtensible and customizable set of \u003ca href="/docs/overview/containers"\u003e\u003cstrong\u003eData Containers\u003c/strong\u003e\u003c/a\u003e, internally based on B+Trees crafted from reusable building blocks by the metaprogramming framework. The spectrum of possible containers is from plain dynamic arrays and sets, via row-wise/column-wise tables, multitude of graphs, to compressed spatial indexes and beyond. Everything that maps well to B+Trees can be a first-class citizen of data containers framework. Containers and Hermes data objects are deeply integrated with each other.\u003c/li\u003e\n\u003cli\u003ePluggable \u003ca href="/docs/overview/storage"\u003e\u003cstrong\u003eStorage Engines\u003c/strong\u003e\u003c/a\u003e based on Copy-on-Write principles. Storage is completely separated from containers via simple but efficient contracts. Out of the box, OLTP-optimized and HTAP-optimized storage, as well as In-Memory storage options, are provided, supporting advanced features like serializable transactions and Git-like branching.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/vm"\u003e\u003cstrong\u003eDSL execution engine\u003c/strong\u003e\u003c/a\u003e. Lightweight embeddable VM with Hermes-backed code model (classes, byte-code, resources) natively supporting Hermes data types. Direct Interpreter and AOT compilation to optimized C++.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/runtime"\u003e\u003cstrong\u003eRuntime Environments\u003c/strong\u003e\u003c/a\u003e. Single thread per CPU core, non-migrating fibers, high-performance IO on top of io-uring and hardware accelerators.\u003c/li\u003e\n\u003cli\u003e\u003ca href="/docs/overview/mbt"\u003e\u003cstrong\u003eDevelopment Automation\u003c/strong\u003e\u003c/a\u003e tools. Clang-based build tools to extract metadata directly from C++ sources and generate boilerplate code.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe purpose of the project is to integrate all aspects and components described above into a single vertical framework, starting from \u003cem\u003ebare silicon\u003c/em\u003e up to networking and human-level interfaces. The framework may eventually grow up into a fully-featured \u003cem\u003emetaprogramming platform\u003c/em\u003e.\u003c/p\u003e\n\u003ch3 id="what-memoria-is-not"\u003eWhat Memoria is not\u0026hellip;\u003c/h3\u003e\n\u003cp\u003eFirst of all, Memoria is not a database engine. It may contain one as a part of the Framework, but its scope will be limited (like etcd for k8s). Large-scale \u003cem\u003edistributed\u003c/em\u003e storage is intentionally outside of the scope of the project.\u003c/p\u003e\n'}).add({id:1,href:"/docs/overview/hermes/",title:"Hermes",description:"",content:'\u003cblockquote\u003e\n\u003cp\u003eIn ancient Greece mythology Hermes is a messenger between gods and humans.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id="basic-information"\u003eBasic information\u003c/h2\u003e\n\u003cp\u003eIn the context of Memoria Framework, Hermes is a solution to the \u0026ldquo;last mile\u0026rdquo; data modeling problem. Containers can be used to model lage amouts of \u003cem\u003ehigly structured\u003c/em\u003e data. Hermes is intended to represent relatively small amout of \u003cem\u003eunstructured or semi-structured data\u003c/em\u003e and can be used together with containers. Notable feature of Hermes as a data format is that all objects and data types have canonical \u003cem\u003etextual representation\u003c/em\u003e, so Hermes data can be consumed and produced by humans. Hense, the name of the data format.\u003c/p\u003e\n\u003cp\u003eHermes defines an arbitrarily structured \u003cem\u003eobject graph\u003c/em\u003e that is allocated in a continous memory segment (or a series of fixed size segments) working as an \u003cem\u003earena\u003c/em\u003e. Top-level object is a \u003cem\u003edocument\u003c/em\u003e. Document is a container for Hermes \u003cem\u003eobjects\u003c/em\u003e. Hermes objects internally use \u003cem\u003erelative pointers\u003c/em\u003e, so the data is \u003cem\u003erelocatable\u003c/em\u003e. Hermes objects in this form can be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003estored in Memoria containers,\u003c/li\u003e\n\u003cli\u003ememory-mapped from files or between processes,\u003c/li\u003e\n\u003cli\u003eembedded into an executable as a form of a \u003cem\u003eresource\u003c/em\u003e,\u003c/li\u003e\n\u003cli\u003esent over a network or shared between host CPU and \u003cem\u003ehardware accenerators\u003c/em\u003e, even if the latter have a separate \u003cem\u003eaddress space\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHermes documents can be of arbitrary memory size, the format internally has 64-bit poiters (even on 32-bit architectures). It\u0026rsquo;s fine to have TB-size documents, but the format is not intended for that. Hermes documents are \u003cem\u003egarbage-collected\u003c/em\u003e with a simple \u003cem\u003ecopying GC\u003c/em\u003e algorithm. So the larger documents are, the more time will be spent in compactifications. Ideally, Hermes documents \u003cem\u003eshould\u003c/em\u003e (but not required to) fit into a single storage block, that is typically 4-8KB and may be up to 1MB in Memoria. In this case, accessing Hermes objects stored in containers will be in a \u003cem\u003ezero-copy\u003c/em\u003e way.\u003c/p\u003e\n\u003cp\u003eThere are three serialization formats for Hermes data.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eZero\u003c/strong\u003e serialization. Hermes data segment use relative addresses and can be extrnalized as is, as a raw memory block. All Hermes documents support fast \u003cem\u003eintegrity checking\u003c/em\u003e procedure to make sure that reading foreign segments is safe. This is the fasterst format but not particularily the densest one. This format is mainly for \u003cem\u003edata storage\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eText\u003c/strong\u003e serialization. Human-readable, safe, and the slowest (but still fast in raw numbers).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eBinary\u003c/strong\u003e serialization. The densest option, but faster than the textual one. Safe. Best for networking when human-readability is not a requirement.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHermes documents are \u003cem\u003emutable\u003c/em\u003e and support run-time immutability enforcement (\u0026ldquo;make it immutable\u0026rdquo;). Immutable documents may be safely shared between threads with minor restrictions.\u003c/p\u003e\n\u003ch2 id="hermes-object"\u003eHermes Object\u003c/h2\u003e\n\u003cp\u003eHermes memory is \u003cem\u003etagged\u003c/em\u003e. A tag is a type label and has various length, from 1 to 32 bytes. Most commonly used types, like \u003ccode\u003eInt\u003c/code\u003e have tag length of 1 byte. Rarely used types may have tags up 32 bytes, in that case it\u0026rsquo;s most likely a SHA256 \u003cem\u003ehash code\u003c/em\u003e of the \u003cem\u003etype\u0026rsquo;s declaration\u003c/em\u003e. Hermes assumes that type hash collisions will be an extremely rare event.\u003c/p\u003e\n\u003cp\u003eMemory for tag is allocated address-wise \u003cem\u003ebefore\u003c/em\u003e the object. Objects may have gaps between them in memory caused by alignment requirements. In that case, if a tag fits into this gap, it\u0026rsquo;s allocated there. With high probability, short tags (for commonly used objects) do not take extra memory.\u003c/p\u003e\n\u003cp\u003eFrom the API\u0026rsquo;s perspecitve, Hermes objects consist from two part. The first part is a private C++ object with verbose API conforming to certain rules. The second part is a public \u003ccode\u003eView\u003c/code\u003e for this object, this is much like \u003ccode\u003estd::string_view\u003c/code\u003e for string-like data. Mutable Hermes object receive a reference to the corresponding Document\u0026rsquo;s arena allocator to allocate new data in. View object encapsulates all these complexities.\u003c/p\u003e\n\u003cp\u003eViews are \u003cem\u003eowning\u003c/em\u003e in Hermes. Object\u0026rsquo;s view holds a reference to the corresponding Document (for memory allocation) with a fast, \u003cem\u003enon-atomic\u003c/em\u003e, reference counter. Hermes Views are nearly zero-cost, but not thread-safe.\u003c/p\u003e\n\u003cp\u003eDocuments as containers have additional levels of reference counting indirection, including atomic counters. Sharing document\u0026rsquo;s \u003cem\u003edata\u003c/em\u003e between threads may be permitted in some cases.\u003c/p\u003e\n\u003ch3 id="56-bit-types"\u003e56-bit types\u003c/h3\u003e\n\u003cp\u003e1-byte tag size has special consequences in Hermes: we are using 56-bit integers and identifiers to be able to fit the entire value into a 64-bit memory slot of a pointer. That, in many cases, saves memory.\u003c/p\u003e\n\u003ch2 id="datatypes"\u003eDatatypes\u003c/h2\u003e\n\u003cp\u003eHermes has explicit notion of a type, and Hermes types are pretty close in semantics to C++ classes, they may be \u003cem\u003eparametric\u003c/em\u003e in two ways. The first way is common with C++: \u003ccode\u003eMyType\u0026lt;Parameter\u0026gt;\u003c/code\u003e creates new instance of \u003ccode\u003eMyType\u003c/code\u003e parametrized by \u003ccode\u003eParameter\u003c/code\u003e. The second type of parametrization is trickier. Hermes type may have an associated \u003cem\u003estate\u003c/em\u003e that is considered as a shared state for all Hermes objects of this type. Type \u003ccode\u003eDecimal(10,2)\u003c/code\u003e have two \u003cem\u003etype constructor\u003c/em\u003e \u003ccode\u003e(10, 2)\u003c/code\u003e parameters: precision 10 and scale 2. Type constructor in Hermes does not create a new type, so there is no way to \u003cem\u003estatically\u003c/em\u003e specialize some code for \u003ccode\u003eDecimal(10, 2)\u003c/code\u003e. Object instances of type \u003ccode\u003eDecimal\u003c/code\u003e will have a pointer to a memory, storing the corresponding type constructor\u0026rsquo;s data.\u003c/p\u003e\n\u003cp\u003eGeneric types in Hermes are monomorphic.\u003c/p\u003e\n\u003cp\u003eTo distiguish between C++ types and Hermes types that may have type constructors, the letter are called \u003cem\u003eDatatypes\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="document"\u003eDocument\u003c/h2\u003e\n\u003cp\u003eIn Hermes \u0026lsquo;document\u0026rsquo; has two meanings.\u003c/p\u003e\n\u003cp\u003eFirst, \u003ccode\u003eDocument\u003c/code\u003e is a container for Hermes data. Second, \u0026lsquo;document\u0026rsquo; is a specific set of predefined collections organaizing objects into a tree-like structure, similar to Json. Below there is a short walk-through Hermes document text-serialization features giving us json-like experience.\u003c/p\u003e\n\u003ch3 id="null-object"\u003eNull object\u003c/h3\u003e\n\u003cp\u003eThis is the simplest document that has no object.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003enull\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="generic-object-array"\u003eGeneric object array\u003c/h3\u003e\n\u003cp\u003eGeneric array of Objects containing intgers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe same generic array of Objects containing numbers of different types (integer, unsigned integer, short and float):\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e[1, 2u, 3s, 4.567f]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="typed-integer-array"\u003eTyped integer array\u003c/h3\u003e\n\u003cp\u003eThe same array but of datatype \u003ccode\u003eArray\u0026lt;Int\u0026gt;\u003c/code\u003e using optimized memory layout:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e@Array\u0026lt;Int\u0026gt; = [1, 2, 3, 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eImportant note that notation above does not mean that collection \u003ccode\u003e[]\u003c/code\u003e will have the type specified before it. It means that we create a typed collection \u003cem\u003efrom\u003c/em\u003e a generic one. Parser may otimize this process by not creating the latter and supplying the data directly to the former.\u003c/p\u003e\n\u003cp\u003eThe point is that there may be may ways to create a Hermes object at parse time from hermes data. For example, notation like \u003ccode\u003e\u0026quot;string value\u0026quot;@SomeDataType\u003c/code\u003e is a syntactic shortcat meaning that \u003ccode\u003e\u0026quot;string value\u0026quot;\u003c/code\u003e will be \u003cem\u003econverted\u003c/em\u003e to Hermes object of datatype \u003ccode\u003eSomeDataType\u003c/code\u003e at the \u003cem\u003eparse time\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e\u0026quot;19345068945625345634563564094564563458.609\u0026quot;@Decimal(50,3)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTechnically, string representation of Hermes data is not a static format, like Json, but a \u003cem\u003edomain specific language\u003c/em\u003e. The purpose of this DSL is to make crafting complex Hermes data in a text form easier for humans.\u003c/p\u003e\n\u003ch3 id="generic-map-between-strings-and-objects"\u003eGeneric map between strings and objects\u003c/h3\u003e\n\u003cp\u003eGeneric map between strings and objects:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-json"\u003e{\n  \u0026quot;key1\u0026quot;: 1, \n  \u0026quot;key2\u0026quot;: [1, 2, true]},\n  \u0026quot;key3\u0026quot;: @Array\u0026lt;Int\u0026gt; = [5,6,7,8]\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id="tinyobjectmap"\u003eTinyObjectMap\u003c/h3\u003e\n\u003cp\u003eThere is a special memory-efficient variant of typed map, mapping from small integer in range [0, 51] to an Object. This map is very memory efficinet, using only 8 bytes overhead per map. It\u0026rsquo;s also very fast for reading, because it uses just one \u003ccode\u003ePopCnt()\u003c/code\u003e instruction (usually 1 cycle on modern CPUs) to find the slot in the hash array, given the key. Values up to 56 bits (small strings, short integers, floats) may be embedded into the hash array. Hash array has no empty slots.\u003c/p\u003e\n\u003cp\u003eGiven its runtime versatility, this type of a map is used extensively to represent C-like \u003cem\u003edynamic\u003c/em\u003e structuers in Hermes, without creating a corresponding C++ objects. DSLs over Hermes may combine this type of map with \u003cem\u003ecode\u003c/em\u003e, resulting in dynamically typed \u003cem\u003eobjects\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="semantic-graph"\u003eSemantic Graph\u003c/h2\u003e\n\u003ch2 id="hermespath"\u003eHermesPath\u003c/h2\u003e\n\u003ch2 id="templating-engine"\u003eTemplating engine\u003c/h2\u003e\n\u003ch2 id="schema"\u003eSchema\u003c/h2\u003e\n\u003ch2 id="hrpc"\u003eHRPC\u003c/h2\u003e\n\u003ch2 id="interoperability-with-other-languages"\u003eInteroperability with other languages\u003c/h2\u003e\n\u003ch2 id="sources"\u003eSources\u003c/h2\u003e\n\u003cp\u003eTBC\u0026hellip;\u003c/p\u003e\n'}).add({id:2,href:"/docs/overview/containers/",title:"Containers",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:3,href:"/docs/overview/storage/",title:"Storage Engines",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:4,href:"/docs/overview/vm/",title:"DSL \u0026 VM",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:5,href:"/docs/overview/runtime/",title:"Runtime Environments",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:6,href:"/docs/overview/mbt/",title:"Memoria Build Tool",description:"",content:"\u003cp\u003eTBD\u003c/p\u003e\n"}).add({id:7,href:"/docs/overview/qt_creator_instructions/",title:"QT Creator Instructions",description:"",content:'\u003ch2 id="dependencies"\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eMemoria relies on third-party libraries that either may not be available on supported developenment platfroms or have outdated versions there. Vcpkg package manager is currently being used for dependency management. Memoria itself is avaialble via \u003ca href="https://github.com/victor-smirnov/memoria-vcpkg-registry"\u003ecustom Vcpkg registry\u003c/a\u003e. Conan recipies and source packages for Linux distributions (via CPack) may be provided in the future.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/docker/Dockerfile"\u003eDockerfile\u003c/a\u003e on how to configure development environment on Ubuntu 22.04. Standard development environment will be the latest Ubuntu LTS.\u003c/p\u003e\n\u003ch2 id="install-vcpkg-for-memoria"\u003eInstall VCPkg for Memoria\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd /path/to/checout/vcpkg/into\n$ git clone https://github.com/microsoft/vcpkg.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor now, supporting compiler is Clang. Gcc 10/11/12 are crashing on Memoria. Gcc 13.1 is known to work.\u003c/p\u003e\n\u003ch2 id="configuring-vcpkgs-provided-cmake-tool"\u003eConfiguring VCPkg\u0026rsquo;s provided cmake tool\u003c/h2\u003e\n\u003cp\u003eIn Options/Kits/Cmake tab add another cmake configuration by specifying full path VCPkg\u0026rsquo;s own cmake distribution.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-required-clang-compiler"\u003eConfigure Required clang compiler\u003c/h2\u003e\n\u003cp\u003eMemoria currently is built with clang compiler version 6.0 or newer. If you system already provides it, like most Linux distributions do, then this step is unnecessary. Otherwise, build clang yourself and configure it on the Options/Kits/Compiler tab:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-clang.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="add-new-kit-for-clang"\u003eAdd new Kit for Clang\u003c/h2\u003e\n\u003cp\u003eAdding new Kit is necessary if QtCreator did not recognize clang compiler automatically. Just create new kit by cloning and existing one and specify clang 17 as C and C++ compilers:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-newkit.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="vcpkgs-cmake-selection"\u003eVCPkg\u0026rsquo;s Cmake Selection\u003c/h2\u003e\n\u003cp\u003eNow specify that VCPkg\u0026rsquo;s provided cmake tool will be used for new Kit, and specify the path to VCPkg\u0026rsquo;s libraries definitions:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-kit-cmake.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eProvide your full path to vcpkg.cmake:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-vcpkg-toolchain.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="configure-memorias-build-parameters"\u003eConfigure Memoria\u0026rsquo;s build parameters\u003c/h2\u003e\n\u003cp\u003eToggle BUILD_* options as specified on the screenshot. This will build Tests, as well as threads- and fibers-based Memoria allocators, with libbacktrace support in case of exceptions.\u003c/p\u003e\n\u003cp\u003eMore details on build options can be found in top-level CMakeLists.txt\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="qtcreator-project-cfg.png" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThat\u0026rsquo;s it!\u003c/p\u003e\n\u003cp\u003ePress Ctrl+B to start build process.\u003c/p\u003e\n'}).add({id:8,href:"/docs/data-zoo/overview/",title:"Core Data Structures -- Overview",description:"",content:"\u003cp\u003eThis sections contans detailed description of some core data structures Memoria\u0026rsquo;s containers are based on.\u003c/p\u003e\n"}).add({id:9,href:"/docs/data-zoo/partial-sum-tree/",title:"Partial Sums Tree",description:"",content:'\u003ch2 id="description"\u003eDescription\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s take a sequence of monotonically increasing numbers, and get delta sequence from it, as it is shown on the following figure. Partial Sum Tree is a tree of sums over this delta sequence.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="trees.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003ePacked tree in Memoria is a multiary balanced tree mapped to an array. For instance in a level order as it shown on the figure.\u003c/p\u003e\n\u003cp\u003eGiven a sequence of N numbers with monotonically increasing values, partial sum tree provides several important operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efindLT(value)\u003c/code\u003e finds position of maximal element less than \u003ccode\u003evalue\u003c/code\u003e, time complexity $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efindLE(value)\u003c/code\u003e finds position of maximal element less than or equals to \u003ccode\u003evalue\u003c/code\u003e, $T = O(log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esum(to)\u003c/code\u003e computes plain sum of values in the delta sequence in range [0, to), $T = O(log(N))$\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eadd(from, value)\u003c/code\u003e adds \u003ccode\u003evalue\u003c/code\u003e to all elements of original sequence in the range of [from, N), $T = O(log(N))$.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt is obvious that first two operations can be computed with binary search without partial sum tree and all that overhead it introduces.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003ePartial or prefix sum trees of higher degrees are especially suitable for hardware acceleration. DDR and HBM memory transfer data in batches and works best if data is processed also in batches. The idea here is to offload tree traversal operation from CPU to the memory controller or even to DRAM memory chips. Sum and compare operations are relatively cheap to implement, and no complex control is required.\u003c/p\u003e\n\u003cp\u003eBy offloading tree traversal to the memory controller (that usually works in front of caches), we can save precious cache space for more important data. By offloading summing and comparison to DRAM chips, we can better exploit internal memory parallelism and save memory bandwidth. In such distributed architecture, a single tree level scan can be performed with the latency and in the power budget of a \u003cem\u003esingle random memory access\u003c/em\u003e, saving energy and silicon for other computations.\u003c/p\u003e\n'}).add({id:10,href:"/docs/data-zoo/searchable-seq/",title:"Searchable Sequence",description:"",content:'\u003cp\u003eSearchable sequence or rank/select dictionary is a sequence of symbols that supports two operations:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003erank(position, symbol)\u003c/code\u003e is number of occurrences of \u003ccode\u003esymbol\u003c/code\u003e in the sequence in the range [0, position)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect(rank, symbol)\u003c/code\u003e is a position of \u003ccode\u003erank\u003c/code\u003e-th occurrence of the \u003ccode\u003esymbol\u003c/code\u003e in the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsually the alphabet is {0, 1} because of its practical importance, but larger alphabets are of significant interest too. Especially in Bioinformatics and Artificial Intelligence.\u003c/p\u003e\n\u003cp\u003eThere are many implementations of binary searchable sequences (bitmaps) providing fast query operations with $O(1)$ time complexity. Memoria uses partial sum indexes to speedup rank/select queries. They are asymptotically slower than other methods but have additional space overhead for the index.\u003c/p\u003e\n\u003cp\u003ePacked searchable sequence is a searchable sequences that has all its data structured packed into a single contiguous memory block with packed allocator. It consists from two data structures:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003emulti-index partial sum tree to speedup rank/select queries;\u003c/li\u003e\n\u003cli\u003earray of sequence\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for the case of searchable bitmap.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_seq.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that for a sequence with K-symbol alphabet, packed sum tree has K indexes, that results in significant overhead even for relatively small alphabets. For example 8-bit sequence has 256-index packed tree that takes more than 200% of raw data size if the tree is not compressed. To lower this overhead Memoria provides various compressed encodings for the index\u0026rsquo;s values.\u003c/p\u003e\n\u003ch2 id="creation-and-access"\u003eCreation and Access\u003c/h2\u003e\n\u003cp\u003eTo create partial sum tree for a sequence we first need to split it logically into blocks of fixed number of symbols (16 at the figure). Then sum different symbols in the block, each such vector is a simple partial sum tree leaf. Build other levels of the tree accordingly.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSymbol update is relatively fast, it takes $O(log(N))$ time.\u003c/li\u003e\n\u003cli\u003eSymbol insertion is $O(N)$, it requires full rebuilding of partial sum tree.\u003c/li\u003e\n\u003cli\u003eSymbol access does not require the tree to perform, it takes $O(1)$ time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003erank(position, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven \u003ccode\u003eposition\u003c/code\u003e, determine sequence \u003ccode\u003eblock_number\u003c/code\u003e for that position, and \u003ccode\u003eblock_pos\u003c/code\u003e position in the block, $O(1)$;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eblock_rank\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) in the sum tree, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003ecount number of \u003ccode\u003esymbol\u003c/code\u003es in the block to \u003ccode\u003eblock_pos\u003c/code\u003e, $O(1)$;\u003c/li\u003e\n\u003cli\u003efinal rank is (2) + (3).\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eTo compute \u003ccode\u003eselect(rank, symbol)\u003c/code\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGiven partial sum tree and a \u003ccode\u003esymbol\u003c/code\u003e, determine target sequence \u003ccode\u003eblock_number\u003c/code\u003e having \u003ccode\u003etotal_rank \u0026lt;= rank\u003c/code\u003e for the given \u003ccode\u003esymbol\u003c/code\u003e using \u003ca href="/docs/data-zoo/partial-sum-tree"\u003efindLE\u003c/a\u003e operation, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eFor the given block, compute \u003ccode\u003erank_prefix\u003c/code\u003e = \u003ca href="/docs/data-zoo/partial-sum-tree"\u003esum\u003c/a\u003e(0, \u003ccode\u003eblock_number\u003c/code\u003e) for the given \u003ccode\u003esymbol\u003c/code\u003e, $O(log(N))$;\u003c/li\u003e\n\u003cli\u003eCompute \u003ccode\u003elocal_rank = rank - rank_prefix\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003eScan the block and find position of \u003ccode\u003esymbol\u003c/code\u003e having rank in the block = \u003ccode\u003elocal_rank\u003c/code\u003e, $O(1)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eActual implementation joins operations (1) and (2) into a single traverse of the sum tree.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eConsideration are the same as for \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial/prefix sum trees\u003c/a\u003e, especially, because searchable sequence contains partial sum tree as an indexing structure.\u003c/p\u003e\n\u003cp\u003eModern CPUs usually have direct implementations for rank (PopCount) for binary alphabets. Select operation may also be partially supported. To accelerate searchable sequences, it\u0026rsquo;s necessary to implement rang/select over arbitrary alphabets (1-8 bits per symbol).\u003c/p\u003e\n\u003cp\u003eSymbol blocks are also contiguous in memory and can be multiple of DRAM memory blocks. Rank/select machinery is simpler or comparable with machinery for addition and subtraction. Those operations can be efficiently implemented in a small silicon budget and at high frequency.\u003c/p\u003e\n'}).add({id:11,href:"/docs/data-zoo/compressed-symbol-seq/",title:"Compressed Symbol Sequence",description:"",content:"\u003cp\u003eTBC\u003c/p\u003e\n"}).add({id:12,href:"/docs/data-zoo/hierarchical-table/",title:"Hierarchical Table",description:"",content:"\u003cp\u003eTBC\u003c/p\u003e\n"}).add({id:13,href:"/docs/data-zoo/louds-tree/",title:"Level Order Unary Degree Sequence (LOUDS) ",description:"",content:'\u003cp\u003eLevel Order Unary Degree Sequence or LOUDS is a special form of ordered tree encoding. To get it we first need to enumerate all nodes of a tree in level order as it is shown of the following figure.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds.png"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThen for each node we write its degree in unitary encoding. For example, degree of the fist node is 3, then its unitary encoding is \u0026lsquo;1110\u0026rsquo;. To finish LOUDS string we need to prepend substring \u0026lsquo;10\u0026rsquo; to it as it is shown on the figure. Given an ordered tree on N nodes LOUDS takes no more than 2N + 1 bits. This is very succinct implicit data structure.\u003c/p\u003e\n\u003cp\u003eLOUDS is a bit vector. We also need the following operations on it:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erank1(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;1\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank0(i)\u003c/code\u003e \u0026ndash; returns number of \u0026lsquo;0\u0026rsquo; in the range [0, i)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect1(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;1\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eselect0(rnk)\u003c/code\u003e \u0026ndash; returns position of rnk-th \u0026lsquo;0\u0026rsquo; in the LOUDS string, rnk = 1, 2, 3, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDifferent ways of tree node numbering for LOUDS are possible, Memoria uses the simplest one. Tree node positions are coded by \u0026lsquo;1\u0026rsquo;.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003enode_num = rank1(i + 1)\u003c/code\u003e \u0026ndash; gets tree node number at position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ei = select1(node_num)\u003c/code\u003e \u0026ndash; finds position of a node in LOUDS given its number in the tree.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHaving this node numbering we can define the following tree navigation operations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efist_child(i) = select0(rank1(i + 1)) + 1\u003c/code\u003e \u0026ndash; finds position of the first child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elast_child(i) = select0(rank1(i + 1) + 1) - 1\u003c/code\u003e \u0026ndash; finds position of the last child for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparent(i) = select1(rank0(i + 1))\u003c/code\u003e \u0026ndash; finds position of the parent for the node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echildren(i) = last_child(i) - first_child(i)\u003c/code\u003e \u0026ndash; return number of children for node at the position \u003ccode\u003ei\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003echild(i, num) = first_child(i) + num\u003c/code\u003e \u0026ndash; returns position of num-th child for the node at the position \u003ccode\u003ei\u003c/code\u003e, \u003ccode\u003enum \u0026gt;= 0\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eis_node(i) = LOUDS[i] == 1 ? true : false\u003c/code\u003e \u0026ndash; checks if \u003ccode\u003ei\u003c/code\u003e-th position in tree node.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that navigation operations only defined for positions \u003ccode\u003ei\u003c/code\u003e for those \u003ccode\u003eis_leaf(i) == true\u003c/code\u003e.\u003c/p\u003e\n\u003ch2 id="example"\u003eExample\u003c/h2\u003e\n\u003cp\u003eLet we find number of the first child for the node 8.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eselect1(8) = 11\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efirst_child(11) = select0(rank1(11 + 1)) + 1 = select0(8) + 1 = 19\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003erank1(19 + 1) = 12\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first child for node 8 is node 12.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how the parent() operation works:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="louds-parent.png"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="limitations"\u003eLimitations\u003c/h2\u003e\n\u003cp\u003eNode numbers are valid only until insertion (or deletion) into the tree. Additional data structure is necessary to keep track of node positions after tree structure updates.\u003c/p\u003e\n\u003ch2 id="labelled-tree"\u003eLabelled Tree\u003c/h2\u003e\n\u003cp\u003eLabelled tree is a LOUDS tree with a fixed set of numbers (or \u0026lsquo;labels\u0026rsquo;) associated with each node. It is implemented as multistream balanced tree where the first stream is dynamic bit vector with rank/select support, and the rest are streams for each label.\u003c/p\u003e\n\u003cp\u003eStreams elements distribution is very simple for this data structures. Each label belongs to a tree node that is coded by position of \u0026lsquo;1\u0026rsquo; in LOUDS. Each leaf of balanced tree has some subsequence of the LOUDS.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="labeled_tree_leaf.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLet we have a LOUDS sub-stream of size N with M 1s for a given leaf. Then this leaf must contain M labels in each label stream. The following expressions links together node and level positions withing a leaf:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003elabel_idx = rank1(node_idx + 1) - 1\u003c/code\u003e;\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003enode_idx = select1(label_idx + 1)\u003c/code\u003e;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWhere \u003ccode\u003elabel_idx\u003c/code\u003e is in [0, M) and \u003ccode\u003enode_idx\u003c/code\u003e is in [0, N)\u003c/p\u003e\n\u003cp\u003eLabeledTree uses balanced partial sum tree as a basic data structure. Because of that it optionally supports partial sums of labels is specified tree node range. The most interesting range is \u003ccode\u003e[0, node_idx)\u003c/code\u003e. See \u003ca href="/docs/data-zoo/wavelet-tree"\u003eMultiary Wavelet Tree\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2 id="cardinal-trees"\u003eCardinal Trees\u003c/h2\u003e\n\u003cp\u003eIn ordered trees like in the example above, all children nodes are naturally ordered, and a node may have arbitrary number of children. In the cardinal tree of degree $D$, a node always have $D$ children, but some children can be omitted. And this information is stored in the tree, like \u0026ldquo;child $i$ is absent\u0026rdquo;. Binary search trees are cardinal trees of degree 2.\u003c/p\u003e\n\u003cp\u003eCardinal tree of degree 4 (and greater, where degree is a power of 2) is a trie-based (or region-based) \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eCardinal LOUDS tree can be implemented either as a labelled tree, where node labels (from 0 to $D-1$) are \u003cem\u003ecardinal labels\u003c/em\u003e (for sparse cardinal trees like spatial trees), or by using a searchable bitmap specifying which children are present. The bitmap can be \u003cem\u003ecompressed\u003c/em\u003e, saving space for sparse cases.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS trees are especially important type of compact/succinct trees, because all children of a node are stored linearly in memory, that is DRAM-friendly: traversal of a tree is a series of liner scans split by random jumps. But the number of random jumps is much smaller comparing to other types of trees. LOUDS trees do not require any specific hardware support, providing that \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable bitmap\u003c/a\u003e is fully accelerated.\u003c/p\u003e\n'}).add({id:14,href:"/docs/data-zoo/wavelet-tree/",title:"Multiary Wavelet Trees",description:"",content:'\u003cp\u003eWavelet trees (WT) are data succinct rank/select dictionaries for large alphabets with many \u003ca href="http://arxiv.org/abs/1011.4532"\u003epractical applications\u003c/a\u003e. There is a good \u003ca href="http://alexbowe.com/wavelet-trees/"\u003eexplanation\u003c/a\u003e of what binary wavelet trees are and how they work. They provide rank() and select() over symbol sequences ($N$ symbols) drawn from arbitrary fixed-size alphabets ($K$ symbols) in $O(log(N) * log(K))$ operations, where logarithms are on the base of 2. Therefore, for large alphabets, $log(K)$ is quite a big value that leads to big hidden constants in practical implementations of the binary WT.\u003c/p\u003e\n\u003cp\u003eIn order to improve runtime efficiency of wavelet trees we have to lower this constant. And one of the way here is to use multiary cardinal trees instead of binary ones. In this case, for $M$-ary cardinal tree we will have $log(M)$ speedup factor over binary trees (tree height is $log(M)$-times smaller).\u003c/p\u003e\n\u003ch2 id="wavelet-tree-structure"\u003eWavelet Tree Structure\u003c/h2\u003e\n\u003cp\u003eLet we have a sequence of integers, say, 54.03.12.21.47.03.17.54.22.51 drawn from 6-bit alphabet. The following figure shows 4-ary wavelet tree for this sequence. Such WT has $6/log_2(4) = 3$ levels.\u003c/p\u003e\n\u003cp\u003eFirst we need to represent our sequence in a different format. Our WT is 4-ary and has 3 layers. We need to \u0026ldquo;split\u0026rdquo; the sequence in 3 layers horizontally where symbols of each layer are drawn from 2-bit alphabet. In other words, we need to recode our sequence from base of 10 to base of 4, and then write numbers vertically:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that this is just a logical operation, it doesn\u0026rsquo;t require any transformation of the sequence itself.\u003c/p\u003e\n\u003cp\u003eOur WT is a 4-ary cardinal tree, each node has from 0 to 4 children. Each child represents one symbol from the layer\u0026rsquo;s alphabet. Note that in general case it isn\u0026rsquo;t necessary to draw all layers from the same alphabet, but it simplifies implementation.\u003c/p\u003e\n\u003cp\u003eIn order to build WT, perform the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAssign top layer (Layer 2) of the sequence to the root node of WT.\u003c/li\u003e\n\u003cli\u003eFor each symbol of Layer 1 put it to the subsequence of the node with the cardinal label matched with corresponding symbol from the same position in the Layer 2.\u003c/li\u003e\n\u003cli\u003eRepeat step (2) for symbols at Layer 0 but now select appropriate child at Level 1 of the tree, using pair of symbols from the same positions at Layer 1 and Layer 2 of the sequence.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the figure for details. Symbols in the WT and the sequence are colored to simplify understanding of symbols\' distribution.\u003c/p\u003e\n\u003cp\u003eNote that for an alphabet with K symbols, multiary WT has up to K leafs that can be very significant number. But for most practical cases this number is moderate. The larger number of distinct symbols in the sequence, the bigger tree is. Dynamic LOUDS with associated cardinality labels is used to code structure of WT.\u003c/p\u003e\n\u003cp\u003eAlso, it is not necessary to keep empty nodes in the tree (they are shown in gray on the figure).\u003c/p\u003e\n\u003ch2 id="insertion-and-access"\u003eInsertion and Access\u003c/h2\u003e\n\u003cp\u003eTo insert a value into WT we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the path from root to leaf for inserted values, insert tree nodes if necessary;\u003c/li\u003e\n\u003cli\u003efind correct position in the node\u0026rsquo;s subsequence to insert current symbol.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe path in the wavelet tree is determined by \u0026ldquo;layered\u0026rdquo; representation if inserted symbol. Computation of insertion position is a bit tricky.\u003c/p\u003e\n\u003cp\u003eLet we insert the value of 37 into position 7. Layered representation of 37 is \u0026ldquo;211\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLevel 2. Insert \u0026ldquo;2\u0026rdquo; into position 7 of root node\u0026rsquo;s subsequence of WT.\u003c/li\u003e\n\u003cli\u003eLevel 1. Next child is \u0026ldquo;2\u0026rdquo;. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(7 + 1, 2) - 1 = rank(8, 2) - 1 = 1\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003cli\u003eLevel 0. Next child is \u0026ldquo;1\u0026rdquo;, create it. Repeat the procedure for Layer 1. Insertion position for \u0026ldquo;1\u0026rdquo; is \u003ccode\u003erank(1 + 1, 1) - 1 = rank(2, 1) - 1 = 0\u003c/code\u003e computed in the parent node\u0026rsquo;s sequence for this child.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSee the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_insert.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAccess is similar, but instead of to insert a symbol to a node\u0026rsquo;s subsequence, take the symbol form it and use it to select next child.\u003c/p\u003e\n\u003ch2 id="rank"\u003eRank\u003c/h2\u003e\n\u003cp\u003eTo compute rank(position, symbol) we need:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003efind the leaf in WT for the symbol;\u003c/li\u003e\n\u003cli\u003efind position in the leaf to compute the final rank.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_rank.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="select"\u003eSelect\u003c/h2\u003e\n\u003cp\u003eComputation of select(rank, symbol) is different. If rank() is computed top-down, then select() is computed bottom-up.\u003c/p\u003e\n\u003cp\u003eLet we need to select position of the 2nd 3 in the original sequence. Layered representation for 3 is \u0026ldquo;003\u0026rdquo;.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFind the leaf in WT for the given symbol.\u003c/li\u003e\n\u003cli\u003ePerform \u003ccode\u003eselect(2, \u0026quot;3\u0026quot;) = Pos0\u003c/code\u003e on the leaf\u0026rsquo;s sequence.\u003c/li\u003e\n\u003cli\u003eWalk up to parent for his leaf. Perform \u003ccode\u003eselect(Pos0 + 1, \u0026quot;0\u0026quot;) = Pos1\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eStep up to the parent node (the root). Perform \u003ccode\u003eselect(Pos1 + 1, \u0026quot;0\u0026quot;) = Pos\u003c/code\u003e. This is the final result.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCheck the following figure for details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiary_wavelet_tree_select.svg"/\u003e\n\u003c/figure\u003e\n\n\u003ch2 id="implementations"\u003eImplementations\u003c/h2\u003e\n\u003cp\u003eIn Memoria, Multiary wavelet tree consists of four distinct data structures.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLOUDS to store wavelet tree structure.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of cardinal labels for tree nodes.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of sizes for tree node\u0026rsquo;s sub-sequences.\u003c/li\u003e\n\u003cli\u003eTree-ordered sequence of node\u0026rsquo;s symbols.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe first three structures are implemented as single \u003ca href="/docs/data-zoo/louds-tree"\u003eLabeled Tree\u003c/a\u003e with two labels. The first one is cardinality of the node in its parent. The second one is size of node\u0026rsquo;s subsequence.\u003c/p\u003e\n\u003cp\u003eThe fourth data structure is a separate \u003ca href="/docs/data-zoo/searchable-seq"\u003eSearchable Sequence\u003c/a\u003e for small sized alphabets.\u003c/p\u003e\n\u003cp\u003eMemoria has two different implementations of WT algorithm. The first one is dynamic WT that provides access/insert/select/rank operations performing in O(log \u003cem\u003eN\u003c/em\u003e) time.\u003c/p\u003e\n\u003cp\u003eThe second one has all those four data structures implemented with \u003ca href="Memory_Allocation"\u003ePacked Allocator\u003c/a\u003e placed in a single raw memory block of limited size. This implementation has fast access/select/rank operations but slow insert operation with O(\u003cem\u003eN\u003c/em\u003e) time complexity.\u003c/p\u003e\n\u003cp\u003eCurrently Memoria provides only 256-ary wavelet tree for 32-bit sequences. Other configurations will be provided in upcoming releases of the framework.\u003c/p\u003e\n'}).add({id:15,href:"/docs/data-zoo/mutistream-tree/",title:"Mutistream Balanced Tree",description:"",content:'\u003ch2 id="the-problem"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eLet us consider data structure mapping an integer \u003ccode\u003eID\u003c/code\u003e to a dynamic vector. Nothing complicated is here but there is one requirement: it has to represent small data values compactly, without that overhead most file systems have. That means we can\u0026rsquo;t just use \u003ccode\u003eMap\u0026lt;ID, Vector\u0026lt;\u0026gt;\u0026gt;\u003c/code\u003e because like any other container, even empty \u003ccode\u003eVector\u0026lt;\u0026gt;\u003c/code\u003e consumes at least one memory block. So this overhead will be very significant for large number of small data values.\u003c/p\u003e\n\u003cp\u003eThe solution is to store all data values in a single dynamic vector and use an additional data structure for the dictionary of data values. This dictionary is a set of pairs in the form \u003ccode\u003e\u0026lt;ID, DataOffset\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="vector_map.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe dictionary can be easily represented via a balanced partial sum tree having two indexes, one for \u003ccode\u003eID\u003c/code\u003e and one for \u003ccode\u003eDataOffset\u003c/code\u003e as it is shown on the following figure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="double_index.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eAn multi-index partial sums tree is just an ordinary \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e except that scalar values of sums regarded as vectors.\u003c/p\u003e\n\u003cp\u003eThere is no overhead for small data values in this model because both data structures (the dictionary and the dynamic vector) are represented in the most compact way. But now there is performance overhead because each data value search requires traversal of two balanced trees, the dictionary and the dynamic vector.\u003c/p\u003e\n\u003ch2 id="the-solution"\u003eThe Solution\u003c/h2\u003e\n\u003cp\u003eThe solution is to intermix the dictionary and the data within a single balanced search tree so that the dictionary entries are placed closer to its data.\u003c/p\u003e\n\u003cp\u003eLet us think that a balanced tree of a dictionary entry is the primary one. If all data values are small we can put all of them into the same tree leafs with their dictionary entry. But in general case data entries can be much larger than limited capacity of the leaf page. To maintain large data values we need complete dynamic vector sub-structure in the balanced tree of dictionary entries.\u003c/p\u003e\n\u003cp\u003eThe following figure shows how it can be done:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree_nodes.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the leaf nodes of balanced tree we put two dynamic array:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ean array of dictionary entries (red, blue) and\u003c/li\u003e\n\u003cli\u003ean array of data (green).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIn the branch nodes we put three dynamic arrays:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ethe array of partial sums for dictionary entries (red, blue),\u003c/li\u003e\n\u003cli\u003ethe array of partial sums for data lengths (green),\u003c/li\u003e\n\u003cli\u003ethe array of child node IDs.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis joined balanced partial sum tree contains all information from both source trees, dictionary and dynamic entry ones. It is not necessary to place dictionary entries in the same leafs with their data.\u003c/p\u003e\n\u003cp\u003eThen total balanced tree structure will look like this:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multistream_tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIt can be shown that entries can be placed in any place relative to their data providing that sequences of entries is ordered correctly. But logically linked entities should be placed closer to each other in terms of leaf nodes for performance reasons.\u003c/p\u003e\n'}).add({id:16,href:"/docs/data-zoo/packed-allocator/",title:"Packed Allocator",description:"",content:'\u003cp\u003eWe need to place several, possibly resizable (see below), objects into a single contiguous memory block of limited size. Classical malloc-like memory allocator is not suitable here because it doesn\u0026rsquo;t work well with resizable objects. Especially if they are allocated in a relatively small memory block. To maintain resizability efficiently we have to relocate other objects if some object is resized.\u003c/p\u003e\n\u003ch2 id="resizable-object-pattern"\u003eResizable Object Pattern\u003c/h2\u003e\n\u003cp\u003eResizable object is an object that has unbounded size. Usually it has the following pattern:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  int object_size_;\n  char[] variable_size_data_;\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, the last member \u003ccode\u003echar[] variable_size_data_\u003c/code\u003e is an unbounded array. For any object \u003ccode\u003esizeof()\u003c/code\u003e does\nnot count the last member if it is unbounded array. For example, \u003ccode\u003esizeof(ResizableObject) == sizeof(int)\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf a custom allocator allocates more memory than necessary for resizable objects, this memory can be accessed via the last member. Usually such an object should know length of the memory block it is mapper to. But it can also be stored by memory allocator.\u003c/p\u003e\n\u003ch2 id="linear-contiguous-allocator"\u003eLinear Contiguous Allocator\u003c/h2\u003e\n\u003cp\u003eThe idea is to place all abjects contiguously in a memory block and shift them if some object is resized. Separate layout dictionary is used to locate objects in a block. Objects are accessed by their indexes, not by direct addresses in the block.\u003c/p\u003e\n\u003cp\u003eLayout dictionary is an ordered list of block offsets. If dictionary if large, \u003ca href="/docs/data-zoo/partial-sum-tree"\u003epartial sum tree\u003c/a\u003e can be used to speedup access.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_contiguous_allocator.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eLayout dictionary is placed into the same memory block as object\u0026rsquo;s data.\u003c/p\u003e\n\u003ch2 id="allocator-aware-objects"\u003eAllocator-Aware Objects\u003c/h2\u003e\n\u003cp\u003eThe main property of resizable objects is that their size can be changed dynamically, that requires interaction with allocator. Consider the following example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass ResizableObject {\n  //...\n  int insert(int index, int value); // enlarge object\n  int remove(int index);            // shrink object\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have two methods affecting object\u0026rsquo;s size. And it is good to incapsulate required interaction with allocator within resizable objects. But the problem is we don\u0026rsquo;t want to store raw memory pointers to the allocator withing objects for various reasons. The main reason is we want allocators to be relocatable. If the allocator itself is relocated, all pointers have to be updated.\u003c/p\u003e\n\u003cp\u003eThe idea is to put allocator and objects into a single addressable memory block. In this case we can get address of allocator having only address of the object and its relative offset in the memory block. Let\u0026rsquo;s consider \u003ccode\u003ePackedAllocatable\u003c/code\u003e base class for any allocator-aware resizable object:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\nclass PackedAllocator;\n\nclass PackedAllocatable {\n  typedef PackedAllocator Allocator;\n  int allocator_offset_; // resizable object offset in the allocator\'s memory block\npublic:\n  Allocator* allocator() {\n    if (allocator_offset_ \u0026gt; 0) {\n      return reinterpret_cast\u0026lt;Allocator*\u0026gt;(reinterpret_cast\u0026lt;char*\u0026gt;(this) - allocator_offset_);\n    }\n    else return nullptr;\n  }\n\n  // Other methods go here...\n};\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee the following figure how it may look like:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_brief.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn Memoria we call such allocator \u003cem\u003epacked\u003c/em\u003e, because it packs all objects and itself into single self-sufficient memory block that can be relocated or serialized. That doesn\u0026rsquo;t affect relative positions of objects within the memory block.\u003c/p\u003e\n\u003cp\u003eEach allocator-aware resizable object must derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e class that maintains relative offset of an object to the allocator. The following figure explains it in greater details:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere we have four resizable objects with sizes 4, 7, 11, and 9 respectively. Each object maintains its relative offset in the memory block, that is converted to a pointer to the allocator.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s increase object #1 by 8 units:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="packed_allocator_full_enlarged.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWe need to perform the following operations for objects #2 and #3.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eShift objects\' data right by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease objects\' offsets in layout dictionary by 8 units.\u003c/li\u003e\n\u003cli\u003eIncrease \u003ccode\u003ePackedAllocatable::allocator_offset_\u003c/code\u003e by 8 units.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNot all abjects are resizable, they don\u0026rsquo;t need to maintain a pointer to the allocator. But now allocator has to know which objects are instances of \u003ccode\u003ePackedAllocatable\u003c/code\u003e and which are not to update pointers properly.\u003c/p\u003e\n\u003ch2 id="recursive-allocator"\u003eRecursive Allocator\u003c/h2\u003e\n\u003cp\u003eThe next idea is to define packed allocator recursively by deriving from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this case it is possible to embed any packed allocator into another allocator just as any resizable object.\u003c/p\u003e\n\u003ch2 id="packedallocator-api"\u003ePackedAllocator API\u003c/h2\u003e\n\u003cp\u003eSo we have:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003ePackedAllocator\u003c/code\u003e managing resizable memory regions withing contiguous relocatable memory block.\u003c/li\u003e\n\u003cli\u003eAllocator-aware objects derive from \u003ccode\u003ePackedAllocatable\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003ePacked allocator also derives from \u003ccode\u003ePackedAllocatable\u003c/code\u003e that mean it can be embedded into another packed allocator.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe last idea is to use \u003ccode\u003ePackedAllocator\u003c/code\u003e as a base class for resizable objects. That enables them to have more than one resizable section.\u003c/p\u003e\n\u003cp\u003eThe following code snippet explains basic PackedAllocator API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass PackedAllocatable {/*...*}; // maintains a managed pointer to packed allocator\n\nclass PackedAllocator: public PackedAllocatable {\npublic:\n  //...\n\n  template \u0026lt;typename T\u0026gt;\n  T* get(int i);             //returns address of i-th memory block, i = 0...\n  template \u0026lt;typename T\u0026gt;\n  const T* get(int i) const; //returns address of i-th memory block, i = 0...\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i, int size); // allocates size bytes for i-th memory block and initializes\n                                // it properly if T derives from PackedAllocatable.\n\n  template \u0026lt;typename T\u0026gt;\n  T* allocate(int i); // Allocates space for i-th memory block and initializes\n                      // it properly if T derives from PackedAllocatable. \n                      // Size of the block is got via T::empty_size() if T derives \n                      // form PackedAllocatable and sizeof(T) otherwise.\n\n  // resize memory block at address \'addr\', resize parent allocator if necessary\n  // throws PackedOOMException if top-most allocator runs out of memory\n  void resize(const void* addr, int new_size); \n\n  //the same as above but returns size of i-th block in bytes\n  void resize(int i, int new_size);\n\n  int size(int i) const; \n\n  void init(int entries); // initializes allocator with the specified number of \n                          // empty blocks (entries)\n \n  // returns size in bytes of empty allocator having specified number of entries\n  static int empty_size(int entries); \n\n  static int round(int size); // round size to alignment blocks. e.g. if alignment block is 8 bytes\n                              // round(1) = 8; round(12) = 16\n\n  //...\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo interact with PackedAllocator each allocatable object provides two methods. One of them is for initialization, and another one is to query object size:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003eclass SimpleResizableObject: public PackedAllocatable {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\nclass AdvancedResizableObject: public PackedAllocator {\npublic:\n  \n  // initialize the object\n  void init();\n  \n  // returns smallest size of the object in bytes\n  static int empty_size();\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that resizable objects must be \u003ca href="http://en.cppreference.com/w/cpp/types/is_trivial"\u003etrivial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote also that Memoria doesn\u0026rsquo;t currently use placement \u003ca href="http://en.cppreference.com/w/cpp/memory/new/operator_new"\u003enew\u003c/a\u003e и \u003ca href="http://en.cppreference.com/w/cpp/language/delete"\u003edelete\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e// There is no way to specify custom allocator and other parameters here, only address \n// of the object to delete. PackedAllocator does not allow to get allocator address given\n// only address of a block it manages. so it provides explicit API for allocation \n// and deallocation.\n\nvoid operator delete (void *ptr); // placement delete operator\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee this \u003ca href="https://github.com/victor-smirnov/memoria/blob/master/include/memoria/core/packed/tools/packed_allocator.hpp"\u003esource\u003c/a\u003e for more details about PackedAllocator implementation.\u003c/p\u003e\n\u003ch2 id="resizable-object-example-seachable-sequence"\u003eResizable Object Example: Seachable Sequence\u003c/h2\u003e\n\u003cp\u003eLet\u0026rsquo;s consider a relatively simple but live example: \u003ca href="/docs/data-zoo/searchable-seq"\u003esearchable sequence\u003c/a\u003e. It is a sequence of symbols providing rank and select operations performed in logarithmic time. To provide such time complexity, data structure uses additional index that have to be quite complex for large alphabets. Let\u0026rsquo;s say that in general case the index is compressed and we don\u0026rsquo;t know it\u0026rsquo;s actual size ahead of time. The size of index is a variable value depending of a sequence content.\u003c/p\u003e\n\u003cp\u003eSo the sequence has at least two resizable blocks: index, and symbols.\u003c/p\u003e\n\u003cp\u003eBelow there is a code snipped explaining how update operations on the object interact with its allocator(s).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class="language-c++"\u003e\ntemplate \u0026lt;int BitsPerSymbol\u0026gt;\nclass SearchableSequence: public PackedAllocator {\n  typedef PackedAllocator                              Base;\n  typedef SearchableSequence\u0026lt;BitsPerSymbol\u0026gt;            MyType;\n\n  typedef unsigned int                                 Symbol;\n\n  // Instantiate index only if the sequence is larger\n  // than this value.\n  static const int IndexSizeThreshold                  = 64;\n\n  // no members should be declared here\n  // use Metadata class instead\npublic:\n  class Metadata {\n    int size_;\n    int\u0026amp; size() {return size_;} \n    const int\u0026amp; size() const {return size_;} \n  };\n\n  class Index: public PackedAllocator {\n    // index data structure for this SearchableSequence\n  };\n\n  // This enum codes memory block indexes.\n  enum {\n    METADATA, // Metadata block\n    INDEX,    // Indexes block\n    SYMBOLS   // Symbols block\n  };\n\n  // returns address of Metadata object\n  Metadata* metadata() {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n  const Metadata* metadata() const {\n    return Base::template get\u0026lt;Metadata\u0026gt;(METADATA);\n  }\n\n  // returns address of Index object\n  Index* index() {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n  const Index* index() const {\n    return Base::template get\u0026lt;Index\u0026gt;(INDEX);\n  }\n\n  // returns address of symbols block\n  Symbol* symbols() {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  const Symbol* symbols() const {\n    return Base::template get\u0026lt;Symbol\u0026gt;(SYMBOS);\n  }\n  \n  // returns size in bytes of empty sequence. this method is used by \n  // PackedAllocator::allocateEmpty(int) to get object\'s default size\n  static int empty_size() \n  {\n    int allocator_size = Base::empty_size(); // size of allocator itself\n    int metadata_size  = Base::round(sizeof(Metadata)); // size of metadata block\n    int index_size     = 0; // index is empty for empty sequence\n    int symbols_size   = 0; // symbols block is also empty for empty sequence\n\n    return allocator_size + metadata_size + index_size + symbols_size;\n  }\n\n  void init() \n  {\n    Base::init(3); // the object has three resizable sections.\n    \n    // Allocate metadata block and initialize it\n    Base::template allocate\u0026lt;Metadata\u0026gt;(METADATA);\n\n    // Allocate empty block for index. Do not initialize it\n    Base::template allocate\u0026lt;Index\u0026gt;(INDEX, 0);\n    \n    // Allocate empty block for symbols. \n    Base::template allocate\u0026lt;Symbol\u0026gt;(SYMBOLS, 0);    \n  }\n\n  // change the value of idx-th symbol\n  void setSymbol(int idx, int symbol);\n\n  // insert new symbol at the specified position\n  int insert(int idx, int symbol) \n  {\n    enlarge(1);              // enalrge SYMBOLS block\n    insertSpace(idx, 1);     // shift symbols\n    setSymbol(idx, symbol);  // set new symbol value\n\n    reindex();               // update search index\n  }\n\n  int size() const \n  {\n    return metadata()-\u0026gt;size();\n  }\n\n  // update index for the searchable sequence\n  void reindex() \n  {\n     // check if the sequence if large enough to have index\n     if (size() \u0026gt; IndexSizeThreshold) \n     {\n       if (Base::size(INDEX) == 0)\n       {\n         Base::template allocate\u0026lt;Index\u0026gt;(0); // create empty index if it doesn\'t exist\n       }\n\n       Index* index = this-\u0026gt;index();\n\n       // compute index size for given symbols and resize the index.\n       index-\u0026gt;resize_for(this-\u0026gt;symbols(), size());\n\n       // rebuild the index\n       // note that any resize operation invalidates pointers to blocks\n       // going after the resized one.\n       index-\u0026gt;update(this-\u0026gt;symbols(), size());\n     }\n     else {\n       // the sequence if not full enough to have the index,\n       // free it.\n       Base::resize(INDEX, 0); // free index block\n     }\n  }\n  \nprivate:\n  // insert empty space into symbols\'s data block,\n  // shift symbols in the range [idx, size()) \'length\' positions right.\n  void insertSpace(int idx, int length);\n\n  // returns symbols block size for specified number of symbols\n  static int symbols_size(int symbols);\n\n  //enlarge symbols block by \'items\' elements.\n  void enlarge(int items) \n  {\n    // get new size for SYMBOLS block\n    int new_symbols_size = MyType::symbols_size(size() + items)\n    \n    // enlarge SYMBOLS block\n    Base::resize(SYMBOLS, new_symbols_size);\n  }\n};\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis pattern is used intensively for packed data structures in Memoria.\u003c/p\u003e\n'}).add({id:17,href:"/docs/data-zoo/associative-memory-1/",title:"Associative Memory (Part 1)",description:null,content:'\u003ch2 id="what-is-associative-memory"\u003eWhat is Associative Memory\u003c/h2\u003e\n\u003cp\u003eAssociative memory is content-addressable memory, where the item is being addressed given some part of it. In a broad sense, associative memory is a model for high-level mental function of \u003cem\u003eMemory\u003c/em\u003e. Such level of complexity is by no means a simple thing for implementation, though artificial neural networks have demonstrated pretty impressive results (at scale). In this article we are scaling things down to the level of bits and showing how to design and implement content-addressable memory at the level of \u003cem\u003ebits\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id="motivating-example-rdf"\u003eMotivating Example: RDF\u003c/h2\u003e\n\u003cp\u003eIn \u003ca href="https://en.wikipedia.org/wiki/Resource_Description_Framework"\u003eResource Description Framework\u003c/a\u003e data is modelled with a special form of a labelled graph, consisting from \u003cem\u003efacts\u003c/em\u003e (represented as URIs) and \u003cem\u003etriples\u003c/em\u003e in a form of $(Subject, Predicate, Object)$ linking various facts together. Logical representation of this \u003cem\u003esemantic graph\u003c/em\u003e is a table, enumerating all the triples in the graph. The main operation on the graph is \u003cem\u003epattern matching\u003c/em\u003e using SQL-like query language \u003ca href="https://en.wikipedia.org/wiki/SPARQL"\u003eSPARQL\u003c/a\u003e. Another common mode of operations over graphs is traversal, but this mode is secondary for semantic graphs. Pattern-matching in semantic graphs is based of \u003cem\u003eself-joins\u003c/em\u003e over the triple tables:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="triples.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBecause of the flexibility of SQPRQL, self-joins can be performed on any combination of Subject, Predicate and Objects, and the join on objects is the main reason why basic representation of graphs for RDF is a relational table. And, if we want fast execution, this table has to be properly indexed.\u003c/p\u003e\n\u003cp\u003eThe simplest way to provide indexing over a triple table is to sort it in some order, but ordered table only allows ordered \u003cem\u003ecomposite\u003c/em\u003e keys. If a table is ordered like (S, P, O), that means $(Subject, Predicate, Object)$, \u003cem\u003ethen\u003c/em\u003e we can search first by Subject, \u003cem\u003ethen\u003c/em\u003e buy Predicate, and only then by Object. But not vise versa. If we want to search by an Object first, we need another table ordered by object: (O, X, Y). To be able to search in any order we need all possible permutations of S, P and O: it\u0026rsquo;s $3! = 6$ tables.\u003c/p\u003e\n\u003cp\u003eSo, fully indexed RDF triple store will need at least 6 triple tables. How many is it?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times the size of the original single-table store (not counting URIs and Objects if they are stored separately).\u003c/li\u003e\n\u003cli\u003eIt\u0026rsquo;s \u003cem\u003eup to\u003c/em\u003e 6 times slower insertions if they are not paralleled. Of course we can make many insertions in parallel, but it\u0026rsquo;s \u003cem\u003e6 times more energy\u003c/em\u003e anyway.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eModern triple stores model semantic graphs with \u003cem\u003equads\u003c/em\u003e, adding \u003cem\u003eGraph ID\u003c/em\u003e to a triple, so nodes can link to entire graphs, no just other nodes (self-referentiality). In addition to, higher-dimensional tables ($D \u0026gt; 4$) can also be provided to speedup certain queries. And, in general case, if we have $D$-dimensional data table, we need \u003cem\u003eup to\u003c/em\u003e $D!$ orderings of this table. That is 24 for $D=4$ and grows faster than an exponent.\u003c/p\u003e\n\u003cp\u003eSorting relational tables does not scale at all for higher-order graphs ($D \u0026gt; 3$), but we can do better.\u003c/p\u003e\n\u003ch2 id="definition-of-associative-memory"\u003eDefinition of Associative Memory\u003c/h2\u003e\n\u003cp\u003eSo, without loss of generality, associative memory is a $D$-dimensional relation $R$ with \u003cem\u003eset\u003c/em\u003e semantics, over integer numbers drawn from some finite set (domain). The main operation on the memory is \u003cem\u003elookup\u003c/em\u003e that can be performed using arbitrary number of dimensions, specifying \u003cem\u003ematch\u003c/em\u003e, \u003cem\u003erange\u003c/em\u003e or \u003cem\u003epoint\u003c/em\u003e lookup, or any combination of for any number of dimensions. \u003cem\u003eRecall\u003c/em\u003e is the result of \u003cem\u003elookup\u003c/em\u003e operation, and is enumeration of all entries in $R$ matching the query.\u003c/p\u003e\n\u003cp\u003eAssociative memory can be either \u003cem\u003estatic\u003c/em\u003e, if only lookups are allowed. Or \u003cem\u003edynamic\u003c/em\u003e, if it supports insert, update and delete operation for individual elements (Update operation can be reduced to delete + insert).\u003c/p\u003e\n\u003ch2 id="multiscale-decomposition"\u003eMultiscale Decomposition\u003c/h2\u003e\n\u003cp\u003eLet we have a $D$-dimensional relation $R = \\lbrace {r_0, r_1, r_2, \u0026hellip;, r_{N-1}}\\rbrace$, representing a $set$, where $r_i = (c_0, c_1, \u0026hellip;, c_{D-1})_i$ - а $D$-dimensional tuple and $N$ is a \u0026lsquo;size\u0026rsquo; of the table (number of elements in the set). $c_{i,j}$ is a table\u0026rsquo;s cell value from row $i$ and dimension (column) $j$. Each cell value $c_{i,j}$ has a domain of $H$ bits.\u003c/p\u003e\n\u003cp\u003eExample: A set of 8x8 images with 8 bits per pixel can be represented with 64-dimensional relation with $H = 8$. Maximal number of images in a set is $N \u0026lt;= 8^{64} = 2^{192}$. Given such table we can easily define an associative memory reducing content-addressable lookup to linear table scans and bit manipulations (time complexity is $O(N)$). And, actually, this is how it\u0026rsquo;s implemented for approximate nearest neighbour search on massively-parallel hardware. Parallel linear scan is fast, but it\u0026rsquo;s not scalable (fast memory is expensive) and it\u0026rsquo;s not energy-efficient.\u003c/p\u003e\n\u003cp\u003eFortunately, we can transform $O(N)$ into $O(P H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e, where $1 \u0026lt;= P \u0026lt;= 2^D$ \u0026ndash; average number of nodes per \u003cstrong\u003ebucket\u003c/strong\u003e (see below), that is, thanks to the \u003ca href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality"\u003e\u0026ldquo;Blessing of Dimensionality\u0026rdquo;\u003c/a\u003e, usually tends to 1. And $M$ is a \u003cem\u003erecall size\u003c/em\u003e, number of points returned by the query over $R$.\u003c/p\u003e\n\u003cp\u003eTo perform the multiscale decomposition of relation $R$, we need to perform the following transformation for each row $r_i$:\u003c/p\u003e\n\u003cp\u003eLet $c_{ij} = S_{ij} = (s_0, s_1,s_2,\u0026hellip;,s_{H-1})_{ij}$, where $s \\in \\lbrace{0, 1}\\rbrace$ is a bit-string representation of cell value $c$. Let $s_h = B(S, h)$ \u0026ndash; $h$-th bit of string $S$.\u003c/p\u003e\n\u003cp\u003eNow, $M(r)$ is a multiscale decomposition of a row $r = (S_0, S_2, \u0026hellip;, S_{D-1})$. Informally, $M(r)$ is a bit string consisting from a concatenation of shuffling of all bits form $S_j$:\u003c/p\u003e\n\u003cp\u003e$M(r) = B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip;B(S_0, H-1) B(S_1, H-1) \u0026hellip; B(S_{D-1}, H-1)| \u0026hellip; B(S_0, 0) B(S_1, 0) \u0026hellip; B(S_{D-1}, 0)$.\u003c/p\u003e\n\u003cp\u003eThe symbol $|$ is added to graphically separate $H$ \u003cem\u003elayers\u003c/em\u003e of the multiscale representation from each other.\u003c/p\u003e\n\u003cp\u003eExample. Let $r = (100, 110, 001)$. Then $M(r) = 111|010|001$. Note, that in some sense, $M(r)$ is producing a point on a \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003e$Z$-order curve\u003c/a\u003e for $r$. This correspondence may help in some applications.\u003c/p\u003e\n\u003cp\u003eSo, multiscale decomposition of $T = M(R)$ converts a table with $D$ columns into a table with $H$ columns, which are called \u003cem\u003elayers\u003c/em\u003e here.\u003c/p\u003e\n\u003cp\u003eNow, let\u0026rsquo;s assume, that the table $T$ is sorted in a bit-lexicographic order.\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="tables.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is special about table $T$ is that every column $L_j$ contains some information form each column $D_j$ from table $R$. So, by searching in a column of $T$ we can search in all columns from $R$ at once. Table $T$ itself does not provide any speedup over the sorted $R$ because the number of rows is still the same as for table $R$. But quick look at the table will show that there are many \u003cem\u003erepetitions\u003c/em\u003e. So, we can transform $T$ into a tree, by hierarchically (here, from left to right) collapsing repetitive elements in tables\' columns. Now, a \u003cstrong\u003ebucket\u003c/strong\u003e is a list of all children of the same parent node sorted lexicographically. It can be shown, that there may be \u003cem\u003eat most\u003c/em\u003e $2^D$ elements in a bucket. So, search in such data structure is $O(2^D H + M)$ \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. If $D$ is small, say, 16 or less, this may dramatically improve performance relative to linear scan of $R$ (even if it\u0026rsquo;s sorted). See the \u003ca href="#analysis"\u003eAnalysis\u003c/a\u003e section below for additional properties and limitations.\u003c/p\u003e\n\u003cp\u003eNote that each path from root to leaf in the tree encodes a single row in the table $T$, and after the inverse multiscale decomposition, in $R$.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s demonstrate how search works. Let we want to enumerate all rows in $R$ with $D_2$ = 0111, so $Q = (X, Y, 0111)$, where $X$ and $Y$ are \u003cem\u003eplacehoders\u003c/em\u003e. First, we need a multiscale representation of $Q$, $M(Q) = xy0|xy1|xy1|xy1$. Now, we need to traverse the tree from root to leafs, according to this pattern:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="search.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHaving the multiscale query encoding, the tree traversal is straightforward. We are visiting sub-trees in-order, providing that current pattern matches the node\u0026rsquo;s label. Visiting the leaf (+ its label is matched) means full match. The excess number of nodes visited by a range query is called \u003cem\u003eoverwork\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this example, the selectivity of the query is pretty good. All leafs matching the query are being visited (visited nodes are drawn in the dotted-green line style). Note the nodes marked with (*). The traversal process visited some nodes \u003cem\u003enot leading to the full match\u003c/em\u003e. This is why type complexity estimation for this data structure is logarithmic \u003cem\u003e\u0026ldquo;on average\u0026rdquo;\u003c/em\u003e. Its performance depends on how data is distributed in the tree.\u003c/p\u003e\n\u003ch2 id="using-louds-for-the-tree"\u003eUsing LOUDS for the Tree\u003c/h2\u003e\n\u003cp\u003eTable $T$ has the same size (in bits) as the table $R$, but the tree, if implemented using pointer-based structure, will add a lot to the table representation. Fortunately, we can use \u003ca href="/docs/data-zoo/louds-tree"\u003eLOUDS Tree\u003c/a\u003e to encode the tree. LOUDS tree has very small memory footprint, only 2 bits per node (+ a small overhead, like 10%, for rank/select indices). Search tree size estimation is at most $NH$ nodes, so the tree itself will take at most size of two columns of the table $R$ ($2NH$ + small overhead). In most cases LOUDS-encoded $T$ will take less space that original table $R$, including auxiliary data structures.\u003c/p\u003e\n\u003cp\u003eWhat is the most important for LOUDS Tree is that it\u0026rsquo;s structured in memory in a linear order. So, if for some reason a spatial tree traversal degrades into linear search, the tree will be pretty good at this. We just need to read many layers of the tree in parallel. Such I/O operations can be efficiently prefetched.\u003c/p\u003e\n\u003ch2 id="analysis"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eIt can be shown that the tree is similar to a trie-based Quad Tree (for high dimensions), so many expected (average-case) and worst-case estimations also apply. In worst case, for high-dimensional trees, traversal degenerates into linear a search. Fortunately for LOUDS, it\u0026rsquo;s both memory-efficient for linear search \u003cem\u003eand\u003c/em\u003e for tree traversal. But on average, overwork is moderate, if doesn\u0026rsquo;t even tend to zero, so queries should perform pretty well.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s return to RDF triple stores and compare things to each other. Let\u0026rsquo;s assume that $D = 3$ (number of dimensions) and $H = 32$ (resource identifier\u0026rsquo;s size or search tree depth). So, in case of insertion of a triple, we have to perform 6 insertions into 6 triple tables (for each ordering) and 32 insertions in case of the tree (into each tree level). Reading is also slower: one lookup in a sorted table vs 32 lookups in the tree. It looks unreasonable to switch from triple tables (worst case logarithmic) to search tree (average case logarithmic), unless we are limited in memory and want to fit as many triples as possible into the available amount. But things start changing when we go into higher dimension ($D \u0026gt; 3$) and need more indices to speedup our queries.\u003c/p\u003e\n\u003cp\u003eSo far\u0026hellip;\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ePoint-like operations (to check if some row exists in the relation $R$) will take $O(D log(N))$ time.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRange search in the quad trees is logarithmic on average, but it\u0026rsquo;s relatively easy to build a worst-case example, when performance degrades to a linear scan. For example, if $D = 64$, maximal bucket size is $2^{64}$, that is much larger than any practical $N$ (number of entries in $R$). Unless the data is distributed uniformly among \u003cem\u003edifferent levels\u003c/em\u003e of the tree, we will end up having a few but very big buckets. So, special care must be taken on how we map our high-level data to dimensions of the tree. Random mapping is usually a safe bet, but always the best choice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn the search tree \u0026ldquo;Blessing of Dimensionality\u0026rdquo; is fighting with \u0026ldquo;Curse of Dimensionality\u0026rdquo;, it\u0026rsquo;s kind of $0 \\cdot \\infty$. In higher dimensions data tends to be extremely \u003cem\u003esparse\u003c/em\u003e because volume size grows exponentially with number of dimensions. So, normally, even in high dimensions, buckets will tend to have small number of elements. The bigger the number \u0026ndash; the better, because it improves data compression and speeds up queries. But beware of the worst case, when the tree has one big bucket that all queries are visiting. It has also been observed for similar K-d trees, that with higher number of dimensions, \u003cem\u003eoverwork\u003c/em\u003e also tens to increase (the blessing vs curse situation, $0 \\cdot \\infty$, who wins?).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIn case if LOUDS tree is dynamic and implemented using B-tree, \u003cem\u003einsert\u003c/em\u003e, \u003cem\u003eupdate\u003c/em\u003e and \u003cem\u003edelete\u003c/em\u003e operations to the search tree have $O(H log(N))$ time complexity for point-like updates and $O(H(log(N) + M))$ for batch updates of size $M$.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe data structure representation in memory is very compact. It\u0026rsquo;s the size of original table $R$ + (up to) the size of two columns from $R$ for LOUDS tree + 5-10% of the tree to auxiliary data structures. Overhead of the tree is constant and is amortizing with higher number of dimensions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEven if we work with high-dimensional data, and we are losing to the curse of dimensionality, it\u0026rsquo;s possible to perform approximate queries. Many applications where high-dimensional data analysis is required, like AI, are essentially approximate. LOUDS tree allows to compactly and efficiently remember additional bits of information with each node, to facilitate approximate queries (if necessary).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eLOUDS tree-based Associative memory seems to be impractical specifically for RDF triple stores, but if hardware accelerated, can be cost-effective at scale, providing also many other benefits, not just memory savings (which are huge for higher dimensions). The bottleneck is on the update operations, where insertion and deletion may require tens of B-tree updates. Fortunately, this operation is well-parallelizable so we can use thousands of small RISC-V cores equipped with special command for direct and energy-efficient implementation of essential operations (partial/prefix sums, rank and select). An array or cluster of such cores can even be embedded into \u003ca href="/subprojects/smart-storage"\u003estorage memory controller\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAdvanced data structures like LOUDS-based associative memory, considered to be impractical in the past, relative to more traditional things, like sorted tables and pointer-based data structures (trees, lists, graphs, \u0026hellip;) for main memory. But progress in computer hardware makes task-specific hardware accelerator a much more viable options, opening the road to completely new applications.\u003c/p\u003e\n\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-2"\u003enext post\u003c/a\u003e it will be shown how LOUDD-backed associative memory can be used for generic function approximation and inversion.\u003c/p\u003e\n'}).add({id:18,href:"/docs/data-zoo/associative-memory-2/",title:"Associative Memory (Part 2)",description:"",content:'\u003cp\u003eIn the \u003ca href="/docs/data-zoo/associative-memory-1"\u003eprevious part\u003c/a\u003e we saw how LOUDS tree can be used for generic compact and efficient associative associative memory over arbitrary number of dimensions. In this part we will see how LOUDS trees can be used for function approximation and inversion.\u003c/p\u003e\n\u003ch2 id="definitions"\u003eDefinitions\u003c/h2\u003e\n\u003cp\u003eLet $[0, 1] \\in R$ is the domain and range we are operating on. $D$ is a number of dimensions of our space, and for the sake of visualizability, $D = 2$.\u003c/p\u003e\n\u003cp\u003eLet $\\vec{X}$ is a vector encoding a point in $[0, 1]^D$, $\\vec{X} = \u0026lt;x_0, x_1, \u0026hellip;, x_{D-1}\u0026gt;$. Let $str(x [, h])$ is a function converting a real number from $[0, 1]$ into a binary string by taking a binary representation of a real number to a form like \u0026ldquo;$0.0010111010\u0026hellip;$\u0026rdquo;, and removing leading \u0026ldquo;$0.$\u0026rdquo; and trailing \u0026ldquo;$0\u0026hellip;$\u0026rdquo;. so, $str(0.181640625) = 001011101$. If $h$ argument is specified for $str(\\cdot, \\cdot)$, then resulting string is trimmed to $h$ binary digits, if it\u0026rsquo;s longer than that.\u003c/p\u003e\n\u003cp\u003eLet $len(x)$ is a number of digits in the result of $str(x)$. Note that $len(\\pi / 10) = \\infty$, so irrational numbers are literally infinite in this notation. Let $H$ is a maximal \u003cem\u003edepth\u003c/em\u003e of data, and there is some \u003cem\u003eimplicitly assumed\u003c/em\u003e arbitrary value for $h$, like 32 or 64, or even 128. So we can work with \u003cem\u003eapproximations\u003c/em\u003e of irrational and transcendent numbers, or with long rational numbers in a same way and without loss of generality.\u003c/p\u003e\n\u003cp\u003eLet $len(\\vec{X}) = max_{\\substack{i \\in \\lbrace 0,\u0026hellip;,D-1 \\rbrace }}(len(x_i))$.\u003c/p\u003e\n\u003cp\u003eLet $str(\\vec{X}) = (str(x_0),\u0026hellip;, str(x_{D-1}))$ is a string representation (a tuple) of $\\vec{X}$. And let we assume, elements of the tuple are implicitly extended with \u0026lsquo;$0$\u0026rsquo; from the right, if their length is less than $len(\\vec{X})$. In other words, all elements (binary string) of a tuple are implicitly of the same length.\u003c/p\u003e\n\u003cp\u003eLet $str(\\lbrace \\vec{X_0}, \\vec{X_1}, \u0026hellip; \\rbrace) = \\lbrace str(\\vec{X_0}), str(\\vec{X_1}), \u0026hellip; \\rbrace$. String representation of set of vectors is a set of string representation of individual vectors.\u003c/p\u003e\n\u003cp\u003eLet $M(\\vec{X}) = M(str(\\vec{X}))$ is a \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003emultiscale transformation\u003c/a\u003e of binary string representation of $\\vec{X}$. Informally, to compute $M(\\vec{X})$ we need to take all strings from its string representation (the tuple of binary strings) and produce another string of length $len(\\vec{X})  D$ by concatenating interleaved bits from each binary string in the tuple.\u003c/p\u003e\n\u003cp\u003eExample. Let $H = 3$ and $D = 2$. $\\vec{X} = \u0026lt;0.625, 0.25\u0026gt;$, $str(\\vec{X}) = (101, 01)$ and $M(\\vec{X}) = 10|01|10$. Note that signs $|$ are added to separate $H$ \u003cem\u003epath elements\u003c/em\u003e in the recording, they are here for the sake of visualization and are not a part of the representation. The string $10|01|10$ is also called \u003cem\u003epath expression\u003c/em\u003e because it\u0026rsquo;s a unique path in the multidimensional space decomposition \u003cem\u003eencoding\u003c/em\u003e the position of $\\vec{X}$ in this decomposition. Visually:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="multiscale1.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eNote that the order in which \u003cem\u003epath elements\u003c/em\u003e of a path expression enumerate the volume is \u003ca href="https://en.wikipedia.org/wiki/Z-order_curve"\u003eZ-order\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eGiven a \u003cem\u003eset\u003c/em\u003e of $D$-dimensional vectors $\\lbrace \\vec{X} \\rbrace$, it\u0026rsquo;s multiscale transformation can be represented as a $D$-dimensional \u003ca href="https://en.wikipedia.org/wiki/Quadtree"\u003eQuad Tree\u003c/a\u003e. Such quad tree can be represented with a \u003ca href="/docs/data-zoo/louds-tree/#cardinal-trees"\u003ecardinal LOUDS tree\u003c/a\u003e of degree $2^D$. Here, implicit parameter $H$ is a \u003cem\u003emaximal height\u003c/em\u003e of the Quad Tree.\u003c/p\u003e\n\u003ch2 id="basic-asymptotic-complexity"\u003eBasic Asymptotic Complexity\u003c/h2\u003e\n\u003cp\u003eGiven that $N = |\\lbrace \\vec{X} \\rbrace|$, and given that LOUDS tree is dynamic (represented internally as a b-tree), the following complexity estimations apply:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInsertion and deletion\u003c/strong\u003e of a point is $O(H log(N))$. Update is semantically not defined because it\u0026rsquo;s a \u003cem\u003eset\u003c/em\u003e. Batch updates in Z-order are $O(H (log(N) + B))$, where $B$ is a batch size. Otherwise can be slightly worse, up to $O(H log(N) B)$ in the worst case.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePoint lookup\u003c/strong\u003e (membership query) is $O(D H log(N))$.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRange and projection\u003c/strong\u003e queries are $O(D H log(N) + M)$ \u003cem\u003eon average\u003c/em\u003e, where $M$ is a recall size (number of vectors matching the query). In worst case tree traversal degrades to the linear scan of the entire set of vectors.\u003c/li\u003e\n\u003cli\u003eThe data structure is \u003cstrong\u003espace efficient\u003c/strong\u003e. 2 bits per LOUDS tree node + \u003cem\u003ecompressed bitmap\u003c/em\u003e of cardinal labels. For most usage scenarios, space complexity will be within \u003cstrong\u003e2x the raw bit size\u003c/strong\u003e of $str(\\lbrace \\vec{X} \\rbrace)$.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id="functions-approximation-using-quad-trees"\u003eFunctions Approximation Using Quad Trees\u003c/h2\u003e\n\u003cp\u003eLet we have some function $y = f(x)$, and we also have the graph of this function on $[0, 1]^2$. If function $f(\\cdot)$ is elementary, or we have another way to compute it, it\u0026rsquo;s computable (for us). What if we have $f(\\cdot)$, but we want to compute inverse function: $x = f^{-1}(y)$? With compact quad trees we can \u003cem\u003eapproximate\u003c/em\u003e both functions out of the same \u003cem\u003efunction graph\u003c/em\u003e using compact quad trees:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="function.svg" width="100%"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHere the tree has four layers shown upside down (drawing more detailed layers above less detailed ones). And if we want to compute $f(a)$, where $str(a) = a_3a_2a_1a_0$, the path expression will be $a_3x|a_2x|a_1x|a_0x$, where $x$ here is a \u003cem\u003eplaceholder sign\u003c/em\u003e. Now we need to traverse the tree as it defined in \u003ca href="/docs/data-zoo/associative-memory-1/#multiscale-decomposition"\u003eprevious part\u003c/a\u003e. If there is a point for $a$ of a function graph, it will be found in a logarithmic expected time. The path expression for $f^{-1}(b)$ will be $b_3x|b_2x|b_1x|b_0x$.\u003c/p\u003e\n\u003cp\u003eNote that compressed cardinal LOUDS tree will use less than 4 bits (+ some % of auxiliary data) \u003cem\u003eper square\u003c/em\u003e on the graph above. Sol, looking into this graph we already can say something specific about what will be the cost of approximation, depending on required precision (maximal $H$).\u003c/p\u003e\n\u003ch2 id="function-compression"\u003eFunction Compression\u003c/h2\u003e\n\u003cp\u003eLet we have a function that checks if some point is inside some ellipse:\u003c/p\u003e\n$$\ny = f(x_1, x_2): \\begin{cases} \n    1 \u0026\\text{if } (x_1, x_2) \\text{ is inside the the ellipse,} \\\\ \n    0 \u0026\\text{if it\'s outside.} \n\\end{cases}\n$$\n\u003cp\u003eThis function defines some \u003cem\u003earea\u003c/em\u003e on the graph. Let $N$ is the number of \u0026ldquo;pixels\u0026rdquo; we need to define the function $f(x,y)$ on a graph. Then, using compressed quad trees, we can do it with ${O(\\sqrt{N})}$ bits \u003cem\u003eon average\u003c/em\u003e for 2D space :\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="region.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe square root here is because we need \u0026ldquo;detailed\u0026rdquo; only for the border of the area, while \u0026ldquo;in-lands\u0026rdquo; needs much lower resolution.\u003c/p\u003e\n\u003ch2 id="blessing-of-dimensionality-vs-curse-of-dimensionality"\u003eBlessing of Dimensionality vs Curse of Dimensionality\u003c/h2\u003e\n\u003cp\u003eLet we have 2D space and we have a tree encoding the following structure:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="corners.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThere are four points in each corner of the coordinate plane. As it can be obvious from the picture, each new level of the tree will add four new bits for just for cardinal labels (not including LOUDS tree itself): three \u0026lsquo;0\u0026rsquo; and one \u0026lsquo;1\u0026rsquo; (in the corresponding corners). Now if we go to higher dimensions, for 3D we will have 8 new bits, for 8D \u0026ndash; 256 new bits and for 32D \u0026ndash; $2^32$ bits. This is \u003cstrong\u003eCurse of Dimensionality\u003c/strong\u003e (CoD) for spatial data structures: volume grows exponentially with dimensions, so linear sizes \u0026ndash; too.\u003c/p\u003e\n\u003cp\u003eNevertheless, with higher dimensions, the volume is getting exponentially sparser, so we can use data compression techniques like RLE encoding to represent long sparse bitmaps for cardinal labels. This is \u003cstrong\u003eBlessing of Dimensionality\u003c/strong\u003e (BoD). For \u003cem\u003ecompressed\u003c/em\u003e LOUDS cardinal trees, the example above will require $O(D)$ bits per quadrant per tree layer for $D$-dimensional Quad Tree.\u003c/p\u003e\n\u003cp\u003eSo, the the whole idea of compression in this context is implicit (or in-place) \u003cstrong\u003eDimensionality Reduction\u003c/strong\u003e. Compressed data structure don\u0026rsquo;t degrade so fast as their uncompressed analogs, yet maintain the same \u003cem\u003elogical API\u003c/em\u003e. Nevertheless, data compression is not the final cure for CoD, because practical compression itself is not that powerful, especially in the case of using RLE for bitmap compression. So, in each practical case high-dimensional tree can become \u0026ldquo;unstable\u0026rdquo; and \u0026ldquo;explode\u0026rdquo; in size. Fortunately, such highly-dimensional data ($D \u0026gt; 16$) is rarely makes sense to work with directly (without prior dimensionality reduction).\u003c/p\u003e\n\u003cp\u003eFor example, for $D=8$ exponential effects in space are still pretty moderate (256-degree cardinal tree), yet 8 dimensions is already a good approximation of real objects in symbolic methods. High-dimensional structures are effectively \u003cem\u003eblack boxes\u003c/em\u003e for us, because our visual intuitions about properties of objects don\u0026rsquo;t work in \u003ca href="https://www.math.wustl.edu/~feres/highdim"\u003ehigh dimensions\u003c/a\u003e. Like, volume of cube is concentrating in it\u0026rsquo;s corners (because there is an exponentional number of corners). Or the volume of sphere is concentrating near its surface, and many more\u0026hellip; Making decisions in high dimensions suffer from noise in data and machine rounding, because points tend to be very close to each other. And, of course, computing Euclidian distance does not make (much) sense.\u003c/p\u003e\n\u003ch2 id="comparison-with-multi-layer-perceptrons"\u003eComparison with Multi-Layer Perceptrons\u003c/h2\u003e\n\u003cp\u003eNeural networks has been known to be a pretty good function approximators, especially for multi-dimensional cases. Let\u0026rsquo;s check how compressed spatial tree can be compared with multi-layer perceptrons (MLP). This type of artificial neural networks is by no means the best example of ANNs, yet it\u0026rsquo;s a pretty ideomatic member of this family.\u003c/p\u003e\n\u003cp\u003eIn the core of MLP is the idea of \u003cem\u003elinear separability\u003c/em\u003e. In a bacis case, there are two regions of multidimensional space that can\u0026rsquo;t be separated by a hyperplane from each other. MLP has multiple ($K$) layers, where first $K-1$ layers perform specific space transformations using linear (weights) and non-linear (thresholds) operators in such way that $K$-th layer can perform the linear separation:\u003c/p\u003e\n\n\u003cdiv style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"\u003e\n  \u003ciframe src="https://www.youtube.com/embed/k-Ann9GIbP4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eSo, this condition is simplified of the picture below. Here, we have two classes (\u003cem\u003ered\u003c/em\u003e and \u003cem\u003egreen\u003c/em\u003e dots) with complex non-linear boundary between those classes. After transforming the space towards linear separation of those classes and making inverse transformation, the initial hyperplane (here, a line) is broken in many places:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eWhat is important, is that each such like break is an inverse transformation of the original hyperplane. Those transformations has to be described, and description will take some space. So we can speak about the \u003cstrong\u003edescriptional (Kolmogorov) complexity of the decision boundary\u003c/strong\u003e. Or, in the other way, \u003cem\u003ehow many neurons (parameters) we need to encode the decision boundary\u003c/em\u003e?\u003c/p\u003e\n\u003cp\u003eFrom Algorithmic Information Theory it\u0026rsquo;s known that arbitrary string $s$, drawn from a uniform distribution, will be \u003cem\u003eincompressible\u003c/em\u003e with high probability, or $K(s) \\to |s|$. In other words, most mathematically possible objects are \u003cem\u003erandom\u003c/em\u003e-looking, we hardly can find and exploit any structure in them.\u003c/p\u003e\n\u003cp\u003eReturning back to MLP, it\u0026rsquo;s expected that in \u0026ldquo;generic case\u0026rdquo; decision boundaries will be \u003cem\u003ecomplex\u003c/em\u003e: the line between classes will have many breaks, so, may transformations will be needed to describe it with required precision (and this is even not taking CoD into account).\u003c/p\u003e\n\u003cp\u003eDescribing decision boundaries (DB) with compressed spatial trees may look like a bad idea from the first glance. MLPs encode DB with superpositions of elementary functions (hyperplanes and non-linear units). Quad Trees do it with hyper-cubes, and it\u0026rsquo;s obvious that we may need a lot of hyper-cubes in place of just one arbitrary hyper-plane. If it\u0026rsquo;s the case, we say that hyper-planes \u003cem\u003egeneralize\u003c/em\u003e DB \u003cem\u003emuch better\u003c/em\u003e than hyper-cubes:\u003c/p\u003e\n\u003cfigure\u003e\u003cimg src="classifier-tree.svg"/\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eBut it should hardly be a surprise, if in a real life case it will be find out that descriptions or the network and the corresponding tree are roughly the same.\u003c/p\u003e\n\u003cp\u003eSo, Neural Networks may generalize much better in some cases than compressed quad trees and perform better in very high dimensional spaces (they suffer less from CoD because of better generalization), but trees have the following benefits:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf computational complexity of MLP is $\\Omega(W)$ (a lot of large matrix multiplications), where $W$ is number of parameters, complexity of inference in the quad tree is \u003cem\u003eroughly\u003c/em\u003e from $O(log(N))$, where $N$ is number of bits of information in the tree. So trees may be much faster than networks \u003cem\u003eon average\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eQuad Trees are dynamic. Neural Networks require retraining in case of updates, at the same time adding (or removing) an element to the tree is $O(log(N))$ \u003cem\u003eworst case\u003c/em\u003e. It may be vital for may applications operating on-line, like robotics.\u003c/li\u003e\n\u003cli\u003eQuad Trees support \u0026ldquo;inverse inference\u0026rdquo; mode, when we can specify classes (outputs) and see\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eSo, compressed quad trees may be much better for complex dynamic domains with tight decision boundaries with moderate number of dimensions (8-24). It\u0026rsquo;s not that clear yet how trees will perform in real life applications. Memoria is providing (1) \u003cem\u003eexperimental\u003c/em\u003e compressed dynamic cardinal LOUDS tree for low dimensional spaces (2 - 64).\u003c/p\u003e\n\u003cp\u003e(1) Not yet ready at the time of writing.\u003c/p\u003e\n\u003ch2 id="hardware-acceleration"\u003eHardware Acceleration\u003c/h2\u003e\n\u003cp\u003eThe main point of hardware acceleration of compressed Quad Trees is that inference may be really cheap (on average). It\u0026rsquo;s just a bunch of memory lookups (it\u0026rsquo;s a \u003cem\u003ememory-bound\u003c/em\u003e problem). Matrix multipliers, from other size, are also pretty energy efficient. Nevertheless, \u003cem\u003etrees scale better with complexity of decision boundaries\u003c/em\u003e.\u003c/p\u003e\n'}).add({id:19,href:"/docs/data-zoo/",title:"Containers List",description:"",content:""}).add({id:20,href:"/docs/overview/",title:"Overview List",description:"Overview List",content:""}).add({id:21,href:"/docs/",title:"Docs",description:"Docs Memoria.",content:""}),search.addEventListener('input',b,!0),suggestions.addEventListener('click',c,!0);function b(){var d,e;const c=5;d=this.value,e=a.search(d,{limit:c,enrich:!0}),suggestions.classList.remove('d-none'),suggestions.innerHTML="";const b={};e.forEach(a=>{a.result.forEach(a=>{b[a.doc.href]=a.doc})});for(const d in b){const e=b[d],a=document.createElement('div');if(a.innerHTML='<a href><span></span><span></span></a>',a.querySelector('a').href=d,a.querySelector('span:first-child').textContent=e.title,a.querySelector('span:nth-child(2)').textContent=e.description,suggestions.appendChild(a),suggestions.childElementCount==c)break}}function c(){while(suggestions.lastChild)suggestions.removeChild(suggestions.lastChild);return!1}})()